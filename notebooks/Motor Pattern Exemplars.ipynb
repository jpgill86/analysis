{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import quantities as pq\n",
    "import elephant\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import neurotic\n",
    "from neurotic.datasets.data import _detect_spikes, _find_bursts\n",
    "from modules.utils import BehaviorsDataFrame, DownsampleNeoSignal\n",
    "from modules import r_stats\n",
    "from modules.plot_utils import add_scalebar, solve_figure_horizontal_dimensions, solve_figure_vertical_dimensions, plot_signals_with_scalebars\n",
    "\n",
    "pq.markup.config.use_unicode = True  # allow symbols like mu for micro in output\n",
    "pq.mN = pq.UnitQuantity('millinewton', pq.N/1e3, symbol = 'mN');  # define millinewton\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.markers import CARETLEFT, CARETRIGHT, CARETUP, CARETDOWN, CARETUPBASE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# don't warn about invalid comparisons to NaN\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "# np.nanmax raises a warning if all values are NaN and returns NaN, which is the behavior we want\n",
    "warnings.filterwarnings('ignore', message='All-NaN slice encountered')\n",
    "\n",
    "# elephant.statistics.instantaneous_rate always complains about negative values\n",
    "warnings.filterwarnings('ignore', message='Instantaneous firing rate approximation contains '\n",
    "                                          'negative values, possibly caused due to machine '\n",
    "                                          'precision errors')\n",
    "\n",
    "# with matplotlib>=3.1 and seaborn<=0.9.0, deprecation warnings are raised\n",
    "# whenever tick marks are placed on the right axis but not the left\n",
    "from matplotlib.cbook import MatplotlibDeprecationWarning\n",
    "warnings.simplefilter('ignore', MatplotlibDeprecationWarning)\n",
    "\n",
    "# don't complain about opening too many figures\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make figures interactive and inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = 'motor-pattern-exemplars'\n",
    "if not os.path.exists(export_dir):\n",
    "    os.mkdir(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general plot settings\n",
    "sns.set(\n",
    "    style = 'ticks',\n",
    "    font_scale = 1,\n",
    "    font = ['Palatino Linotype', 'serif'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_colors = {\n",
    "    'B38':       '#EFBF46', # yellow\n",
    "    'I2':        '#DC5151', # red\n",
    "    'B8a/b':     '#DA8BC3', # pink\n",
    "    'B6/B9':     '#64B5CD', # light blue\n",
    "    'B3/B6/B9':  '#5A9BC5', # medium blue\n",
    "    'B3':        '#4F80BD', # dark blue\n",
    "    'B4/B5':     '#00A86B', # jade green\n",
    "    'Force':     '#1A1A1A', # very dark gray\n",
    "}\n",
    "force_colors = {\n",
    "    'shoulder':     unit_colors['B38'],\n",
    "    'dip':          unit_colors['I2'],\n",
    "    'initial rise': unit_colors['B8a/b'],\n",
    "    'rise':         unit_colors['B8a/b'],\n",
    "    'plateau':      unit_colors['B3/B6/B9'],\n",
    "    'drop':         '#808080', # medium gray\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the selected unit colors\n",
    "sns.palplot(unit_colors.values())\n",
    "with plt.rc_context({'font.weight': 'bold'}):\n",
    "    for i, (unit, color) in enumerate(unit_colors.items()):\n",
    "        plt.gca().annotate(f'{unit}\\n{color[1:]}', xy=(i, 0), ha='center', va='center', color='w')\n",
    "plt.axis('off')\n",
    "plt.subplots_adjust(0, 0, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print hex codes for selected unit colors\n",
    "for unit, color in unit_colors.items():\n",
    "    print(f'{unit.ljust(10)} {mcolors.to_hex(color).upper()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see simulated colorblindness for selected unit colors\n",
    "print(\n",
    "    'https://davidmathlogic.com/colorblind/#' +\n",
    "    '-'.join([mcolors.to_hex(c).upper().replace('#', '%23') for c in unit_colors.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = '../../data/metadata.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [\n",
    "    'B38',\n",
    "    'I2',\n",
    "    'B8a/b',\n",
    "    'B6/B9',\n",
    "    'B3',\n",
    "    'B4/B5',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_units = [\n",
    "    'uV', # I2\n",
    "    'uV', # RN\n",
    "    'uV', # BN2\n",
    "    'uV', # BN3\n",
    "    'mN', # Force\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names_by_animal = {\n",
    "#     'JG07': ['I2-L', 'RN-L', 'BN2-L', 'BN3-L',    'Force'],\n",
    "#     'JG08': ['I2',   'RN',   'BN2',   'BN3',      'Force'],\n",
    "#     'JG11': ['I2',   'RN',   'BN2',   'BN3-PROX', 'Force'],\n",
    "    'JG12': ['I2',   'RN',   'BN2',   'BN3-DIST', 'Force'],\n",
    "#     'JG14': ['I2',   'RN',   'BN2',   'BN3-PROX', 'Force'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_types_by_food = {\n",
    "    'Bite':      ['Bite (regular 5-cm nori strip)'],\n",
    "    'Swallow':   ['Swallow (tape nori)'],\n",
    "    'Rejection': ['Rejection (tubing)'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeding_bouts = {\n",
    "    # (animal, food, bout_index): (data_set_name, time_window)\n",
    "\n",
    "    ('JG12', 'Bite',      0): ('IN VIVO / JG12 / 2019-05-10 / 001', [2170.5, 2174.2]), # 1 bite\n",
    "    ('JG12', 'Swallow',   0): ('IN VIVO / JG12 / 2019-05-10 / 002', [2977.3, 2984.5]), # 1 swallow\n",
    "    ('JG12', 'Rejection', 0): ('IN VIVO / JG12 / 2019-05-10 / 002', [1429.3, 1441.3]), # 1 rejection\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_union(queries):\n",
    "    return '|'.join([f'({q})' for q in queries if q is not None])\n",
    "\n",
    "def label2query(label):\n",
    "    animal, food = label[:4], label[5:]\n",
    "    query = f'(Animal == \"{animal}\") & (Food == \"{food}\")'\n",
    "    return query\n",
    "\n",
    "def contains(series, string):\n",
    "#     return np.array([string in x for x in series])\n",
    "    return series.map(lambda x: string in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig(blk, channel):\n",
    "    sig = next((sig for sig in blk.segments[0].analogsignals if sig.name == channel), None)\n",
    "    if sig is None:\n",
    "        raise Exception(f'Channel \"{channel}\" could not be found')\n",
    "    else:\n",
    "        return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_index(blk, channel):\n",
    "    index = next((i for i, sig in enumerate(blk.segments[0].analogsignals) if sig.name == channel), None)\n",
    "    if index is None:\n",
    "        raise Exception(f'Channel \"{channel}\" could not be found')\n",
    "    else:\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(blk, metadata):\n",
    "    # nearly identical to neurotic's implementation except\n",
    "    # time_slice ensures proxies are loaded\n",
    "    \n",
    "    for sig_filter in metadata['filters']:\n",
    "        index = get_sig_index(blk, sig_filter['channel'])\n",
    "        high = sig_filter.get('highpass', None)\n",
    "        low  = sig_filter.get('lowpass',  None)\n",
    "        if high:\n",
    "            high *= pq.Hz\n",
    "        if low:\n",
    "            low  *= pq.Hz\n",
    "        blk.segments[0].analogsignals[index] = elephant.signal_processing.butter(  # may raise a FutureWarning\n",
    "            signal = blk.segments[0].analogsignals[index].time_slice(None, None),\n",
    "            highpass_freq = high,\n",
    "            lowpass_freq  = low,\n",
    "        )\n",
    "    \n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_burst(burst):\n",
    "    time, duration, n_spikes = burst\n",
    "    return duration >= 0.5*pq.s and n_spikes > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must first convert args from quantities to simple ndarrays\n",
    "# (use .rescale('s').magnitude)\n",
    "def normalize_time(fixed_times, t, extrapolate=True):\n",
    "    \n",
    "    if not isinstance(t, np.ndarray):\n",
    "        if type(t) is list:\n",
    "            t = np.array(t)\n",
    "        else:\n",
    "            t = np.array([t])\n",
    "    \n",
    "    assert not isinstance(fixed_times, pq.Quantity), f'fixed_times should not be a quantity (use .magnitude)'\n",
    "    assert not isinstance(t, pq.Quantity), f't should not be a quantity (use .magnitude)'\n",
    "    assert np.all(np.diff(fixed_times[~np.isnan(fixed_times)])>=0), f'fixed_times must be sorted: {fixed_times}'\n",
    "    \n",
    "    # create a copy in which NaNs are removed\n",
    "    # - this is done because searchsorted does not work well with NaNs\n",
    "    # - infinities are retained here\n",
    "    fixed_times_without_nans = fixed_times[np.where(~np.isnan(fixed_times))[0]]\n",
    "    \n",
    "    # find the indexes of the fixed values that are just after each value in t\n",
    "    indexes = np.searchsorted(fixed_times_without_nans, t)\n",
    "    \n",
    "    # adjust indexes to account for the NaNs that were removed\n",
    "    nan_indexes = np.where(np.isnan(fixed_times))[0]\n",
    "    for nan_index in nan_indexes:\n",
    "        indexes[indexes >= nan_index] += 1\n",
    "    \n",
    "    # increment/decrement any index equal to 0/N, where N=len(fixed_times)\n",
    "    # - this is needed for values in t that are less/greater than the min/max fixed\n",
    "    #   time, and for NaNs in t which get assigned an index of N by searchsorted\n",
    "    # - for values in t less/greater than the min/max fixed time, this\n",
    "    #   increment/decrement in index will prepare that value to be normalized\n",
    "    #   using extrapolation based on the first/last interval in fixed_times\n",
    "    indexes = np.clip(indexes, 1, len(fixed_times)-1)\n",
    "    \n",
    "    # compute the normalized values of t using linear interpolation between the\n",
    "    # bordering fixed times\n",
    "    # - normalization of values in t that are less than the min fixed time is\n",
    "    #   accomplished by extrapolation, as the fraction (after-t)/(after-before)\n",
    "    #   will be greater than 1 since the t value is in fact earlier than the\n",
    "    #   \"before\" fixed time\n",
    "    # - normalization of values in t that are greater than the max fixed time is\n",
    "    #   accomplished by extrapolation, as the fraction (after-t)/(after-before)\n",
    "    #   will be less than 0 since the t value is in fact later than the \"after\"\n",
    "    #   fixed time\n",
    "    before = fixed_times[indexes-1]\n",
    "    after  = fixed_times[indexes]\n",
    "    t_normalized = indexes - (after-t)/(after-before)\n",
    "    \n",
    "    if not extrapolate:\n",
    "        # extrapolation is already done, so here we undo it by setting to NaN\n",
    "        # the normalized value for any t that is less/greater than the min/max\n",
    "        # fixed time\n",
    "        with np.errstate(invalid='ignore'): # don't warn about invalid comparisons to NaN\n",
    "            t_normalized[t < np.nanmin(fixed_times)] = np.nan\n",
    "            t_normalized[t > np.nanmax(fixed_times)] = np.nan\n",
    "    \n",
    "    return t_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must first convert args from quantities to simple ndarrays\n",
    "# (use .rescale('s').magnitude)\n",
    "def unnormalize_time(fixed_times, t_normalized, extrapolate=True):\n",
    "    \n",
    "    if not isinstance(t_normalized, np.ndarray):\n",
    "        if type(t_normalized) is list:\n",
    "            t_normalized = np.array(t_normalized)\n",
    "        else:\n",
    "            t_normalized = np.array([t_normalized])\n",
    "    \n",
    "    assert not isinstance(fixed_times, pq.Quantity), f'fixed_times should not be a quantity (use .magnitude)'\n",
    "    assert not isinstance(t_normalized, pq.Quantity), f't_normalized should not be a quantity (use .magnitude)'\n",
    "    assert np.all(np.diff(fixed_times[~np.isnan(fixed_times)])>=0), f'fixed_times must be sorted: {fixed_times}'\n",
    "    \n",
    "    # get the index of the fixed time that comes after each t\n",
    "    indexes = np.ceil(t_normalized)\n",
    "    \n",
    "    # clip the \"after\" indexes so that the first or last interval\n",
    "    # in fixed_times will be used for extrapolation\n",
    "    indexes = np.clip(indexes, 1, len(fixed_times)-1)\n",
    "    \n",
    "    # get the fixed times before and after each t\n",
    "    before = np.array([fixed_times[int(i)-1] if not np.isnan(i) else np.nan for i in indexes])\n",
    "    after  = np.array([fixed_times[int(i)]   if not np.isnan(i) else np.nan for i in indexes])\n",
    "    \n",
    "    # compute the real values of t\n",
    "    t = after + (t_normalized-indexes)*(after-before)\n",
    "    \n",
    "    if not extrapolate:\n",
    "        # extrapolation is already done, so here we undo it by setting to NaN\n",
    "        # the value for any t that is less/greater than the min/max fixed time\n",
    "        with np.errstate(invalid='ignore'): # don't warn about invalid comparisons to NaN\n",
    "            t[t < np.nanmin(fixed_times)] = np.nan\n",
    "            t[t > np.nanmax(fixed_times)] = np.nan\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these arrays are used as interp_times for resample_sig_in_normalized_time\n",
    "# depending on the segmentation scheme\n",
    "# - linspace ensures samples will be taken at regular intervals in normalized time\n",
    "interp_resolution = 1000\n",
    "video_seg_interp_times = np.linspace(0, 9, interp_resolution)\n",
    "force_seg_interp_times = np.linspace(0, 9, interp_resolution)\n",
    "\n",
    "def resample_sig_in_normalized_time(fixed_times, sig, interp_times):\n",
    "    \n",
    "    # get normalized times and signal values\n",
    "    # - normalize_time will put into times_normalized a np.nan wherever a time was not normalizable,\n",
    "    #   i.e., wherever the time occurred adjacent to a missing fixed time (np.nan in fixed_times)\n",
    "    times = sig.times.rescale('s').magnitude\n",
    "    times_normalized = normalize_time(fixed_times.magnitude, times)\n",
    "    y = sig.magnitude.flatten()\n",
    "\n",
    "    # drop times that could not be normalized due to missing fixed times\n",
    "    # - interp1d will erroneously interpolate across the gaps created by this deletion,\n",
    "    #   but we will then replace those interpolated values with np.nan\n",
    "    where_normalizable = np.where(~np.isnan(times_normalized))[0]\n",
    "    times_normalized = times_normalized[where_normalizable]\n",
    "    y = y[where_normalizable]\n",
    "\n",
    "    # resample evenly in normalized time\n",
    "    # - with bounds_error=False and fill_value=np.nan, interp1d will put np.nan in places outside\n",
    "    #   the min and max of times_normalized\n",
    "    # - interp1d will interpolate across the regions we deleted, but we want np.nan there,\n",
    "    #   so we will insert them manually in the next step\n",
    "    interp_func = sp.interpolate.interp1d(times_normalized, y, kind='linear', bounds_error=False, fill_value=np.nan)\n",
    "    y_interp = interp_func(interp_times)\n",
    "\n",
    "    # replace erroneously interpolated values with np.nan\n",
    "    # - now the points in y_interp which would correspond to times that could not be normalized\n",
    "    #   have been set to np.nan\n",
    "    where_not_normalizable = np.where(np.isnan(unnormalize_time(fixed_times.magnitude, interp_times)))[0]\n",
    "    y_interp[where_not_normalizable] = np.nan\n",
    "\n",
    "    return y_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_column_analysis(column):\n",
    "    unit = column.split('(')[-1].split(')')[0]\n",
    "    mean = df_all[column].mean()\n",
    "    std = df_all[column].std()\n",
    "    cv = std/abs(mean)\n",
    "    print(f'Overall mean +/- std: {mean:.2f} +/- {std:.2f} {unit}')\n",
    "    print(f'Overall coefficient of variation CV: {cv:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differences_test(x, y, x_label='GROUP 1', y_label='GROUP 2', measure_label='MEASURE', units='UNITS', alpha=0.05):\n",
    "\n",
    "    # descriptive statistics\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    x_std = np.std(x, ddof=1)\n",
    "    y_std = np.std(y, ddof=1)\n",
    "    x_n = len(x)\n",
    "    y_n = len(y)\n",
    "    assert x_n == y_n, 'expected x and y to be paired but they have unequal length'\n",
    "    print(f'{x_label}: (M = {x_mean:g}, SD = {x_std:g}, N = {x_n:g})')\n",
    "    print(f'{y_label}: (M = {y_mean:g}, SD = {y_std:g}, N = {y_n:g})')\n",
    "    print()\n",
    "\n",
    "    # Shapiro-Wilk test for normality of differences\n",
    "    # - equivalent R test: shapiro.test(x-y)\n",
    "    shapiro_result = r_stats.shapiro_test(x.values-y.values)\n",
    "    shapiro_W, shapiro_p = shapiro_result['W'], shapiro_result['p']\n",
    "    shapiro_signif = '*' if shapiro_p < alpha else '(n.s.)'\n",
    "    print(f'H0: Differences have normal distribution, W = {shapiro_W:g},\\tp = {shapiro_p:g} {shapiro_signif}')\n",
    "\n",
    "    if shapiro_p >= 0.05:\n",
    "        print('- Because the differences can be assumed to be normal, a paired t-test will be used')\n",
    "\n",
    "        # paired one-tailed T-test for an increase in means\n",
    "        # - equivalent R test : t.test(x, y, paired=TRUE, alternative=\"greater\")\n",
    "        ttest_result = r_stats.t_test(x.values, y.values, paired=True, alternative='greater')\n",
    "        ttest_t, ttest_p = ttest_result['t'], ttest_result['p']\n",
    "        ttest_signif = '*' if ttest_p < alpha and x_mean > y_mean else '(n.s.)'\n",
    "        print(f'H0: Difference in means is not positive,  t = {ttest_t:g},\\tp = {ttest_p:g} {ttest_signif}')\n",
    "\n",
    "        # Cohen's d for effect size\n",
    "        # - equivalent R function: library(effsize); cohen.d(x, y)\n",
    "        # - although the R function offers a paired=TRUE case, I'm not using it here\n",
    "        #   because I don't understand the paper cited in the documentation. It appears\n",
    "        #   to be a variation on a different effect size measure, so it's unclear that\n",
    "        #   it should be referred to as \"Cohen's d\". When paired=TRUE, the result is\n",
    "        #   generally a little smaller.\n",
    "        cohen_d = r_stats.cohen_d(x.values, y.values)['estimate']\n",
    "        print()\n",
    "        print(f'Effect size: Cohen\\'s d = {cohen_d:g}')\n",
    "        \n",
    "        if ttest_p < alpha and x_mean > y_mean:\n",
    "            print()\n",
    "            print(f'\"A paired-samples one-tailed t-test indicated that {measure_label} for the ' \\\n",
    "                  f'[{x_n}] animals were significantly [greater/longer] for ' \\\n",
    "                  f'{x_label} (M = {x_mean:.2f} {units}, SD = {x_std:.2f} {units}) than for ' \\\n",
    "                  f'{y_label} (M = {y_mean:.2f} {units}, SD = {y_std:.2f} {units}), ' \\\n",
    "                  f't = {ttest_t:.3f}, df = {x_n-1}, p = {ttest_p:.3f}, Cohen\\'s d = {cohen_d:.2f}.\"')\n",
    "        else:\n",
    "            print()\n",
    "            print(f'\"A paired-samples one-tailed t-test indicated no significant increase in {measure_label} in the [{x_n}] animals between ' \\\n",
    "                  f'{x_label} (M = {x_mean:.2f} {units}, SD = {x_std:.2f} {units}) and ' \\\n",
    "                  f'{y_label} (M = {y_mean:.2f} {units}, SD = {y_std:.2f} {units}), ' \\\n",
    "                  f't = {ttest_t:.3f}, df = {x_n-1}, p = {ttest_p:.3f}, Cohen\\'s d = {cohen_d:.2f}.\"')\n",
    "        \n",
    "        return ttest_signif\n",
    "\n",
    "    else:\n",
    "        print('- Because the differences cannot be assumed to be normal, a Wilcoxon signed rank test will be used')\n",
    "\n",
    "        # paired Wilcoxon signed rank test for increase in locations (medians?)\n",
    "        # - equivalent R test: wilcox.test(x, y, paired=TRUE, alternative=\"greater\")\n",
    "        wilcoxon_result = r_stats.wilcox_test(x.values, y.values, paired=True, alternative='greater')\n",
    "        wilcoxon_W, wilcoxon_p = wilcoxon_result['W'], wilcoxon_result['p']\n",
    "        wilcoxon_signif = '*' if wilcoxon_p < alpha else '(n.s.)'\n",
    "        print(f'H0: Difference in medians is not positive, W = {wilcoxon_W:g},\\tp = {wilcoxon_p:g} {wilcoxon_signif}')\n",
    "        \n",
    "        return wilcoxon_signif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crunch the Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download each file that is not already stored locally\n",
    "metadata = neurotic.MetadataSelector(file=metadata_file)\n",
    "for (animal, food, bout_index), (data_set_name, time_window) in feeding_bouts.items():\n",
    "    metadata.select(data_set_name)\n",
    "    metadata.download_all_data_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "# use Neo RawIO lazy loading to load much faster and using less memory\n",
    "# - with lazy=True, filtering parameters specified in metadata are ignored\n",
    "#     - note: filters are replaced below and applied manually anyway\n",
    "# - with lazy=True, loading via time_slice requires neo>=0.8.0\n",
    "lazy = True\n",
    "\n",
    "# load the metadata containing file paths\n",
    "metadata = neurotic.MetadataSelector(file=metadata_file)\n",
    "\n",
    "# filter epochs for each bout and perform calculations\n",
    "df_list = []\n",
    "last_data_set_name = None\n",
    "pbar = tqdm(total=len(feeding_bouts), unit='feeding bout')\n",
    "for (animal, food, bout_index), (data_set_name, time_window) in feeding_bouts.items():\n",
    "\n",
    "    channel_names = channel_names_by_animal[animal]\n",
    "    epoch_types = epoch_types_by_food[food]\n",
    "\n",
    "    ###\n",
    "    ### LOAD DATASET\n",
    "    ###\n",
    "\n",
    "    metadata.select(data_set_name)\n",
    "\n",
    "    if data_set_name is last_data_set_name:\n",
    "        # skip reloading the data if it's already in memory\n",
    "        pass\n",
    "    else:\n",
    "        blk = neurotic.load_dataset(metadata, lazy=lazy)\n",
    "        if lazy:\n",
    "            # manually perform filters\n",
    "            blk = apply_filters(blk, metadata)\n",
    "\n",
    "    last_data_set_name = data_set_name\n",
    "\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### LOCATE BEHAVIOR EPOCHS AND SUBEPOCHS\n",
    "    ###\n",
    "\n",
    "    # construct a query for locating behaviors\n",
    "    behavior_query = f'(Type in {epoch_types}) & ({time_window[0]} <= Start) & (End <= {time_window[1]})'\n",
    "\n",
    "    # construct queries for locating epochs associated with each behavior\n",
    "    # - each query should match at most one epoch\n",
    "    # - dictionary keys are used as prefixes for the names of new columns\n",
    "    subepoch_queries = {}\n",
    "\n",
    "    subepoch_queries['B38 activity']            = (f'(Type == \"B38 activity\") & ' \\\n",
    "                                                   f'(@behavior_start-3 <= End) & (End <= @behavior_start+4)',\n",
    "                                                   'last') # use last if there are multiple matches\n",
    "                                                  # must end within a few seconds of behavior start (3 before or 4 after)\n",
    "\n",
    "    subepoch_queries['I2 protraction activity'] = f'(Type == \"I2 protraction activity\") & ' \\\n",
    "                                                  f'(@behavior_start-1 <= Start) & (End <= @behavior_end)'\n",
    "                                                  # must start no earlier than 1 second before behavior and end within it\n",
    "\n",
    "    subepoch_queries['B8 activity']             = f'(Type == \"B8 activity\") & ' \\\n",
    "                                                  f'(@behavior_start <= Start) & (End <= @behavior_end)'\n",
    "                                                  # must be fully contained within behavior\n",
    "\n",
    "    subepoch_queries['B3/6/9/10 activity']      = f'(Type == \"B3/6/9/10 activity\") & ' \\\n",
    "                                                  f'(@behavior_start <= Start) & (End <= @behavior_end)'\n",
    "                                                  # must be fully contained within behavior\n",
    "\n",
    "    subepoch_queries['B4/B5 activity']          = (f'(Type == \"B4/B5 activity\") & ' \\\n",
    "                                                   f'(@behavior_start <= Start) & (Start <= @behavior_end)',\n",
    "                                                   'first') # use first if there are multiple matches\n",
    "                                                  # must start within behavior\n",
    "\n",
    "    subepoch_queries['Force shoulder end']      = f'(Type == \"Force shoulder end\") & ' \\\n",
    "                                                  f'(@behavior_start-3 <= End) & (End <= @behavior_start+3)'\n",
    "                                                  # must end within 3 seconds of behavior start\n",
    "\n",
    "    subepoch_queries['Force rise start']        = f'(Type == \"Force rise start\") & ' \\\n",
    "                                                  f'(@behavior_start <= Start) & (Start <= @behavior_end)'\n",
    "                                                  # must start within behavior\n",
    "\n",
    "    subepoch_queries['Force plateau start']     = f'(Type == \"Force plateau start\") & ' \\\n",
    "                                                  f'(@behavior_start <= Start) & (Start <= @behavior_end)'\n",
    "                                                  # must start within behavior\n",
    "\n",
    "    subepoch_queries['Force plateau end']       = f'(Type == \"Force plateau end\") & ' \\\n",
    "                                                  f'(@behavior_end-2 <= Start) & (Start <= @behavior_end+2)'\n",
    "                                                  # must start within 2 seconds of behavior end\n",
    "\n",
    "    subepoch_queries['Force drop end']          = f'(Type == \"Force drop end\") & ' \\\n",
    "                                                  f'(@behavior_end-2 <= Start) & (Start <= @behavior_end+2)'\n",
    "                                                  # must start within 2 seconds of behavior end\n",
    "\n",
    "    subepoch_queries['Inward movement']         = f'(Type == \"Inward movement\") & ' \\\n",
    "                                                  f'(@behavior_start <= Start) & (Start <= @behavior_end)'\n",
    "                                                  # must start within behavior\n",
    "\n",
    "    subepoch_queries['Outward movement']        = f'(Type == \"Outward movement\") & ' \\\n",
    "                                                  f'(@behavior_start <= Start) & (Start <= @behavior_end)'\n",
    "                                                  # must start within behavior\n",
    "\n",
    "    # construct a table in which each row is a behavior and subepoch data\n",
    "    # is added as columns, e.g. df['B38 activity start (s)']\n",
    "    df = BehaviorsDataFrame(blk.segments[0].epochs, behavior_query, subepoch_queries)\n",
    "\n",
    "    # renumber behaviors assuming all behaviors are from a single contiguous sequence\n",
    "    df = df.sort_values('Start (s)').reset_index(drop=True).rename_axis('Behavior_index')\n",
    "\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### START CALCULATIONS\n",
    "    ###\n",
    "\n",
    "    # some columns must have type 'object', which\n",
    "    # can be accomplished by initializing with None or np.nan\n",
    "    df['Video segmentation times (s)'] = None\n",
    "    df['Force segmentation times (s)'] = None\n",
    "    df['Force, video segmented interpolation (mN)'] = None\n",
    "    df['Force, force segmented interpolation (mN)'] = None\n",
    "    for unit in units:\n",
    "        df[f'{unit} spike train'] = None\n",
    "        df[f'{unit} firing rate (Hz)'] = None\n",
    "        df[f'{unit} firing rate, video segmented interpolation (Hz)'] = None\n",
    "        df[f'{unit} firing rate, force segmented interpolation (Hz)'] = None\n",
    "        df[f'{unit} all bursts'] = None\n",
    "\n",
    "        # while we're at it, initialize some other things that might otherwise never be given values\n",
    "        df[f'{unit} burst start (s)'] = np.nan\n",
    "        df[f'{unit} burst end (s)'] = np.nan\n",
    "        df[f'{unit} burst duration (s)'] = 0\n",
    "        df[f'{unit} burst spike count'] = 0\n",
    "        df[f'{unit} burst mean frequency (Hz)'] = 0\n",
    "        df[f'{unit} burst start (video seg normalized)'] = np.nan\n",
    "        df[f'{unit} burst end (video seg normalized)'] = np.nan\n",
    "        df[f'{unit} burst start (force seg normalized)'] = np.nan\n",
    "        df[f'{unit} burst end (force seg normalized)'] = np.nan\n",
    "    df['Inward movement start (video seg normalized)'] = np.nan\n",
    "    df['Inward movement end (video seg normalized)'] = np.nan\n",
    "    df['Inward movement start (force seg normalized)'] = np.nan\n",
    "    df['Inward movement end (force seg normalized)'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "    # get smoothed force for entire time window\n",
    "    channel = 'Force'\n",
    "    sig = get_sig(blk, channel)\n",
    "    if lazy:\n",
    "        sig = sig.time_slice(None, None)\n",
    "    sig = elephant.signal_processing.butter(sig, lowpass_freq = 5*pq.Hz)\n",
    "    sig = sig.time_slice(time_window[0]*pq.s, time_window[1]*pq.s)\n",
    "    sig = sig.rescale('mN')\n",
    "    force_smoothed_sig = sig\n",
    "\n",
    "\n",
    "\n",
    "    df['End to next start (s)'] = np.nan\n",
    "    df['Start to next start (s)'] = np.nan\n",
    "    previous_i = None\n",
    "\n",
    "    # iterate over all swallows\n",
    "    for j, i in enumerate(df.index):\n",
    "\n",
    "        behavior_start = df.loc[i, 'Start (s)']*pq.s\n",
    "        behavior_end = df.loc[i, 'End (s)']*pq.s\n",
    "        inward_movement_start = df.loc[i, 'Inward movement start (s)']*pq.s\n",
    "        inward_movement_end = df.loc[i, 'Inward movement end (s)']*pq.s\n",
    "\n",
    "        # calculate interbehavior intervals assuming all behaviors are from a single contiguous sequence\n",
    "        if previous_i is not None:\n",
    "            df.loc[previous_i, 'End to next start (s)']   = df.loc[i, 'Start (s)'] - df.loc[previous_i, 'End (s)']\n",
    "            df.loc[previous_i, 'Start to next start (s)'] = df.loc[i, 'Start (s)'] - df.loc[previous_i, 'Start (s)']\n",
    "            df.loc[previous_i, 'Inward movement start to next inward movement start (s)'] = \\\n",
    "                df.loc[i, 'Inward movement start (s)'] - df.loc[previous_i, 'Inward movement start (s)']\n",
    "            df.loc[previous_i, 'Inward movement end to next inward movement start (s)'] = \\\n",
    "                df.loc[i, 'Inward movement start (s)'] - df.loc[previous_i, 'Inward movement end (s)']\n",
    "        previous_i = i\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### VIDEO SEGMENTATION\n",
    "        ###\n",
    "\n",
    "        # inward movement start and end are required\n",
    "        video_is_segmented = np.all(np.isfinite(np.array([\n",
    "            inward_movement_start, inward_movement_end])))\n",
    "\n",
    "        if video_is_segmented:\n",
    "            inward_movement_duration = df.loc[i, 'Inward movement duration (s)']*pq.s\n",
    "\n",
    "            # get the list of fixed times for normalization\n",
    "            video_segmentation_times = df.at[i, 'Video segmentation times (s)'] = np.array([\n",
    "                inward_movement_start-inward_movement_duration*4,\n",
    "                inward_movement_start-inward_movement_duration*3,\n",
    "                inward_movement_start-inward_movement_duration*2,\n",
    "                inward_movement_start-inward_movement_duration*1,\n",
    "                inward_movement_start,\n",
    "                inward_movement_end,\n",
    "                inward_movement_end+inward_movement_duration*1,\n",
    "                inward_movement_end+inward_movement_duration*2,\n",
    "                inward_movement_end+inward_movement_duration*3,\n",
    "                inward_movement_end+inward_movement_duration*4,\n",
    "            ])*pq.s # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "        else: # video is not segmented\n",
    "            video_segmentation_times = df.at[i, 'Video segmentation times (s)'] = np.array([\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "            ])*pq.s # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### FORCE SEGMENTATION\n",
    "        ###\n",
    "\n",
    "        force_shoulder_end  = df.loc[i, 'Force shoulder end start (s)']*pq.s  # start of \"Force shoulder end\" epoch\n",
    "        force_rise_start    = df.loc[i, 'Force rise start start (s)']*pq.s    # start of \"Force rise start\" epoch\n",
    "        force_plateau_start = df.loc[i, 'Force plateau start start (s)']*pq.s # start of \"Force plateau start\" epoch\n",
    "        force_plateau_end   = df.loc[i, 'Force plateau end start (s)']*pq.s   # start of \"Force plateau end\" epoch\n",
    "        force_drop_end      = df.loc[i, 'Force drop end start (s)']*pq.s      # start of \"Force drop end\" epoch\n",
    "\n",
    "        # force rise start, plateau start and end, and drop end are required\n",
    "        force_is_segmented = np.all(np.isfinite(np.array([\n",
    "            force_rise_start, force_plateau_start, force_plateau_end, force_drop_end])))\n",
    "\n",
    "        if force_is_segmented:\n",
    "            # get some times for the previous and next swallow\n",
    "            epochs_force_shoulder_end  = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force shoulder end'), None)\n",
    "            epochs_force_rise_start    = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force rise start'), None)\n",
    "            epochs_force_plateau_start = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force plateau start'), None)\n",
    "            epochs_force_plateau_end   = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force plateau end'), None)\n",
    "            epochs_force_drop_end      = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force drop end'), None)\n",
    "            assert epochs_force_shoulder_end  is not None, 'failed to find \"Force shoulder end\" epochs'\n",
    "            assert epochs_force_rise_start    is not None, 'failed to find \"Force rise start\" epochs'\n",
    "            assert epochs_force_plateau_start is not None, 'failed to find \"Force plateau start\" epochs'\n",
    "            assert epochs_force_plateau_end   is not None, 'failed to find \"Force plateau end\" epochs'\n",
    "            assert epochs_force_drop_end      is not None, 'failed to find \"Force drop end\" epochs'\n",
    "\n",
    "            try:\n",
    "                prev_force_plateau_start = df.loc[i, 'Previous force plateau start (s)'] = epochs_force_plateau_start.time_slice(None, force_rise_start)[-1]\n",
    "                assert force_rise_start-prev_force_plateau_start < 16*pq.s, f'for swallow {i}, previous force plateau start is too far away'\n",
    "            except IndexError:\n",
    "                prev_force_plateau_start = df.loc[i, 'Previous force plateau start (s)'] = np.nan\n",
    "\n",
    "            try:\n",
    "                prev_force_plateau_end = df.loc[i, 'Previous force plateau end (s)'] = epochs_force_plateau_end.time_slice(None, force_rise_start)[-1]\n",
    "                assert force_rise_start-prev_force_plateau_end < 12*pq.s, f'for swallow {i}, previous force plateau end is too far away'\n",
    "            except IndexError:\n",
    "                prev_force_plateau_end = df.loc[i, 'Previous force plateau end (s)'] = np.nan\n",
    "\n",
    "            try:\n",
    "                prev_force_drop_end = df.loc[i, 'Previous force drop end (s)'] = epochs_force_drop_end.time_slice(None, force_rise_start)[-1]\n",
    "                assert force_rise_start-prev_force_drop_end < 12*pq.s, f'for swallow {i}, previous force drop end is too far away'\n",
    "            except IndexError:\n",
    "                prev_force_drop_end = df.loc[i, 'Previous force drop end (s)'] = np.nan\n",
    "\n",
    "            try:\n",
    "                next_force_rise_start = df.loc[i, 'Next force rise start (s)'] = epochs_force_rise_start.time_slice(force_drop_end, None)[0]\n",
    "                assert next_force_rise_start-force_drop_end < 12*pq.s, f'for swallow {i}, next force rise start is too far away'\n",
    "            except IndexError:\n",
    "                next_force_rise_start = df.loc[i, 'Next force rise start (s)'] = np.nan\n",
    "\n",
    "            try:\n",
    "                next_force_shoulder_end = df.loc[i, 'Next force shoulder end (s)'] = epochs_force_shoulder_end.time_slice(force_drop_end, None)[0]\n",
    "                if next_force_shoulder_end > next_force_rise_start:\n",
    "                    # next swallow did not have a shoulder and we instead grabbed a later shoulder\n",
    "                    next_force_shoulder_end = df.loc[i, 'Next force shoulder end (s)'] = np.nan\n",
    "            except IndexError:\n",
    "                next_force_shoulder_end = df.loc[i, 'Next force shoulder end (s)'] = np.nan\n",
    "\n",
    "            # get the list of fixed times for normalization\n",
    "            force_segmentation_times = df.at[i, 'Force segmentation times (s)'] = np.array([\n",
    "                prev_force_plateau_start,\n",
    "                prev_force_plateau_end,\n",
    "                prev_force_drop_end,\n",
    "                force_shoulder_end,\n",
    "                force_rise_start,\n",
    "                force_plateau_start,\n",
    "                force_plateau_end,\n",
    "                force_drop_end,\n",
    "                next_force_shoulder_end,\n",
    "                next_force_rise_start,\n",
    "            ])*pq.s # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "        else: # force is not segmented\n",
    "            force_segmentation_times = df.at[i, 'Force segmentation times (s)'] = np.array([\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "                np.nan,\n",
    "            ])*pq.s # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### BEHAVIORAL MARKERS\n",
    "        ###\n",
    "\n",
    "        if video_is_segmented:\n",
    "            df.loc[i, 'Inward movement start (video seg normalized)'] = \\\n",
    "                normalize_time(video_segmentation_times.magnitude, float(inward_movement_start))\n",
    "            df.loc[i, 'Inward movement end (video seg normalized)'] = \\\n",
    "                normalize_time(video_segmentation_times.magnitude, float(inward_movement_end))\n",
    "\n",
    "        if force_is_segmented:\n",
    "            df.loc[i, 'Inward movement start (force seg normalized)'] = \\\n",
    "                normalize_time(force_segmentation_times.magnitude, float(inward_movement_start))\n",
    "            df.loc[i, 'Inward movement end (force seg normalized)'] = \\\n",
    "                normalize_time(force_segmentation_times.magnitude, float(inward_movement_end))\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### FORCE QUANTIFICATION\n",
    "        ###\n",
    "\n",
    "        if force_is_segmented:\n",
    "\n",
    "            # get smoothed force for whole behavior for remaining force calculations\n",
    "            sig = force_smoothed_sig\n",
    "            if np.isfinite(force_shoulder_end):\n",
    "                sig = sig.time_slice(force_shoulder_end - 0.01*pq.s, force_drop_end + 0.01*pq.s)\n",
    "            else:\n",
    "                sig = sig.time_slice(force_rise_start - 1*pq.s, force_drop_end + 0.01*pq.s)\n",
    "            sig = sig.rescale('mN')\n",
    "\n",
    "            # find force peak, baseline, and the increase\n",
    "            force_min_time = df.loc[i, 'Force minimum time (s)'] = elephant.spike_train_generation.peak_detection(sig, 999*pq.mN, sign='below')[0]\n",
    "            force_min = df.loc[i, 'Force minimum (mN)'] = sig[sig.time_index(force_min_time)][0]\n",
    "            force_peak_time = df.loc[i, 'Force peak time (s)'] = elephant.spike_train_generation.peak_detection(sig, 0*pq.mN)[0]\n",
    "            force_peak = df.loc[i, 'Force peak (mN)'] = sig[sig.time_index(force_peak_time)][0]\n",
    "            force_baseline = df.loc[i, 'Force baseline (mN)'] = sig[sig.time_index(force_rise_start)][0]\n",
    "            force_increase = df.loc[i, 'Force increase (mN)'] = force_peak-force_baseline\n",
    "\n",
    "            # find force plateau, drop, and shoulder values\n",
    "            force_plateau_start_value = df.loc[i, 'Force plateau start value (mN)'] = sig[sig.time_index(force_plateau_start)][0]\n",
    "            force_plateau_end_value = df.loc[i, 'Force plateau end value (mN)'] = sig[sig.time_index(force_plateau_end)][0]\n",
    "            force_drop_end_value = df.loc[i, 'Force drop end value (mN)'] = sig[sig.time_index(force_drop_end)][0]\n",
    "            if np.isfinite(force_shoulder_end):\n",
    "                force_shoulder_end_value = df.loc[i, 'Force shoulder end value (mN)'] = sig[sig.time_index(force_shoulder_end)][0]\n",
    "            else:\n",
    "                force_shoulder_end_value = np.nan\n",
    "\n",
    "            # find force rise and plateau durations\n",
    "            force_rise_duration = df.loc[i, 'Force rise duration (s)'] = force_plateau_start - force_rise_start\n",
    "            force_plateau_duration = df.loc[i, 'Force plateau duration (s)'] = force_plateau_end - force_plateau_start\n",
    "            force_rise_plateau_duration = df.loc[i, 'Force rise and plateau duration (s)'] = force_plateau_end - force_rise_start\n",
    "\n",
    "            # find average slope during rising phase\n",
    "            force_rise_increase = df.loc[i, 'Force rise increase (mN)'] = force_plateau_start_value - force_baseline\n",
    "            force_slope = df.loc[i, 'Force slope (mN/s)'] = (force_rise_increase/force_rise_duration).rescale('mN/s')\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### FORCE NORMALIZATION\n",
    "        ###\n",
    "\n",
    "        if force_is_segmented:\n",
    "            t_start = force_segmentation_times[0]-0.001*pq.s\n",
    "            t_stop = force_segmentation_times[-1]+0.001*pq.s\n",
    "\n",
    "            channel = 'Force'\n",
    "            sig = get_sig(blk, channel)\n",
    "            sig = sig.time_slice(t_start, t_stop)\n",
    "            sig = sig.rescale(channel_units[channel_names.index(channel)])\n",
    "\n",
    "            force_force_seg_interp = df.at[i, 'Force, force segmented interpolation (mN)'] = \\\n",
    "                resample_sig_in_normalized_time(force_segmentation_times, sig, force_seg_interp_times) # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### FIND SPIKE TRAINS\n",
    "        ###\n",
    "\n",
    "        if lazy:\n",
    "            if metadata['amplitude_discriminators'] is not None:\n",
    "                for discriminator in metadata['amplitude_discriminators']:\n",
    "                    sig = get_sig(blk, discriminator['channel'])\n",
    "                    if sig is not None:\n",
    "                        sig = sig.time_slice(max(sig.t_start, behavior_start - 5*pq.s), min(sig.t_stop, behavior_end + 5*pq.s))\n",
    "                        st = _detect_spikes(sig, discriminator, blk.segments[0].epochs)\n",
    "                        st_epoch_start = df.loc[i, discriminator['epoch']+' start (s)']*pq.s\n",
    "                        st_epoch_end = df.loc[i, discriminator['epoch']+' end (s)']*pq.s\n",
    "                        st = st.time_slice(st_epoch_start, st_epoch_end)\n",
    "                        df.at[i, f'{st.name} spike train'] = st # 'at', not 'loc', is important for inserting list into cell\n",
    "        else:\n",
    "            for spiketrain in blk.segments[0].spiketrains:\n",
    "                discriminator = next((d for d in metadata['amplitude_discriminators'] if d['name'] == spiketrain.name), None)\n",
    "                if discriminator is None:\n",
    "                    raise Exception(f'For data set \"{data_set_name}\", discriminator \"{spiketrain.name}\" could not be found')\n",
    "                st_epoch_start = df.loc[i, discriminator['epoch']+' start (s)']*pq.s\n",
    "                st_epoch_end = df.loc[i, discriminator['epoch']+' end (s)']*pq.s\n",
    "                if np.isfinite(st_epoch_start) and np.isfinite(st_epoch_end):\n",
    "                    st = spiketrain.time_slice(st_epoch_start, st_epoch_end)\n",
    "                else:\n",
    "                    # this unit's discriminator epoch was not located for this swallow\n",
    "                    st = None\n",
    "                df.at[i, f'{spiketrain.name} spike train'] = st # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### QUANTIFY SPIKE TRAINS AND BURSTS\n",
    "        ###\n",
    "\n",
    "        for k, unit in enumerate(units):\n",
    "            st = df.loc[i, f'{unit} spike train']\n",
    "            if st is not None:\n",
    "\n",
    "                # get the neural channel\n",
    "                channel = st.annotations['channels'][0]\n",
    "                sig = get_sig(blk, channel)\n",
    "\n",
    "                # create a continuous smoothed firing rate representation\n",
    "                # by convolving the spike train with a kernel\n",
    "                # - choice of t_start and t_stop here ensures firing rates are\n",
    "                #   recorded as zero far from the burst and can be resampled later\n",
    "                if video_is_segmented and force_is_segmented:\n",
    "                    t_start = min(video_segmentation_times[0], force_segmentation_times[0])-0.001*pq.s\n",
    "                    t_stop = max(video_segmentation_times[-1], force_segmentation_times[-1])+0.001*pq.s\n",
    "                elif video_is_segmented:\n",
    "                    t_start = video_segmentation_times[0]-0.001*pq.s\n",
    "                    t_stop = video_segmentation_times[-1]+0.001*pq.s\n",
    "                elif force_is_segmented:\n",
    "                    t_start = force_segmentation_times[0]-0.001*pq.s\n",
    "                    t_stop = force_segmentation_times[-1]+0.001*pq.s\n",
    "                else:\n",
    "                    # no segmentation available\n",
    "                    t_start = behavior_start-5*pq.s\n",
    "                    t_stop = behavior_end+5*pq.s\n",
    "                smoothing_kernel = elephant.kernels.GaussianKernel(0.2*pq.s) # 200 ms standard deviation\n",
    "                firing_rate = df.at[i, f'{unit} firing rate (Hz)'] = elephant.statistics.instantaneous_rate(\n",
    "                    spiketrain=st,\n",
    "                    t_start=t_start,\n",
    "                    t_stop=t_stop,\n",
    "                    sampling_period=sig.sampling_period,\n",
    "                    kernel=smoothing_kernel,\n",
    "                ) # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "                # normalization\n",
    "                if video_is_segmented:\n",
    "                    firing_rate_video_seg_interp = df.at[i, f'{unit} firing rate, video segmented interpolation (Hz)'] = \\\n",
    "                        resample_sig_in_normalized_time(video_segmentation_times, firing_rate, video_seg_interp_times) # 'at', not 'loc', is important for inserting list into cell\n",
    "                if force_is_segmented:\n",
    "                    firing_rate_force_seg_interp = df.at[i, f'{unit} firing rate, force segmented interpolation (Hz)'] = \\\n",
    "                        resample_sig_in_normalized_time(force_segmentation_times, firing_rate, force_seg_interp_times) # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "                if st.size > 0:\n",
    "\n",
    "                    # find every sequence of spikes that qualifies as a burst\n",
    "                    burst_thresholds = {d['spiketrain']: d['thresholds']*pq.Hz for d in metadata['burst_detectors']}\n",
    "                    bursts = df.at[i, f'{unit} all bursts'] = _find_bursts(st, burst_thresholds[unit][0], burst_thresholds[unit][1]) # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "                    first_burst_start = np.nan\n",
    "                    last_burst_end = np.nan\n",
    "                    if len(bursts) > 0:\n",
    "\n",
    "                        for burst in zip(bursts.times, bursts.durations, bursts.array_annotations['spikes']):\n",
    "                            if is_good_burst(burst):\n",
    "                                time, duration, n_spikes = burst\n",
    "                                first_burst_start = time\n",
    "                                break # quit after finding first good burst\n",
    "\n",
    "                        for burst in zip(reversed(bursts.times), reversed(bursts.durations), reversed(bursts.array_annotations['spikes'])):\n",
    "                            if is_good_burst(burst):\n",
    "                                time, duration, n_spikes = burst\n",
    "                                last_burst_end = time + duration\n",
    "                                break # quit after finding first (actually, last) good burst\n",
    "\n",
    "                    # merge the first and last good bursts and anything in between\n",
    "                    if np.isfinite(first_burst_start) and np.isfinite(last_burst_end):\n",
    "                        st_burst = st.time_slice(first_burst_start, last_burst_end)\n",
    "                        burst_start = df.loc[i, f'{unit} burst start (s)'] = first_burst_start\n",
    "                        burst_end = df.loc[i, f'{unit} burst end (s)'] = last_burst_end\n",
    "                        burst_duration = df.loc[i, f'{unit} burst duration (s)'] = (burst_end-burst_start)\n",
    "                        burst_spike_count = df.loc[i, f'{unit} burst spike count'] = st_burst.size\n",
    "                        if burst_spike_count > 1:\n",
    "                            burst_mean_freq = df.loc[i, f'{unit} burst mean frequency (Hz)'] = ((burst_spike_count-1)/burst_duration).rescale('Hz')\n",
    "                        if video_is_segmented:\n",
    "                            df.loc[i, f'{unit} burst start (video seg normalized)'] = normalize_time(video_segmentation_times.magnitude, float(first_burst_start))\n",
    "                            df.loc[i, f'{unit} burst end (video seg normalized)']   = normalize_time(video_segmentation_times.magnitude, float(last_burst_end))\n",
    "                        if force_is_segmented:\n",
    "                            df.loc[i, f'{unit} burst start (force seg normalized)'] = normalize_time(force_segmentation_times.magnitude, float(first_burst_start))\n",
    "                            df.loc[i, f'{unit} burst end (force seg normalized)']   = normalize_time(force_segmentation_times.magnitude, float(last_burst_end))\n",
    "\n",
    "        # B3/B6/B9\n",
    "        df['B3/B6/B9 burst start (s)'] = np.nan\n",
    "        df['B3/B6/B9 burst end (s)'] = np.nan\n",
    "        df['B3/B6/B9 burst duration (s)'] = 0\n",
    "        df['B3/B6/B9 burst spike count'] = 0\n",
    "        df['B3/B6/B9 burst mean frequency (Hz)'] = 0\n",
    "        b3b6b9_burst_start = df['B3/B6/B9 burst start (s)'] = df[['B6/B9 burst start (s)', 'B3 burst start (s)']].min(axis=1)\n",
    "        b3b6b9_burst_end = df['B3/B6/B9 burst end (s)']   = df[['B6/B9 burst end (s)',   'B3 burst end (s)']]  .max(axis=1)\n",
    "        b3b6b9_burst_duration = df['B3/B6/B9 burst duration (s)'] = b3b6b9_burst_end - b3b6b9_burst_start\n",
    "        b3b6b9_burst_spike_count = df['B3/B6/B9 burst spike count'] = df['B6/B9 burst spike count'] + df['B3 burst spike count']\n",
    "        b3b6b9_burst_mean_freq = df['B3/B6/B9 burst mean frequency (Hz)'] = (b3b6b9_burst_spike_count-1)/b3b6b9_burst_duration\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### FINISH\n",
    "    ###\n",
    "\n",
    "    # index the table on 4 variables so that this dataframe can later be merged with others\n",
    "    df['Animal'] = animal\n",
    "    df['Food'] = food\n",
    "    df['Bout_index'] = bout_index\n",
    "    df = df.reset_index().set_index(['Animal', 'Food', 'Bout_index', 'Behavior_index'])\n",
    "\n",
    "    df_list += [df]\n",
    "\n",
    "    pbar.update()\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "df_all = pd.concat(df_list, sort=False).sort_index()\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print('elapsed time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤪 Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_sanity_checks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_sanity_checks:\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # use Neo RawIO lazy loading to load much faster and using less memory\n",
    "    # - with lazy=True, filtering parameters specified in metadata are ignored\n",
    "    #     - note: filters are replaced below and applied manually anyway\n",
    "    # - with lazy=True, loading via time_slice requires neo>=0.8.0\n",
    "    lazy = True\n",
    "\n",
    "    # load the metadata containing file paths\n",
    "    metadata = neurotic.MetadataSelector(file=metadata_file)\n",
    "\n",
    "    last_data_set_name = None\n",
    "    pbar = tqdm(total=len(feeding_bouts), unit='figure')\n",
    "    for (animal, food, bout_index), (data_set_name, time_window) in feeding_bouts.items():\n",
    "\n",
    "        channel_names = channel_names_by_animal[animal]\n",
    "        epoch_types = epoch_types_by_food[food]\n",
    "\n",
    "        df = df_all.loc[animal, food, bout_index]\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### LOAD DATASET\n",
    "        ###\n",
    "\n",
    "        metadata.select(data_set_name)\n",
    "\n",
    "        if data_set_name is last_data_set_name:\n",
    "            # skip reloading the data if it's already in memory\n",
    "            pass\n",
    "        else:\n",
    "            blk = neurotic.load_dataset(metadata, lazy=lazy)\n",
    "            if lazy:\n",
    "                # manually perform filters\n",
    "                blk = apply_filters(blk, metadata)\n",
    "\n",
    "        last_data_set_name = data_set_name\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### START FIGURE\n",
    "        ###\n",
    "\n",
    "#         figsize = (9.5, 10) # dimensions for notebook\n",
    "#         figsize = (11, 8.5) # dimensions for printing\n",
    "        figsize = (16, 9) # dimensions for filling wide screens\n",
    "        fig, axes = plt.subplots(len(channel_names), 1, sharex=True, figsize=figsize)\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### PLOT SIGNALS\n",
    "        ###\n",
    "\n",
    "        # plot all channels for entire time window\n",
    "        for i, channel in enumerate(channel_names):\n",
    "            plt.sca(axes[i])\n",
    "            sig = get_sig(blk, channel)\n",
    "            sig = sig.time_slice(time_window[0]*pq.s, time_window[1]*pq.s)\n",
    "            sig = sig.rescale(channel_units[i])\n",
    "            plt.plot(sig.times, sig.magnitude, c='0.8', lw=1, zorder=-1)\n",
    "\n",
    "            if i == 0:\n",
    "                plt.title(f'({animal}, {food}, {bout_index}): {data_set_name}')\n",
    "\n",
    "            plt.ylabel(sig.name + ' (' + sig.units.dimensionality.string + ')')\n",
    "            axes[i].yaxis.set_label_coords(-0.06, 0.5)\n",
    "\n",
    "            if i < len(channel_names)-1:\n",
    "                # remove right, top, and bottom plot borders, and remove x-axis\n",
    "                sns.despine(ax=plt.gca(), bottom=True)\n",
    "                plt.gca().xaxis.set_visible(False)\n",
    "            else:\n",
    "                # remove right and top plot borders, and set x-label\n",
    "                sns.despine(ax=plt.gca())\n",
    "                plt.xlabel('Time (s)')\n",
    "\n",
    "        # plot smoothed force for entire time window\n",
    "        channel = 'Force'\n",
    "        sig = get_sig(blk, channel)\n",
    "        if lazy:\n",
    "            sig = sig.time_slice(None, None)\n",
    "        sig = elephant.signal_processing.butter(sig, lowpass_freq = 5*pq.Hz)\n",
    "        sig = sig.time_slice(time_window[0]*pq.s, time_window[1]*pq.s)\n",
    "        sig = sig.rescale('mN')\n",
    "        plt.plot(sig.times, sig.magnitude, c='k', lw=1, zorder=0)\n",
    "        force_smoothed_sig = sig\n",
    "\n",
    "\n",
    "\n",
    "        # iterate over all swallows\n",
    "        for j, i in enumerate(df.index):\n",
    "\n",
    "            behavior_start = df.loc[i, 'Start (s)']*pq.s\n",
    "            behavior_end   = df.loc[i, 'End (s)']*pq.s\n",
    "\n",
    "            ###\n",
    "            ### MOVEMENTS\n",
    "            ###\n",
    "\n",
    "            # plot inward food movement\n",
    "            inward_movement_start = df.loc[i, 'Inward movement start (s)']*pq.s\n",
    "            inward_movement_end   = df.loc[i, 'Inward movement end (s)']*pq.s\n",
    "            if np.isfinite(inward_movement_start):\n",
    "                channel = 'Force'\n",
    "                ax = axes[channel_names.index(channel)]\n",
    "                ax.axvspan(\n",
    "                    inward_movement_start, inward_movement_end,\n",
    "                    0.99, 1,\n",
    "                    facecolor='k', edgecolor=None, lw=0)\n",
    "\n",
    "            # plot outward food movement\n",
    "            outward_movement_start = df.loc[i, 'Outward movement start (s)']*pq.s\n",
    "            outward_movement_end   = df.loc[i, 'Outward movement end (s)']*pq.s\n",
    "            if np.isfinite(outward_movement_start):\n",
    "                channel = 'Force'\n",
    "                ax = axes[channel_names.index(channel)]\n",
    "                ax.axvspan(\n",
    "                    outward_movement_start, outward_movement_end,\n",
    "                    0.99, 1,\n",
    "                    facecolor='#666666', edgecolor=None, lw=0)\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### FORCE SEGMENTATION\n",
    "            ###\n",
    "\n",
    "            prev_force_drop_end = df.loc[i, 'Previous force drop end (s)']*pq.s   # start of previous \"Force drop end\" epoch\n",
    "            force_shoulder_end  = df.loc[i, 'Force shoulder end start (s)']*pq.s  # start of \"Force shoulder end\" epoch\n",
    "            force_rise_start    = df.loc[i, 'Force rise start start (s)']*pq.s    # start of \"Force rise start\" epoch\n",
    "            force_plateau_start = df.loc[i, 'Force plateau start start (s)']*pq.s # start of \"Force plateau start\" epoch\n",
    "            force_plateau_end   = df.loc[i, 'Force plateau end start (s)']*pq.s   # start of \"Force plateau end\" epoch\n",
    "            force_drop_end      = df.loc[i, 'Force drop end start (s)']*pq.s      # start of \"Force drop end\" epoch\n",
    "\n",
    "            # force rise start, plateau start and end, and drop end are required\n",
    "            force_is_segmented = np.all(np.isfinite(np.array([\n",
    "                force_rise_start, force_plateau_start, force_plateau_end, force_drop_end])))\n",
    "\n",
    "            force_segmentation_times = df.at[i, 'Force segmentation times (s)']\n",
    "\n",
    "            if force_is_segmented:\n",
    "\n",
    "                plt.sca(axes[channel_names.index('Force')])\n",
    "                \n",
    "                force_min_time = df.loc[i, 'Force minimum time (s)']\n",
    "                force_min = df.loc[i, 'Force minimum (mN)']\n",
    "                force_peak_time = df.loc[i, 'Force peak time (s)']\n",
    "                force_peak = df.loc[i, 'Force peak (mN)']\n",
    "                force_baseline = df.loc[i, 'Force baseline (mN)']\n",
    "\n",
    "                force_plateau_start_value = df.loc[i, 'Force plateau start value (mN)']\n",
    "                force_plateau_end_value = df.loc[i, 'Force plateau end value (mN)']\n",
    "                force_drop_end_value = df.loc[i, 'Force drop end value (mN)']\n",
    "                force_shoulder_end_value = df.loc[i, 'Force shoulder end value (mN)']\n",
    "\n",
    "                # get smoothed force for whole behavior for remaining force calculations\n",
    "                sig = force_smoothed_sig\n",
    "                sig = sig.time_slice(prev_force_drop_end - 0.01*pq.s, force_drop_end + 0.01*pq.s)\n",
    "                sig = sig.rescale('mN')\n",
    "\n",
    "                # plot force shoulder in color\n",
    "                if np.isfinite(force_shoulder_end):\n",
    "                    sig2 = sig.time_slice(prev_force_drop_end, force_shoulder_end)\n",
    "                    plt.plot(sig2.times, sig2.magnitude, c=force_colors['shoulder'], lw=2, zorder=1)\n",
    "                    \n",
    "                # plot force rise in color\n",
    "                sig2 = sig.time_slice(force_rise_start, force_plateau_start)\n",
    "                plt.plot(sig2.times, sig2.magnitude, c=force_colors['rise'], lw=2, zorder=1)\n",
    "\n",
    "                # plot force plateau in color\n",
    "                sig2 = sig.time_slice(force_plateau_start, force_plateau_end)\n",
    "                plt.plot(sig2.times, sig2.magnitude, c=force_colors['plateau'], lw=2, zorder=1)\n",
    "\n",
    "                # plot force peak, baseline, and plateau values\n",
    "                plt.plot([force_peak_time],     [force_peak],                marker=CARETDOWN,  markersize=5, color='k')\n",
    "#                 plt.plot([force_min_time],      [force_min],                 marker=CARETUP,    markersize=5, color='k')\n",
    "                plt.plot([force_rise_start],    [force_baseline],            marker=CARETUP,    markersize=5, color='k')\n",
    "                plt.plot([force_plateau_start], [force_plateau_start_value], marker=CARETRIGHT, markersize=5, color='k')\n",
    "                plt.plot([force_plateau_end],   [force_plateau_end_value],   marker=CARETLEFT,  markersize=5, color='k')\n",
    "\n",
    "                # plot segmentation boundaries\n",
    "                for (t, y, c) in [\n",
    "                        (force_shoulder_end,  force_shoulder_end_value,  force_colors['shoulder']),\n",
    "                        (force_rise_start,    force_baseline,            force_colors['rise']),\n",
    "                        (force_plateau_start, force_plateau_start_value, force_colors['plateau']),\n",
    "                        (force_plateau_end,   force_plateau_end_value,   force_colors['plateau']),\n",
    "                        (force_drop_end,      force_drop_end_value,      force_colors['drop'])]:\n",
    "                    if np.isfinite(y):\n",
    "                        axes[-1].add_artist(patches.ConnectionPatch(\n",
    "                            xyA=(t, y), xyB=(t, 0),\n",
    "                            coordsA='data', coordsB=axes[-1].get_xaxis_transform(),\n",
    "                            axesA=axes[-1], axesB=axes[-1],\n",
    "                            color='0.8', lw=1, ls=':', zorder=-2))\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### SPIKES AND BURSTS\n",
    "            ###\n",
    "\n",
    "            for k, unit in enumerate(units):\n",
    "                st = df.loc[i, f'{unit} spike train']\n",
    "                if st is not None and st.size > 0:\n",
    "\n",
    "                    # get the signal for the behavior with 10 seconds cushion for spikes outside the behavior duration\n",
    "                    channel = st.annotations['channels'][0]\n",
    "                    sig = get_sig(blk, channel)\n",
    "                    sig = sig.time_slice(max(sig.t_start, behavior_start - 10*pq.s), min(sig.t_stop, behavior_end + 10*pq.s))\n",
    "                    sig = sig.rescale(channel_units[channel_names.index(channel)])\n",
    "\n",
    "                    # plot spikes\n",
    "                    plt.sca(axes[channel_names.index(channel)])\n",
    "                    marker = ['.', 'x'][j%2] # alternate markers between behaviors\n",
    "                    spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "                    plt.scatter(st.times.rescale('s'), spike_amplitudes, marker=marker, c=unit_colors[unit])\n",
    "\n",
    "                    # plot burst windows\n",
    "                    discriminator = next((d for d in metadata['amplitude_discriminators'] if d['name'] == st.name), None)\n",
    "                    if discriminator is None:\n",
    "                        raise Exception(f'For data set \"{data_set_name}\", discriminator \"{st.name}\" could not be found')\n",
    "                    bottom = pq.Quantity(discriminator['amplitude'][0], discriminator['units']).rescale(sig.units)\n",
    "                    top = pq.Quantity(discriminator['amplitude'][1], discriminator['units']).rescale(sig.units)\n",
    "                    height = top-bottom\n",
    "                    bursts = df.at[i, f'{unit} all bursts']\n",
    "                    for burst in zip(bursts.times, bursts.durations, bursts.array_annotations['spikes']):\n",
    "                        time, duration, n_spikes = burst\n",
    "                        left = time\n",
    "                        right = time + duration\n",
    "                        width = right-left\n",
    "                        linestyle = '-' if is_good_burst(burst) else '--'\n",
    "                        rect = patches.Rectangle((left, bottom), width, height, ls=linestyle, edgecolor=unit_colors[unit], fill=False)\n",
    "                        plt.gca().add_patch(rect)\n",
    "\n",
    "                    # plot markers for edges of bursts\n",
    "                    burst_start = df.loc[i, f'{unit} burst start (s)']\n",
    "                    burst_end = df.loc[i, f'{unit} burst end (s)']\n",
    "                    if top > 0:\n",
    "                        plt.plot([burst_start], [top],    marker=CARETDOWN, markersize=5, color='k')\n",
    "                        plt.plot([burst_end],   [top],    marker=CARETDOWN, markersize=5, color='k')\n",
    "                    else:\n",
    "                        plt.plot([burst_start], [bottom], marker=CARETUP,   markersize=5, color='k')\n",
    "                        plt.plot([burst_end],   [bottom], marker=CARETUP,   markersize=5, color='k')\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### FINISH FIGURE\n",
    "        ###\n",
    "\n",
    "        # optimize plot margins\n",
    "        plt.subplots_adjust(\n",
    "            left   = 0.1,\n",
    "            right  = 0.99,\n",
    "            top    = 0.96,\n",
    "            bottom = 0.06,\n",
    "            hspace = 0.15,\n",
    "        )\n",
    "\n",
    "        # export figure\n",
    "        export_dir2 = os.path.join(export_dir, 'sanity-checks')\n",
    "        if not os.path.exists(export_dir2):\n",
    "            os.mkdir(export_dir2)\n",
    "        plt.gcf().savefig(os.path.join(export_dir2, f'{animal} {food} {bout_index}.png'), dpi=300)\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print('elapsed time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shared plot settings\n",
    "# inches_per_second = 0.5\n",
    "# subplot_height_in_inches = 1.1\n",
    "# top_margin_in_inches = 0.50\n",
    "# bottom_margin_in_inches = 0.39\n",
    "# left_margin_in_inches = 0.58\n",
    "# right_margin_in_inches = 0.8\n",
    "\n",
    "# shared plot settings\n",
    "inches_per_second = 0.75\n",
    "subplot_height_in_inches = 0.5\n",
    "top_margin_in_inches = 0.50\n",
    "bottom_margin_in_inches = 0.39\n",
    "left_margin_in_inches = 0.58\n",
    "right_margin_in_inches = 0.8\n",
    "\n",
    "boolean_fig_width = 5\n",
    "boolean_fig_height = subplot_height_in_inches*4+top_margin_in_inches+bottom_margin_in_inches  # match 4-trace plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "df = df_all.query('Food == \"Swallow\"').reset_index()\n",
    "(data_set_name, time_window) = feeding_bouts[('JG12', 'Swallow', 0)]\n",
    "animal, _, _ = ('JG12', 'Swallow', 0)\n",
    "\n",
    "t_start, t_stop = time_window*pq.s\n",
    "plots = [\n",
    "    {'channel': 'I2',       'units': 'uV', 'ylim': [ -35,  35], 'scalebar': 50}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'RN',       'units': 'uV', 'ylim': [ -25,  25], 'scalebar': 25}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'BN2',      'units': 'uV', 'ylim': [ -45,  45], 'scalebar': 50}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'BN3-DIST', 'units': 'uV', 'ylim': [ -70,  70], 'scalebar': 50, 'ylabel': 'BN3'}, #, 'decimation_factor': 100},\n",
    "#     {'channel': 'Force',    'units': 'mN', 'ylim': [-100, 400], 'scalebar': 200}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'Force',    'units': 'mN', 'ylim': [-20, 400], 'scalebar': 200}, #, 'decimation_factor': 100},\n",
    "]\n",
    "plot_names = [p['channel'] for p in plots]\n",
    "plot_units = [p['units'] for p in plots]\n",
    "\n",
    "fig_height_in_inches, bottom_fraction, top_fraction = solve_figure_vertical_dimensions(\n",
    "    len(plots), subplot_height_in_inches, bottom_margin_in_inches, top_margin_in_inches, 0)\n",
    "fig_width_in_inches, left_fraction, right_fraction = solve_figure_horizontal_dimensions(\n",
    "    1, inches_per_second*(t_stop-t_start).magnitude, left_margin_in_inches, right_margin_in_inches, 0)\n",
    "\n",
    "kwargs = dict(\n",
    "    formats = ['png'],\n",
    "    dpi = 600,\n",
    "    figsize = (fig_width_in_inches, fig_height_in_inches),\n",
    "    linewidth = 0.5,\n",
    "    x_scalebar = 1*pq.s,\n",
    "    layout_settings = dict(\n",
    "        left   = left_fraction,\n",
    "        right  = right_fraction,\n",
    "        bottom = bottom_fraction,\n",
    "        top    = top_fraction,\n",
    "        hspace = 0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# load the metadata\n",
    "metadata = neurotic.MetadataSelector(metadata_file)\n",
    "metadata.select(data_set_name)\n",
    "\n",
    "# load the data\n",
    "blk = neurotic.load_dataset(metadata, lazy=True)\n",
    "\n",
    "# manually perform filters\n",
    "blk = apply_filters(blk, metadata)\n",
    "\n",
    "# plot the signals\n",
    "fig, axes = plot_signals_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "\n",
    "unit_burst_boxes = {\n",
    "    'B38':   [-12, 12],\n",
    "    'I2':    [-30, 25],\n",
    "    'B8a/b': [-20, 12],\n",
    "    'B6/B9': [-20, 15],\n",
    "    'B3':    [-45, 35],\n",
    "    'B4/B5': [-60, 55],\n",
    "}\n",
    "\n",
    "for j, i in enumerate(df.index):\n",
    "    for k, unit in enumerate(units):\n",
    "        st = df.loc[i, f'{unit} spike train']\n",
    "        if st is not None and st.size > 0:\n",
    "\n",
    "            # get the neural channel\n",
    "            channel = st.annotations['channels'][0]\n",
    "            sig = get_sig(blk, channel)\n",
    "\n",
    "            # get the signal for the entire bout\n",
    "            sig = sig.time_slice(t_start, t_stop)\n",
    "            sig = sig.rescale(plot_units[plot_names.index(channel)])\n",
    "\n",
    "            # plot spikes\n",
    "            ax = axes[plot_names.index(channel)]\n",
    "            spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "            ax.scatter(st.times.rescale('s'), spike_amplitudes, marker='.', s=20, c=unit_colors[unit], zorder=3)\n",
    "\n",
    "            # plot burst windows\n",
    "            left = df.loc[i, f'{unit} burst start (s)']\n",
    "            right = df.loc[i, f'{unit} burst end (s)']\n",
    "            if np.isfinite(left) and np.isfinite(right):\n",
    "                width = right-left\n",
    "                bottom, top = unit_burst_boxes[unit]\n",
    "                height = top-bottom\n",
    "                rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor=unit_colors[unit], fill=False, zorder=3, clip_on=False)\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "# plot a zero baseline for force\n",
    "force_zero = -11.5 # mN, average before animal began swallowing, not zero because of DC offset\n",
    "axes[-1].axhline(force_zero, color='0.75', lw=1, ls='--', zorder=-1)\n",
    "\n",
    "# # plot force phase boundaries\n",
    "# times = df.loc[0, 'Force segmentation times (s)'][2:2+6]\n",
    "# for t in times:\n",
    "#     axes[-1].axvline(x=t, ymin=0.15, lw=1, ls=':', c='gray', zorder=-1)\n",
    "\n",
    "# # add arabic numerals for force phase boundaries\n",
    "# axes[-1].annotate('1', xy=(times[0], -0.05), xycoords=('data', 'axes fraction'), ha='center', va='bottom')\n",
    "# axes[-1].annotate('2', xy=(times[1], -0.05), xycoords=('data', 'axes fraction'), ha='center', va='bottom')\n",
    "# axes[-1].annotate('3', xy=(times[2], -0.05), xycoords=('data', 'axes fraction'), ha='center', va='bottom')\n",
    "# axes[-1].annotate('4', xy=(times[3], -0.05), xycoords=('data', 'axes fraction'), ha='center', va='bottom')\n",
    "# axes[-1].annotate('5', xy=(times[4], -0.05), xycoords=('data', 'axes fraction'), ha='center', va='bottom')\n",
    "# axes[-1].annotate('1', xy=(times[5], -0.05), xycoords=('data', 'axes fraction'), ha='center', va='bottom')\n",
    "\n",
    "# # add roman numerals for force phases\n",
    "# axes[-1].annotate('I',   xy=(times[0:2].mean(), 1), xycoords=('data', 'axes fraction'), ha='center', va='top')\n",
    "# axes[-1].annotate('II',  xy=(times[1:3].mean(), 1), xycoords=('data', 'axes fraction'), ha='center', va='top')\n",
    "# axes[-1].annotate('III', xy=(times[2:4].mean(), 1), xycoords=('data', 'axes fraction'), ha='center', va='top')\n",
    "# axes[-1].annotate('IV',  xy=(times[3:5].mean(), 1), xycoords=('data', 'axes fraction'), ha='center', va='top')\n",
    "# axes[-1].annotate('V',   xy=(times[4:6].mean(), 1), xycoords=('data', 'axes fraction'), ha='center', va='top')\n",
    "\n",
    "# add unit name labels\n",
    "axes[2].annotate('B38',   xy=(2978.15, 0.70), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B38'])\n",
    "axes[0].annotate('I2',    xy=(2979.40, 0.75), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['I2'])\n",
    "axes[1].annotate('B8a/b', xy=(2981.10, 0.80), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B8a/b'])\n",
    "axes[2].annotate('B6/B9', xy=(2980.10, 0.73), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B6/B9'])\n",
    "axes[2].annotate('B3',    xy=(2981.85, 0.79), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['B3'])\n",
    "axes[3].annotate('B4/B5', xy=(2979.10, 0.80), xycoords=('data', 'axes fraction'), ha='right',  c=unit_colors['B4/B5'])\n",
    "\n",
    "# add protraction box\n",
    "left, right = df.loc[0, ['I2 burst start (s)', 'I2 burst end (s)']]\n",
    "# bottom, top = (1, 1.15)\n",
    "bottom, top = (1.2, 1.5)\n",
    "width = right-left\n",
    "height = top-bottom\n",
    "rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', fill=False, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Prot.', xy=((left+right)/2, 1.065), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "axes[0].annotate('Prot.', xy=((left+right)/2, bottom+0.12), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "\n",
    "# add retraction box\n",
    "left, right = df.loc[0, ['I2 burst end (s)', 'End (s)']] # behavior ends with end of B43 burst\n",
    "# bottom, top = (1, 1.15)\n",
    "bottom, top = (1.2, 1.5)\n",
    "width = right-left\n",
    "height = top-bottom\n",
    "rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', facecolor='k', fill=True, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Retraction', xy=((left+right)/2, 1.065), xycoords=('data', 'axes fraction'), ha='center', va='center', c='w', fontsize='small')\n",
    "axes[0].annotate('Retraction', xy=((left+right)/2, bottom+0.12), xycoords=('data', 'axes fraction'), ha='center', va='center', c='w', fontsize='small')\n",
    "\n",
    "# add inward movement box\n",
    "left, right = df.loc[0, ['Inward movement start (s)', 'Inward movement end (s)']]\n",
    "# bottom, top = (1.20, 1.35)\n",
    "bottom, top = (1.6, 1.9)\n",
    "width = right-left\n",
    "height = top-bottom\n",
    "rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', facecolor='0.75', fill=True, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Inward', xy=((left+right)/2, 1.260), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "axes[0].annotate('Inward', xy=((left+right)/2, bottom+0.12), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'swallow.png'), dpi=600)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print('render time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query('Food == \"Swallow\"').reset_index()\n",
    "(data_set_name, time_window) = feeding_bouts[('JG12', 'Swallow', 0)]\n",
    "animal, _, _ = ('JG12', 'Swallow', 0)\n",
    "t_start, t_stop = time_window*pq.s\n",
    "\n",
    "fig, axes = plt.subplots(len(units), 1, sharex=True, figsize=(boolean_fig_width, boolean_fig_height))\n",
    "\n",
    "for j, i in enumerate(df.index):\n",
    "    for k, unit in enumerate(units):\n",
    "        ax = axes[k]\n",
    "        \n",
    "        burst_start, burst_end = df.loc[i, [f'{unit} burst start (s)', f'{unit} burst end (s)']]\n",
    "        if np.isfinite(burst_start):\n",
    "            x = np.array([t_start, burst_start, burst_end, t_stop])\n",
    "            y = np.array([0, 0, 1, 0])\n",
    "        else:\n",
    "            x = np.array([t_start, t_stop])\n",
    "            y = np.array([0, 0])\n",
    "        \n",
    "        ax.step(x, y, unit_colors[unit])\n",
    "        ax.set_ylim([-0.1, 1.1])\n",
    "        ax.set_ylabel(unit, c=unit_colors[unit], rotation=0, ha='right', va='center')\n",
    "        \n",
    "        # hide the box around the subplot\n",
    "        ax.set_frame_on(False)\n",
    "        \n",
    "        # disable tick marks\n",
    "        ax.tick_params(\n",
    "            bottom=False,\n",
    "            left=False,\n",
    "            labelbottom=False,\n",
    "            labelleft=False)\n",
    "\n",
    "add_scalebar(axes[-1],\n",
    "    sizex=1,\n",
    "    labelx='1 s',\n",
    "\n",
    "    loc='upper right',\n",
    "    bbox_to_anchor=(1, 0),\n",
    "\n",
    "    borderpad=1,\n",
    "    sep=5,\n",
    "    barwidth=2,\n",
    ")\n",
    "\n",
    "fig.tight_layout(h_pad=0.5, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'swallow-boolean.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "df = df_all.query('Food == \"Bite\"').reset_index()\n",
    "(data_set_name, time_window) = feeding_bouts[('JG12', 'Bite', 0)]\n",
    "animal, _, _ = ('JG12', 'Bite', 0)\n",
    "\n",
    "t_start, t_stop = time_window*pq.s\n",
    "plots = [\n",
    "    {'channel': 'I2',       'units': 'uV', 'ylim': [ -80,  80], 'scalebar': 50}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'RN',       'units': 'uV', 'ylim': [ -30,  30], 'scalebar': 25}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'BN2',      'units': 'uV', 'ylim': [ -45,  45], 'scalebar': 50}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'BN3-DIST', 'units': 'uV', 'ylim': [ -70,  70], 'scalebar': 50, 'ylabel': 'BN3'}, #, 'decimation_factor': 100},\n",
    "]\n",
    "plot_names = [p['channel'] for p in plots]\n",
    "plot_units = [p['units'] for p in plots]\n",
    "\n",
    "fig_height_in_inches, bottom_fraction, top_fraction = solve_figure_vertical_dimensions(\n",
    "    len(plots), subplot_height_in_inches, bottom_margin_in_inches, top_margin_in_inches, 0)\n",
    "fig_width_in_inches, left_fraction, right_fraction = solve_figure_horizontal_dimensions(\n",
    "    1, inches_per_second*(t_stop-t_start).magnitude, left_margin_in_inches, right_margin_in_inches, 0)\n",
    "\n",
    "kwargs = dict(\n",
    "    formats = ['png'],\n",
    "    dpi = 600,\n",
    "    figsize = (fig_width_in_inches, fig_height_in_inches),\n",
    "    linewidth = 0.5,\n",
    "    x_scalebar = 1*pq.s,\n",
    "    layout_settings = dict(\n",
    "        left   = left_fraction,\n",
    "        right  = right_fraction,\n",
    "        bottom = bottom_fraction,\n",
    "        top    = top_fraction,\n",
    "        hspace = 0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# load the metadata\n",
    "metadata = neurotic.MetadataSelector(metadata_file)\n",
    "metadata.select(data_set_name)\n",
    "\n",
    "# load the data\n",
    "blk = neurotic.load_dataset(metadata, lazy=True)\n",
    "\n",
    "# manually perform filters\n",
    "blk = apply_filters(blk, metadata)\n",
    "\n",
    "# plot the signals\n",
    "fig, axes = plot_signals_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "\n",
    "unit_burst_boxes = {\n",
    "    'B38':   [-12, 12],\n",
    "    'I2':    [-70, 72],\n",
    "    'B8a/b': [-28, 19],\n",
    "    'B6/B9': [-20, 15],\n",
    "    'B3':    [-45, 35],\n",
    "    'B4/B5': [-60, 55],\n",
    "}\n",
    "\n",
    "for j, i in enumerate(df.index):\n",
    "    for k, unit in enumerate(units):\n",
    "        st = df.loc[i, f'{unit} spike train']\n",
    "        if st is not None and st.size > 0:\n",
    "\n",
    "            # get the neural channel\n",
    "            channel = st.annotations['channels'][0]\n",
    "            sig = get_sig(blk, channel)\n",
    "\n",
    "            # get the signal for the entire bout\n",
    "            sig = sig.time_slice(t_start, t_stop)\n",
    "            sig = sig.rescale(plot_units[plot_names.index(channel)])\n",
    "\n",
    "            # plot spikes\n",
    "            ax = axes[plot_names.index(channel)]\n",
    "            spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "            ax.scatter(st.times.rescale('s'), spike_amplitudes, marker='.', s=20, c=unit_colors[unit], zorder=3)\n",
    "\n",
    "            # plot burst windows\n",
    "            left = df.loc[i, f'{unit} burst start (s)']\n",
    "            right = df.loc[i, f'{unit} burst end (s)']\n",
    "            if np.isfinite(left) and np.isfinite(right):\n",
    "                width = right-left\n",
    "                bottom, top = unit_burst_boxes[unit]\n",
    "                height = top-bottom\n",
    "                rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor=unit_colors[unit], fill=False, zorder=3, clip_on=False)\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "# # add unit name labels\n",
    "# axes[2].annotate('B38',   xy=(2978.15, 0.70), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B38'])\n",
    "axes[0].annotate('I2',    xy=(2172.15, 0.75), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['I2'])\n",
    "axes[1].annotate('B8a/b', xy=(2172.20, 0.85), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B8a/b'])\n",
    "axes[2].annotate('B6/B9', xy=(2173.00, 0.20), xycoords=('data', 'axes fraction'), ha='right',  c=unit_colors['B6/B9'])\n",
    "# axes[2].annotate('B3',    xy=(2981.85, 0.79), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['B3'])\n",
    "axes[3].annotate('B4/B5', xy=(2171.80, 0.80), xycoords=('data', 'axes fraction'), ha='right',  c=unit_colors['B4/B5'])\n",
    "\n",
    "# add protraction box\n",
    "left, right = df.loc[0, ['I2 burst start (s)', 'I2 burst end (s)']]\n",
    "# bottom, top = (1, 1.15)\n",
    "bottom, top = (1.2, 1.5)\n",
    "width = right-left\n",
    "height = top-bottom\n",
    "rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', fill=False, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Prot.', xy=((left+right)/2, 1.065), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "axes[0].annotate('Protraction', xy=((left+right)/2, bottom+0.12), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "\n",
    "# add retraction box\n",
    "left, right = df.loc[0, ['I2 burst end (s)', 'End (s)']] # behavior ends with end of B43 burst\n",
    "# bottom, top = (1, 1.15)\n",
    "bottom, top = (1.2, 1.5)\n",
    "width = right-left\n",
    "height = top-bottom\n",
    "rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', facecolor='k', fill=True, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Retraction', xy=((left+right)/2, 1.065), xycoords=('data', 'axes fraction'), ha='center', va='center', c='w', fontsize='small')\n",
    "axes[0].annotate('Retraction', xy=((left+right)/2, bottom+0.12), xycoords=('data', 'axes fraction'), ha='center', va='center', c='w', fontsize='small')\n",
    "\n",
    "# # add inward movement box\n",
    "# left, right = df.loc[0, ['Inward movement start (s)', 'Inward movement end (s)']]\n",
    "# bottom, top = (1.20, 1.35)\n",
    "# width = right-left\n",
    "# height = top-bottom\n",
    "# rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', facecolor='0.75', fill=True, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "# axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Inward', xy=((left+right)/2, 1.260), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'bite.png'), dpi=600)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print('render time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query('Food == \"Bite\"').reset_index()\n",
    "(data_set_name, time_window) = feeding_bouts[('JG12', 'Bite', 0)]\n",
    "animal, _, _ = ('JG12', 'Bite', 0)\n",
    "t_start, t_stop = time_window*pq.s\n",
    "\n",
    "fig, axes = plt.subplots(len(units), 1, sharex=True, figsize=(boolean_fig_width, boolean_fig_height))\n",
    "\n",
    "for j, i in enumerate(df.index):\n",
    "    for k, unit in enumerate(units):\n",
    "        ax = axes[k]\n",
    "        \n",
    "        burst_start, burst_end = df.loc[i, [f'{unit} burst start (s)', f'{unit} burst end (s)']]\n",
    "        if np.isfinite(burst_start):\n",
    "            x = np.array([t_start, burst_start, burst_end, t_stop])\n",
    "            y = np.array([0, 0, 1, 0])\n",
    "        else:\n",
    "            x = np.array([t_start, t_stop])\n",
    "            y = np.array([0, 0])\n",
    "        \n",
    "        ax.step(x, y, unit_colors[unit])\n",
    "        ax.set_ylim([-0.1, 1.1])\n",
    "        ax.set_ylabel(unit, c=unit_colors[unit], rotation=0, ha='right', va='center')\n",
    "        \n",
    "        # hide the box around the subplot\n",
    "        ax.set_frame_on(False)\n",
    "        \n",
    "        # disable tick marks\n",
    "        ax.tick_params(\n",
    "            bottom=False,\n",
    "            left=False,\n",
    "            labelbottom=False,\n",
    "            labelleft=False)\n",
    "\n",
    "add_scalebar(axes[-1],\n",
    "    sizex=1,\n",
    "    labelx='1 s',\n",
    "\n",
    "    loc='upper right',\n",
    "    bbox_to_anchor=(1, 0),\n",
    "\n",
    "    borderpad=1,\n",
    "    sep=5,\n",
    "    barwidth=2,\n",
    ")\n",
    "\n",
    "fig.tight_layout(h_pad=0.5, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'bite-boolean.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "df = df_all.query('Food == \"Rejection\"').reset_index()\n",
    "(data_set_name, time_window) = feeding_bouts[('JG12', 'Rejection', 0)]\n",
    "animal, _, _ = ('JG12', 'Rejection', 0)\n",
    "\n",
    "# t_start, t_stop = time_window*pq.s\n",
    "t_start, t_stop = [1429.3, 1438.6]*pq.s\n",
    "plots = [\n",
    "    {'channel': 'I2',       'units': 'uV', 'ylim': [ -50,  50], 'scalebar': 50}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'RN',       'units': 'uV', 'ylim': [ -25,  25], 'scalebar': 25}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'BN2',      'units': 'uV', 'ylim': [ -45,  35], 'scalebar': 50}, #, 'decimation_factor': 100},\n",
    "    {'channel': 'BN3-DIST', 'units': 'uV', 'ylim': [ -70,  70], 'scalebar': 50, 'ylabel': 'BN3'}, #, 'decimation_factor': 100},\n",
    "]\n",
    "plot_names = [p['channel'] for p in plots]\n",
    "plot_units = [p['units'] for p in plots]\n",
    "\n",
    "fig_height_in_inches, bottom_fraction, top_fraction = solve_figure_vertical_dimensions(\n",
    "    len(plots), subplot_height_in_inches, bottom_margin_in_inches, top_margin_in_inches, 0)\n",
    "fig_width_in_inches, left_fraction, right_fraction = solve_figure_horizontal_dimensions(\n",
    "    1, inches_per_second*(t_stop-t_start).magnitude, left_margin_in_inches, right_margin_in_inches, 0)\n",
    "\n",
    "kwargs = dict(\n",
    "    formats = ['png'],\n",
    "    dpi = 600,\n",
    "    figsize = (fig_width_in_inches, fig_height_in_inches),\n",
    "    linewidth = 0.5,\n",
    "    x_scalebar = 1*pq.s,\n",
    "    layout_settings = dict(\n",
    "        left   = left_fraction,\n",
    "        right  = right_fraction,\n",
    "        bottom = bottom_fraction,\n",
    "        top    = top_fraction,\n",
    "        hspace = 0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# load the metadata\n",
    "metadata = neurotic.MetadataSelector(metadata_file)\n",
    "metadata.select(data_set_name)\n",
    "\n",
    "# load the data\n",
    "blk = neurotic.load_dataset(metadata, lazy=True)\n",
    "\n",
    "# manually perform filters\n",
    "blk = apply_filters(blk, metadata)\n",
    "\n",
    "# plot the signals\n",
    "fig, axes = plot_signals_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "\n",
    "unit_burst_boxes = {\n",
    "    'B38':   [-12, 12],\n",
    "    'I2':    [-30, 38],\n",
    "    'B8a/b': [-23, 15],\n",
    "    'B6/B9': [-20, 15],\n",
    "    'B3':    [-40, 29],\n",
    "    'B4/B5': [-60, 55],\n",
    "}\n",
    "\n",
    "for j, i in enumerate(df.index):\n",
    "    for k, unit in enumerate(units):\n",
    "        st = df.loc[i, f'{unit} spike train']\n",
    "        if st is not None and st.size > 0:\n",
    "\n",
    "            # get the neural channel\n",
    "            channel = st.annotations['channels'][0]\n",
    "            sig = get_sig(blk, channel)\n",
    "\n",
    "            # get the signal for the entire bout\n",
    "            sig = sig.time_slice(t_start, t_stop)\n",
    "            sig = sig.rescale(plot_units[plot_names.index(channel)])\n",
    "\n",
    "            # plot spikes\n",
    "            ax = axes[plot_names.index(channel)]\n",
    "            spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "            ax.scatter(st.times.rescale('s'), spike_amplitudes, marker='.', s=20, c=unit_colors[unit], zorder=3)\n",
    "\n",
    "            # plot burst windows\n",
    "            left = df.loc[i, f'{unit} burst start (s)']\n",
    "            right = df.loc[i, f'{unit} burst end (s)']\n",
    "            if np.isfinite(left) and np.isfinite(right):\n",
    "                width = right-left\n",
    "                bottom, top = unit_burst_boxes[unit]\n",
    "                height = top-bottom\n",
    "                rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor=unit_colors[unit], fill=False, zorder=3, clip_on=False)\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "# add unit name labels\n",
    "axes[2].annotate('B38',   xy=(1430.20, 0.70), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B38'])\n",
    "axes[0].annotate('I2',    xy=(1434.35, 0.75), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['I2'])\n",
    "axes[1].annotate('B8a/b', xy=(1431.60, 0.85), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B8a/b'])\n",
    "axes[2].annotate('B6/B9', xy=(1436.20, 0.73), xycoords=('data', 'axes fraction'), ha='right',  c=unit_colors['B6/B9'])\n",
    "axes[2].annotate('B3',    xy=(1437.80, 0.79), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['B3'])\n",
    "axes[3].annotate('B4/B5', xy=(1432.40, 0.80), xycoords=('data', 'axes fraction'), ha='right',  c=unit_colors['B4/B5'])\n",
    "\n",
    "# add protraction box\n",
    "left, right = df.loc[0, ['I2 burst start (s)', 'I2 burst end (s)']]\n",
    "# bottom, top = (1, 1.15)\n",
    "bottom, top = (1.2, 1.5)\n",
    "width = right-left\n",
    "height = top-bottom\n",
    "rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', fill=False, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Protraction', xy=((left+right)/2, 1.065), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "axes[0].annotate('Protraction', xy=((left+right)/2, bottom+0.12), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "\n",
    "# add retraction box\n",
    "# left, right = df.loc[0, ['I2 burst end (s)', 'End (s)']] # behavior ends with end of B43 burst\n",
    "left, right = df.loc[0, 'I2 burst end (s)'], t_stop.magnitude # behavior ends with end of B43 burst\n",
    "# bottom, top = (1, 1.15)\n",
    "bottom, top = (1.2, 1.5)\n",
    "width = right-left\n",
    "height = top-bottom\n",
    "rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', facecolor='k', fill=True, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Retraction', xy=((left+right)/2, 1.065), xycoords=('data', 'axes fraction'), ha='center', va='center', c='w', fontsize='small')\n",
    "axes[0].annotate('Retraction', xy=((left+right)/2, bottom+0.12), xycoords=('data', 'axes fraction'), ha='center', va='center', c='w', fontsize='small')\n",
    "\n",
    "# # add inward movement box\n",
    "# left, right = df.loc[0, ['Inward movement start (s)', 'Inward movement end (s)']]\n",
    "# bottom, top = (1.20, 1.35)\n",
    "# width = right-left\n",
    "# height = top-bottom\n",
    "# rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', facecolor='0.75', fill=True, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "# axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Inward', xy=((left+right)/2, 1.260), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "\n",
    "# add outward movement box\n",
    "left, right = df.loc[0, ['Outward movement start (s)', 'Outward movement end (s)']]\n",
    "# bottom, top = (1.20, 1.35)\n",
    "bottom, top = (1.6, 1.9)\n",
    "width = right-left\n",
    "height = top-bottom\n",
    "rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor='k', facecolor='0.75', fill=True, zorder=3, clip_on=False, transform=axes[0].get_xaxis_transform())\n",
    "axes[0].add_patch(rect)\n",
    "# axes[0].annotate('Outward', xy=((left+right)/2, 1.260), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "axes[0].annotate('Outward', xy=((left+right)/2, bottom+0.12), xycoords=('data', 'axes fraction'), ha='center', va='center', c='k', fontsize='small')\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'rejection.png'), dpi=600)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print('render time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query('Food == \"Rejection\"').reset_index()\n",
    "(data_set_name, time_window) = feeding_bouts[('JG12', 'Rejection', 0)]\n",
    "animal, _, _ = ('JG12', 'Rejection', 0)\n",
    "# t_start, t_stop = time_window*pq.s\n",
    "t_start, t_stop = [1429.3, 1438.6]*pq.s\n",
    "\n",
    "fig, axes = plt.subplots(len(units), 1, sharex=True, figsize=(boolean_fig_width, boolean_fig_height))\n",
    "\n",
    "for j, i in enumerate(df.index):\n",
    "    for k, unit in enumerate(units):\n",
    "        ax = axes[k]\n",
    "        \n",
    "        burst_start, burst_end = df.loc[i, [f'{unit} burst start (s)', f'{unit} burst end (s)']]\n",
    "        if np.isfinite(burst_start):\n",
    "            x = np.array([t_start, burst_start, burst_end, t_stop])\n",
    "            y = np.array([0, 0, 1, 0])\n",
    "        else:\n",
    "            x = np.array([t_start, t_stop])\n",
    "            y = np.array([0, 0])\n",
    "        \n",
    "        ax.step(x, y, unit_colors[unit])\n",
    "        ax.set_ylim([-0.1, 1.1])\n",
    "        ax.set_ylabel(unit, c=unit_colors[unit], rotation=0, ha='right', va='center')\n",
    "        \n",
    "        # hide the box around the subplot\n",
    "        ax.set_frame_on(False)\n",
    "        \n",
    "        # disable tick marks\n",
    "        ax.tick_params(\n",
    "            bottom=False,\n",
    "            left=False,\n",
    "            labelbottom=False,\n",
    "            labelleft=False)\n",
    "\n",
    "add_scalebar(axes[-1],\n",
    "    sizex=1,\n",
    "    labelx='1 s',\n",
    "\n",
    "    loc='upper right',\n",
    "    bbox_to_anchor=(1, 0),\n",
    "\n",
    "    borderpad=1,\n",
    "    sep=5,\n",
    "    barwidth=2,\n",
    ")\n",
    "\n",
    "fig.tight_layout(h_pad=0.5, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'rejection-boolean.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
