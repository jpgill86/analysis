{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, time\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pyqtgraph as pg\n",
    "import tridesclous as tdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '../ffs-ignore/spike-sorting-working-dir'\n",
    "# filenames = ['../ffs-ignore/2018-06-21_IN-VIVO_JG-08 002.axgd']\n",
    "# channel_groups = {0: {\"channels\": [3], \"geometry\": {3: [0, 0]}}} # BN3 only\n",
    "filenames = ['../ffs-ignore/2018-06-21_IN-VIVO_JG-08 002 BN3-Only-Artifacts-Removed.axgx']\n",
    "\n",
    "# delete prior workspace\n",
    "if os.path.exists(dirname):\n",
    "    shutil.rmtree(dirname)\n",
    "\n",
    "dataio = tdc.DataIO(dirname = dirname)\n",
    "dataio.set_data_source(type = 'Axograph', filenames = filenames)\n",
    "# dataio.set_channel_groups(channel_groups)\n",
    "print(dataio)\n",
    "\n",
    "cc = tdc.CatalogueConstructor(dataio = dataio)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc_params = {\n",
    "    'fullchain_kargs' : {\n",
    "        'duration' : 300.,\n",
    "        'preprocessor' : {\n",
    "            'highpass_freq' : None,\n",
    "            'lowpass_freq' : None,\n",
    "            'chunksize' : 4096,\n",
    "            'lostfront_chunksize' : 64,\n",
    "        },\n",
    "        'peak_detector' : {\n",
    "            'peak_sign' : '-',\n",
    "#             'relative_threshold' : 100,\n",
    "            'relative_threshold' : 5,\n",
    "            'peak_span' : 0.0002,\n",
    "        },\n",
    "        'noise_snippet' : {\n",
    "            'nb_snippet' : 300,\n",
    "        },\n",
    "        'extract_waveforms' : {\n",
    "            'n_left' : -20,\n",
    "            'n_right' : 30,\n",
    "            'mode' : 'rand',\n",
    "            'nb_max' : 20000,\n",
    "            'align_waveform' : False,\n",
    "        },\n",
    "        'clean_waveforms' : {\n",
    "            'alien_value_threshold' : 400., # relative threshold for discarding spikes\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'feat_method' : 'global_pca',\n",
    "    'feat_kargs' : {'n_components': 5},\n",
    "\n",
    "    'clust_method' : 'sawchaincut',\n",
    "    'clust_kargs' : {},\n",
    "#     'clust_method' : 'gmm',\n",
    "#     'clust_kargs' : {\n",
    "#         'n_clusters' : 3,\n",
    "#         'covariance_type' : 'full',\n",
    "#         'n_init' : 10,\n",
    "#     },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create initial catalogue all at once ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc.apply_all_catalogue_steps(cc, **tdc_params)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... or step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set parameters for filter and peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.set_preprocessor_params(**tdc_params['fullchain_kargs']['preprocessor'], **tdc_params['fullchain_kargs']['peak_detector'])\n",
    "\n",
    "# # cc.set_preprocessor_params(\n",
    "# #     chunksize = chunksize,\n",
    "# #     lostfront_chunksize = 64,#1,#None,#64,\n",
    "    \n",
    "# #     highpass_freq = None,#14.,\n",
    "# #     lowpass_freq  = None,#100000.,\n",
    "    \n",
    "# #     peak_sign = '-',\n",
    "# #     relative_threshold = 100,\n",
    "# #     peak_span = 0.0002,\n",
    "# # )\n",
    "\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate background noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.estimate_signals_noise(seg_num=0, duration=min(10., tdc_params['fullchain_kargs']['duration'], dataio.get_segment_length(seg_num=0)/dataio.sample_rate*.99))\n",
    "\n",
    "# # cc.estimate_signals_noise(seg_num=0, duration=15.)\n",
    "# # cc.estimate_signals_noise(seg_num=0, duration=10.)\n",
    "\n",
    "# print(cc.signals_medians)\n",
    "# print(cc.signals_mads)\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the filter and peak detection on a data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.run_signalprocessor(duration=tdc_params['fullchain_kargs']['duration'])\n",
    "\n",
    "# # cc.run_signalprocessor(duration=60.)\n",
    "# # cc.run_signalprocessor(duration=300.)\n",
    "\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract waveforms around the detected peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.extract_some_waveforms(**tdc_params['fullchain_kargs']['extract_waveforms'])\n",
    "\n",
    "# # cc.extract_some_waveforms(n_left=-25, n_right=40, mode='rand', nb_max=10000, align_waveform=True)\n",
    "# # cc.extract_some_waveforms(n_left=-20, n_right=30, mode='rand', nb_max=20000, align_waveform=False)\n",
    "\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discard some bad waveforms (artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.clean_waveforms(**tdc_params['fullchain_kargs']['clean_waveforms'])\n",
    "\n",
    "# # cc.clean_waveforms(alien_value_threshold=400.) # relative threshold for discarding spikes\n",
    "\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shorten or extend waveforms based on amplitude above noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_left, n_right = cc.find_good_limits(mad_threshold = 1.1,)\n",
    "\n",
    "# print(n_left, n_right)\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract noise samples for comparison to spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.extract_some_noise(**tdc_params['fullchain_kargs']['noise_snippet'])\n",
    "\n",
    "# # cc.extract_some_noise(nb_snippet = 300)\n",
    "\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract spike features (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.extract_some_features(method=tdc_params['feat_method'], **tdc_params['feat_kargs'])\n",
    "\n",
    "# # cc.extract_some_features(method='global_pca', n_components=5)\n",
    "# # cc.extract_some_features(method='peak_max')\n",
    "\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster spikes based on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc.find_clusters(method=tdc_params['clust_method'], **tdc_params['clust_kargs'])\n",
    "\n",
    "# # cc.find_clusters(method='kmeans', n_clusters=12)\n",
    "# # cc.find_clusters(method='gmm', n_clusters=5, covariance_type='full', n_init=10)\n",
    "# # cc.find_clusters(method='gmm', n_clusters=3, covariance_type='full', n_init=10)\n",
    "\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview and manually merge/split/delete clusters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must click \"Make catalogue for peeler\" when finished!\n",
    "app = pg.mkQApp()\n",
    "win = tdc.CatalogueWindow(cc)\n",
    "win.traceviewer.params['xsize_max'] = 300.0  # increase upper bound on time zoom\n",
    "win.traceviewer.params['zoom_size'] = 30.0   # increase amount of time plotted after clicking on a spike\n",
    "win.traceviewer.spinbox_xsize.setValue(30.0) # increase amount of time plotted initially\n",
    "win.traceviewer.gain_zoom(50)                # increase amount of voltage plotted initially\n",
    "win.show()\n",
    "app.exec_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... or merge/split/delete clusters programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #order cluster by waveforms rms\n",
    "# cc.order_clusters(by='waveforms_rms')\n",
    "\n",
    "# #put labels to trash\n",
    "# # mask = (\n",
    "# #     cc.all_peaks['cluster_label'] == 0 or\n",
    "# #     cc.all_peaks['cluster_label'] == 1 or\n",
    "# #     cc.all_peaks['cluster_label'] == 2 or\n",
    "# #     cc.all_peaks['cluster_label'] == 3\n",
    "# # )\n",
    "# # mask = cc.all_peaks['cluster_label'] != 4\n",
    "# # cc.all_peaks['cluster_label'][mask] = -1\n",
    "# # cc.on_new_cluster()\n",
    "\n",
    "# #save the catalogue\n",
    "# cc.make_catalogue_for_peeler()\n",
    "\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Peeler: classify spikes in the full dataset using template matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_catalogue = dataio.load_catalogue(chan_grp=0)\n",
    "if initial_catalogue is not None:\n",
    "    print(cc)\n",
    "\n",
    "    peeler = tdc.Peeler(dataio)\n",
    "    peeler.change_params(catalogue=initial_catalogue, chunksize=tdc_params['fullchain_kargs']['preprocessor']['chunksize'])\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    peeler.run()\n",
    "    t2 = time.perf_counter()\n",
    "    print('peeler.run', t2-t1)\n",
    "\n",
    "    print()\n",
    "    for seg_num in range(dataio.nb_segment):\n",
    "        spikes = dataio.get_spikes(seg_num)\n",
    "        print('seg_num', seg_num, 'nb_spikes', spikes.size)\n",
    "else:\n",
    "    print('You need to make a catalogue for the peeler first!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if initial_catalogue is not None:\n",
    "    app = pg.mkQApp()\n",
    "    win = tdc.PeelerWindow(dataio=dataio, catalogue=initial_catalogue)\n",
    "    win.traceviewer.params['xsize_max'] = 300.0  # increase upper bound on time zoom\n",
    "    win.traceviewer.params['zoom_size'] = 30.0   # increase amount of time plotted after clicking on a spike\n",
    "    win.traceviewer.spinbox_xsize.setValue(30.0) # increase amount of time plotted initially\n",
    "    win.traceviewer.gain_zoom(50)                # increase amount of voltage plotted initially\n",
    "    win.show()\n",
    "    app.exec_()\n",
    "else:\n",
    "    print('You need to make a catalogue for the peeler and run the peeler first!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the spikes and (initial) parameters to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if initial_catalogue is not None:\n",
    "    dataio.export_spikes('../ffs-ignore/spike-sorting-export-dir', formats = 'csv')\n",
    "    with open('../ffs-ignore/spike-sorting-export-dir/tdc_initial_params.yml', 'w') as f:\n",
    "        yaml.dump(tdc_params, f, default_flow_style=False)\n",
    "else:\n",
    "    print('You need to make a catalogue for the peeler and run the peeler first!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
