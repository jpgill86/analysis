{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jump to a Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Figure 3](#[FIGURE-3])\n",
    "  - [Figure 3A](#ðŸŒ-Figure-3A)\n",
    "  - [Figure 3B](#ðŸŒ-Figure-3B)\n",
    "  - [Figure 3C](#ðŸŒ-Figure-3C)\n",
    "  - [Figure 3D](#ðŸŒ-Figure-3D)\n",
    "- [Figure 4](#[FIGURE-4])\n",
    "  - [Figure 4C](#ðŸŒ-Figure-4C)\n",
    "  - [Figure 4D](#ðŸŒ-Figure-4D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import quantities as pq\n",
    "import elephant\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import neurotic\n",
    "from neurotic.datasets.data import _detect_spikes, _find_bursts\n",
    "from modules.utils import BehaviorsDataFrame, DownsampleNeoSignal\n",
    "from modules import r_stats\n",
    "from modules.plot_utils import add_scalebar, solve_figure_horizontal_dimensions, solve_figure_vertical_dimensions\n",
    "\n",
    "pq.markup.config.use_unicode = True  # allow symbols like mu for micro in output\n",
    "pq.mN = pq.UnitQuantity('millinewton', pq.N/1e3, symbol = 'mN');  # define millinewton\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.markers import CARETLEFT, CARETRIGHT, CARETUP, CARETDOWN, CARETUPBASE\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import seaborn as sns\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import numpy2ri\n",
    "numpy2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# don't warn about invalid comparisons to NaN\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "# np.nanmax raises a warning if all values are NaN and returns NaN, which is the behavior we want\n",
    "warnings.filterwarnings('ignore', message='All-NaN slice encountered')\n",
    "\n",
    "# elephant.statistics.instantaneous_rate always complains about negative values\n",
    "warnings.filterwarnings('ignore', message='Instantaneous firing rate approximation contains '\n",
    "                                          'negative values, possibly caused due to machine '\n",
    "                                          'precision errors')\n",
    "\n",
    "# with matplotlib>=3.1 and seaborn<=0.9.0, deprecation warnings are raised\n",
    "# whenever tick marks are placed on the right axis but not the left\n",
    "from matplotlib.cbook import MatplotlibDeprecationWarning\n",
    "warnings.simplefilter('ignore', MatplotlibDeprecationWarning)\n",
    "\n",
    "# don't complain about opening too many figures\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# make figures interactive and open in a separate window\n",
    "# %matplotlib qt\n",
    "\n",
    "# make figures interactive and inline\n",
    "%matplotlib notebook\n",
    "\n",
    "# make figures non-interactive and inline\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Plot Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = 'neurotic-manuscript-figures'\n",
    "if not os.path.exists(export_dir):\n",
    "    os.mkdir(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# general plot settings\n",
    "sns.set(\n",
    "#     context = 'poster',\n",
    "    style = 'ticks',\n",
    "    font_scale = 1,\n",
    "    font = 'Palatino Linotype',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display current color palette\n",
    "with sns.axes_style('darkgrid'):\n",
    "    sns.palplot(sns.color_palette(None), size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "unit_colors = {\n",
    "    'B38':       '#EFBF46', # yellow\n",
    "    'I2':        '#DC5151', # red\n",
    "    'B8a/b':     'C6',      # pink\n",
    "    'B6/B9':     'C9',      # light blue\n",
    "    'B3/B6/B9':  '#5A9BC5', # medium blue\n",
    "    'B3':        '#4F80BD', # dark blue\n",
    "    'B4/B5':     '#00A86B', # jade green\n",
    "    'Force':     'k',       # black\n",
    "}\n",
    "force_colors = {\n",
    "    'shoulder':     unit_colors['B38'],\n",
    "    'dip':          unit_colors['I2'],\n",
    "    'initial rise': unit_colors['B8a/b'],\n",
    "    'rise':         unit_colors['B8a/b'],\n",
    "    'plateau':      unit_colors['B3/B6/B9'],\n",
    "    'drop':         'gray',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the selected unit colors\n",
    "with sns.axes_style('darkgrid'):\n",
    "    sns.palplot(unit_colors.values(), size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print hex codes for selected unit colors\n",
    "for unit, color in unit_colors.items():\n",
    "    print(f'{unit.ljust(10)} {mcolors.to_hex(color).upper()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see simulated colorblindness for selected unit colors\n",
    "print(\n",
    "    'https://davidmathlogic.com/colorblind/#' +\n",
    "    '-'.join([mcolors.to_hex(c).upper().replace('#', '%23') for c in unit_colors.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [\n",
    "    'B38',\n",
    "    'I2',\n",
    "    'B8a/b',\n",
    "    'B6/B9',\n",
    "    'B3',\n",
    "    'B4/B5',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burst_thresholds_default = {\n",
    "    'B38':   ( 8,  5)*pq.Hz, # based on McManus et al. 2014\n",
    "    'I2':    (10,  5)*pq.Hz, # same as Cullins et al. 2015a (based on Hurwitz et al. 1996)\n",
    "    'B8a/b': ( 3,  3)*pq.Hz, # same as Cullins et al. 2015a (based on Morton and Chiel 1993a)\n",
    "    'B6/B9': (10,  5)*pq.Hz, # based on Lu et al. 2015\n",
    "    'B3':    ( 8,  2)*pq.Hz, # based on Lu et al. 2015\n",
    "    'B4/B5': ( 3,  3)*pq.Hz, # same as Cullins et al. 2015a ? (Table 1 says 3 Hz, text says first/last spike; based on Warman and Chiel 1995 ?)\n",
    "}\n",
    "\n",
    "burst_thresholds_by_animal = {\n",
    "    'JG07': burst_thresholds_default.copy(),\n",
    "    'JG08': burst_thresholds_default.copy(),\n",
    "    'JG11': burst_thresholds_default.copy(),\n",
    "    'JG12': burst_thresholds_default.copy(),\n",
    "    'JG14': burst_thresholds_default.copy(),\n",
    "}\n",
    "\n",
    "# exceptions\n",
    "burst_thresholds_by_animal['JG07']['B8a/b'] = ( 2,   2  )*pq.Hz # both thresholds reduced for this animal because B8a/b signal was weak with few spikes\n",
    "burst_thresholds_by_animal['JG08']['B6/B9'] = (10,   3  )*pq.Hz # end threshold reduced for this animal\n",
    "burst_thresholds_by_animal['JG11']['B4/B5'] = ( 1.5, 1.5)*pq.Hz # both thresholds reduced for this animal because only one neuron appeared to project\n",
    "burst_thresholds_by_animal['JG14']['B6/B9'] = ( 4,   2  )*pq.Hz # both thresholds reduced for this animal because B6/B9 always fired slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_units = [\n",
    "    'uV', # I2\n",
    "    'uV', # RN\n",
    "    'uV', # BN2\n",
    "    'uV', # BN3\n",
    "    'mN', # Force\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names_by_animal = {\n",
    "    'JG07': ['I2-L', 'RN-L', 'BN2-L', 'BN3-L',    'Force'],\n",
    "    'JG08': ['I2',   'RN',   'BN2',   'BN3',      'Force'],\n",
    "    'JG11': ['I2',   'RN',   'BN2',   'BN3-PROX', 'Force'],\n",
    "    'JG12': ['I2',   'RN',   'BN2',   'BN3-DIST', 'Force'],\n",
    "    'JG14': ['I2',   'RN',   'BN2',   'BN3-PROX', 'Force'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_filters_by_animal = {\n",
    "    'JG07': [{'channel': 'I2-L', 'lowpass': 100}],\n",
    "    'JG08': [{'channel': 'I2',   'lowpass': 100}],\n",
    "    'JG11': [{'channel': 'I2',   'lowpass': 100}],\n",
    "    'JG12': [{'channel': 'I2',   'lowpass': 100}],\n",
    "    'JG14': [{'channel': 'I2',   'lowpass': 100}],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_types_by_food = {\n",
    "    'Regular nori': ['Swallow (regular 5-cm nori strip)'],\n",
    "    'Tape nori':    ['Swallow (tape nori)'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "feeding_bouts = {\n",
    "    # (animal, food, bout_index): (data_set_name, time_window)\n",
    "\n",
    "    ('JG07', 'Regular nori', 0): ('IN VIVO / JG07 / 2018-05-20 / 002', [1496, 1518]), # 4 swallows\n",
    "    ('JG07', 'Regular nori', 1): ('IN VIVO / JG07 / 2018-05-20 / 002', [1169, 1191]), # 4 swallows\n",
    "    ('JG07', 'Regular nori', 2): ('IN VIVO / JG07 / 2018-05-20 / 002', [1582, 1615]), # 5 swallows\n",
    "    ('JG08', 'Regular nori', 0): ('IN VIVO / JG08 / 2018-06-21 / 002', [ 256,  287]), # 4 swallows\n",
    "    ('JG08', 'Regular nori', 1): ('IN VIVO / JG08 / 2018-06-21 / 002', [ 454,  481]), # 4 swallows\n",
    "    ('JG11', 'Regular nori', 0): ('IN VIVO / JG11 / 2019-04-03 / 001', [1791, 1819]), # 5 swallows\n",
    "    ('JG11', 'Regular nori', 1): ('IN VIVO / JG11 / 2019-04-03 / 004', [ 551,  568]), # 3 swallows\n",
    "    ('JG12', 'Regular nori', 0): ('IN VIVO / JG12 / 2019-05-10 / 002', [ 147,  165]), # 3 swallows\n",
    "    ('JG12', 'Regular nori', 1): ('IN VIVO / JG12 / 2019-05-10 / 002', [ 229,  245]), # 3 swallows\n",
    "    ('JG12', 'Regular nori', 2): ('IN VIVO / JG12 / 2019-05-10 / 002', [ 277,  291]), # 3 swallows\n",
    "    ('JG14', 'Regular nori', 0): ('IN VIVO / JG14 / 2019-07-30 / 001', [1834, 1865]), # 4 swallows\n",
    "    ('JG14', 'Regular nori', 1): ('IN VIVO / JG14 / 2019-07-30 / 001', [1910, 1943]), # 5 swallows\n",
    "    ('JG14', 'Regular nori', 2): ('IN VIVO / JG14 / 2019-07-30 / 001', [2052, 2084]), # 5 swallows\n",
    "    \n",
    "    ('JG07', 'Tape nori',    0): ('IN VIVO / JG07 / 2018-05-20 / 002', [2718, 2755]), # 5 swallows\n",
    "    ('JG08', 'Tape nori',    0): ('IN VIVO / JG08 / 2018-06-21 / 002', [ 147,  208]), # 7 swallows, some bucket and head movement\n",
    "    ('JG08', 'Tape nori',    1): ('IN VIVO / JG08 / 2018-06-21 / 002', [ 664,  701]), # 5 swallows, large bucket movement\n",
    "    ('JG08', 'Tape nori',    2): ('IN VIVO / JG08 / 2018-06-21 / 002', [1451, 1477]), # 3 swallows, some bucket movement\n",
    "    ('JG11', 'Tape nori',    0): ('IN VIVO / JG11 / 2019-04-03 / 004', [1227, 1280]), # 5 swallows, inward food movements had very low amplitude\n",
    "    ('JG12', 'Tape nori',    0): ('IN VIVO / JG12 / 2019-05-10 / 002', [ 436,  465]), # 4 swallows\n",
    "    ('JG12', 'Tape nori',    1): ('IN VIVO / JG12 / 2019-05-10 / 002', [2901, 2937]), # 5 swallows\n",
    "    ('JG14', 'Tape nori',    0): ('IN VIVO / JG14 / 2019-07-29 / 004', [ 829,  870]), # 5 swallows\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example figures only -- not used in majority of analysis because of long inter-swallow intervals\n",
    "\n",
    "exemplary_bout    = ('JG12', 'Tape nori', 101)\n",
    "exemplary_swallow = ('JG12', 'Tape nori', 102)\n",
    "\n",
    "feeding_bouts[exemplary_bout]    = ('IN VIVO / JG12 / 2019-05-10 / 002', [2970.7, 2992.0]) # 3 swallows\n",
    "feeding_bouts[exemplary_swallow] = ('IN VIVO / JG12 / 2019-05-10 / 002', [2977.3, 2984.5]) # 1 swallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these unloaded swallows have unreliable inward movement measurement\n",
    "# and are excluded from some parts of the analysis\n",
    "unreliable_inward_movement = [\n",
    "    ('JG07', 'Regular nori', 0, 2), # inward movement unusually short (maybe strip tore?)\n",
    "    ('JG07', 'Regular nori', 0, 3), # finished strip mid-retraction\n",
    "    ('JG07', 'Regular nori', 1, 1), # inward movement unusually short (maybe strip tore?)\n",
    "    ('JG07', 'Regular nori', 1, 3), # finished strip mid-retraction\n",
    "    ('JG07', 'Regular nori', 2, 2), # inward movement unusually short (maybe strip tore?)\n",
    "    ('JG07', 'Regular nori', 2, 4), # finished strip mid-retraction\n",
    "    ('JG08', 'Regular nori', 0, 3), # finished strip mid-retraction\n",
    "    ('JG08', 'Regular nori', 1, 3), # finished strip mid-retraction\n",
    "    ('JG11', 'Regular nori', 0, 4), # finished strip mid-retraction\n",
    "    ('JG11', 'Regular nori', 1, 2), # finished strip mid-retraction\n",
    "    ('JG12', 'Regular nori', 0, 2), # finished strip mid-retraction\n",
    "    # JG12 Regular nori 0 1 is OK\n",
    "    ('JG12', 'Regular nori', 2, 2), # finished strip mid-retraction\n",
    "    ('JG14', 'Regular nori', 0, 3), # finished strip mid-retraction\n",
    "    ('JG14', 'Regular nori', 1, 0), # bite-swallow and inward movement unusually short (maybe strip tore?)\n",
    "    ('JG14', 'Regular nori', 1, 3), # inward movement unusually short (maybe strip tore?)\n",
    "    ('JG14', 'Regular nori', 1, 4), # finished strip mid-retraction\n",
    "    ('JG14', 'Regular nori', 2, 1), # not visible\n",
    "]\n",
    "\n",
    "# these behaviors at the start of each unloaded bout are also excluded\n",
    "# because they are actually bite-swallows instead of pure swallows\n",
    "bite_swallow_behaviors = [\n",
    "    ('JG07', 'Regular nori', 0, 0),\n",
    "    ('JG07', 'Regular nori', 1, 0),\n",
    "    ('JG07', 'Regular nori', 2, 0),\n",
    "    ('JG08', 'Regular nori', 0, 0),\n",
    "    ('JG08', 'Regular nori', 1, 0),\n",
    "    ('JG11', 'Regular nori', 0, 0),\n",
    "    ('JG11', 'Regular nori', 1, 0),\n",
    "    ('JG12', 'Regular nori', 0, 0),\n",
    "    ('JG12', 'Regular nori', 1, 0),\n",
    "    ('JG12', 'Regular nori', 2, 0),\n",
    "    ('JG14', 'Regular nori', 0, 0),\n",
    "    ('JG14', 'Regular nori', 1, 0),\n",
    "    ('JG14', 'Regular nori', 2, 0),\n",
    "]\n",
    "unreliable_inward_movement += bite_swallow_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def query_union(queries):\n",
    "    return '|'.join([f'({q})' for q in queries if q is not None])\n",
    "\n",
    "def label2query(label):\n",
    "    animal, food = label[:4], label[5:]\n",
    "    query = f'(Animal == \"{animal}\") & (Food == \"{food}\")'\n",
    "    return query\n",
    "\n",
    "def contains(series, string):\n",
    "#     return np.array([string in x for x in series])\n",
    "    return series.map(lambda x: string in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_sig(blk, channel):\n",
    "    sig = next((sig for sig in blk.segments[0].analogsignals if sig.name == channel), None)\n",
    "    if sig is None:\n",
    "        raise Exception(f'Channel \"{channel}\" could not be found')\n",
    "    else:\n",
    "        return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_index(blk, channel):\n",
    "    index = next((i for i, sig in enumerate(blk.segments[0].analogsignals) if sig.name == channel), None)\n",
    "    if index is None:\n",
    "        raise Exception(f'Channel \"{channel}\" could not be found')\n",
    "    else:\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(blk, metadata):\n",
    "    # nearly identical to neurotic's implementation except\n",
    "    # time_slice ensures proxies are loaded\n",
    "    \n",
    "    for sig_filter in metadata['filters']:\n",
    "        index = get_sig_index(blk, sig_filter['channel'])\n",
    "        high = sig_filter.get('highpass', None)\n",
    "        low  = sig_filter.get('lowpass',  None)\n",
    "        if high:\n",
    "            high *= pq.Hz\n",
    "        if low:\n",
    "            low  *= pq.Hz\n",
    "        blk.segments[0].analogsignals[index] = elephant.signal_processing.butter(  # may raise a FutureWarning\n",
    "            signal = blk.segments[0].analogsignals[index].time_slice(None, None),\n",
    "            highpass_freq = high,\n",
    "            lowpass_freq  = low,\n",
    "        )\n",
    "    \n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def is_good_burst(burst):\n",
    "    time, duration, n_spikes = burst\n",
    "    return duration >= 0.5*pq.s and n_spikes > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must first convert args from quantities to simple ndarrays\n",
    "# (use .rescale('s').magnitude)\n",
    "def normalize_time(fixed_times, t, extrapolate=True):\n",
    "    \n",
    "    if not isinstance(t, np.ndarray):\n",
    "        if type(t) is list:\n",
    "            t = np.array(t)\n",
    "        else:\n",
    "            t = np.array([t])\n",
    "    \n",
    "    assert not isinstance(fixed_times, pq.Quantity), f'fixed_times should not be a quantity (use .magnitude)'\n",
    "    assert not isinstance(t, pq.Quantity), f't should not be a quantity (use .magnitude)'\n",
    "    assert np.all(np.diff(fixed_times[~np.isnan(fixed_times)])>=0), f'fixed_times must be sorted: {fixed_times}'\n",
    "    \n",
    "    # create a copy in which NaNs are removed\n",
    "    # - this is done because searchsorted does not work well with NaNs\n",
    "    # - infinities are retained here\n",
    "    fixed_times_without_nans = fixed_times[np.where(~np.isnan(fixed_times))[0]]\n",
    "    \n",
    "    # find the indexes of the fixed values that are just after each value in t\n",
    "    indexes = np.searchsorted(fixed_times_without_nans, t)\n",
    "    \n",
    "    # adjust indexes to account for the NaNs that were removed\n",
    "    nan_indexes = np.where(np.isnan(fixed_times))[0]\n",
    "    for nan_index in nan_indexes:\n",
    "        indexes[indexes >= nan_index] += 1\n",
    "    \n",
    "    # increment/decrement any index equal to 0/N, where N=len(fixed_times)\n",
    "    # - this is needed for values in t that are less/greater than the min/max fixed\n",
    "    #   time, and for NaNs in t which get assigned an index of N by searchsorted\n",
    "    # - for values in t less/greater than the min/max fixed time, this\n",
    "    #   increment/decrement in index will prepare that value to be normalized\n",
    "    #   using extrapolation based on the first/last interval in fixed_times\n",
    "    indexes = np.clip(indexes, 1, len(fixed_times)-1)\n",
    "    \n",
    "    # compute the normalized values of t using linear interpolation between the\n",
    "    # bordering fixed times\n",
    "    # - normalization of values in t that are less than the min fixed time is\n",
    "    #   accomplished by extrapolation, as the fraction (after-t)/(after-before)\n",
    "    #   will be greater than 1 since the t value is in fact earlier than the\n",
    "    #   \"before\" fixed time\n",
    "    # - normalization of values in t that are greater than the max fixed time is\n",
    "    #   accomplished by extrapolation, as the fraction (after-t)/(after-before)\n",
    "    #   will be less than 0 since the t value is in fact later than the \"after\"\n",
    "    #   fixed time\n",
    "    before = fixed_times[indexes-1]\n",
    "    after  = fixed_times[indexes]\n",
    "    t_normalized = indexes - (after-t)/(after-before)\n",
    "    \n",
    "    if not extrapolate:\n",
    "        # extrapolation is already done, so here we undo it by setting to NaN\n",
    "        # the normalized value for any t that is less/greater than the min/max\n",
    "        # fixed time\n",
    "        with np.errstate(invalid='ignore'): # don't warn about invalid comparisons to NaN\n",
    "            t_normalized[t < np.nanmin(fixed_times)] = np.nan\n",
    "            t_normalized[t > np.nanmax(fixed_times)] = np.nan\n",
    "    \n",
    "    return t_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must first convert args from quantities to simple ndarrays\n",
    "# (use .rescale('s').magnitude)\n",
    "def unnormalize_time(fixed_times, t_normalized, extrapolate=True):\n",
    "    \n",
    "    if not isinstance(t_normalized, np.ndarray):\n",
    "        if type(t_normalized) is list:\n",
    "            t_normalized = np.array(t_normalized)\n",
    "        else:\n",
    "            t_normalized = np.array([t_normalized])\n",
    "    \n",
    "    assert not isinstance(fixed_times, pq.Quantity), f'fixed_times should not be a quantity (use .magnitude)'\n",
    "    assert not isinstance(t_normalized, pq.Quantity), f't_normalized should not be a quantity (use .magnitude)'\n",
    "    assert np.all(np.diff(fixed_times[~np.isnan(fixed_times)])>=0), f'fixed_times must be sorted: {fixed_times}'\n",
    "    \n",
    "    # get the index of the fixed time that comes after each t\n",
    "    indexes = np.ceil(t_normalized)\n",
    "    \n",
    "    # clip the \"after\" indexes so that the first or last interval\n",
    "    # in fixed_times will be used for extrapolation\n",
    "    indexes = np.clip(indexes, 1, len(fixed_times)-1)\n",
    "    \n",
    "    # get the fixed times before and after each t\n",
    "    before = np.array([fixed_times[int(i)-1] if not np.isnan(i) else np.nan for i in indexes])\n",
    "    after  = np.array([fixed_times[int(i)]   if not np.isnan(i) else np.nan for i in indexes])\n",
    "    \n",
    "    # compute the real values of t\n",
    "    t = after + (t_normalized-indexes)*(after-before)\n",
    "    \n",
    "    if not extrapolate:\n",
    "        # extrapolation is already done, so here we undo it by setting to NaN\n",
    "        # the value for any t that is less/greater than the min/max fixed time\n",
    "        with np.errstate(invalid='ignore'): # don't warn about invalid comparisons to NaN\n",
    "            t[t < np.nanmin(fixed_times)] = np.nan\n",
    "            t[t > np.nanmax(fixed_times)] = np.nan\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these arrays are used as interp_times for resample_sig_in_normalized_time\n",
    "# depending on the segmentation scheme\n",
    "# - linspace ensures samples will be taken at regular intervals in normalized time\n",
    "interp_resolution = 1000\n",
    "video_seg_interp_times = np.linspace(0, 9, interp_resolution)\n",
    "force_seg_interp_times = np.linspace(0, 9, interp_resolution)\n",
    "\n",
    "def resample_sig_in_normalized_time(fixed_times, sig, interp_times):\n",
    "    \n",
    "    # get normalized times and signal values\n",
    "    # - normalize_time will put into times_normalized a np.nan wherever a time was not normalizable,\n",
    "    #   i.e., wherever the time occurred adjacent to a missing fixed time (np.nan in fixed_times)\n",
    "    times = sig.times.rescale('s').magnitude\n",
    "    times_normalized = normalize_time(fixed_times.magnitude, times)\n",
    "    y = sig.magnitude.flatten()\n",
    "\n",
    "    # drop times that could not be normalized due to missing fixed times\n",
    "    # - interp1d will erroneously interpolate across the gaps created by this deletion,\n",
    "    #   but we will then replace those interpolated values with np.nan\n",
    "    where_normalizable = np.where(~np.isnan(times_normalized))[0]\n",
    "    times_normalized = times_normalized[where_normalizable]\n",
    "    y = y[where_normalizable]\n",
    "\n",
    "    # resample evenly in normalized time\n",
    "    # - with bounds_error=False and fill_value=np.nan, interp1d will put np.nan in places outside\n",
    "    #   the min and max of times_normalized\n",
    "    # - interp1d will interpolate across the regions we deleted, but we want np.nan there,\n",
    "    #   so we will insert them manually in the next step\n",
    "    interp_func = sp.interpolate.interp1d(times_normalized, y, kind='linear', bounds_error=False, fill_value=np.nan)\n",
    "    y_interp = interp_func(interp_times)\n",
    "\n",
    "    # replace erroneously interpolated values with np.nan\n",
    "    # - now the points in y_interp which would correspond to times that could not be normalized\n",
    "    #   have been set to np.nan\n",
    "    where_not_normalizable = np.where(np.isnan(unnormalize_time(fixed_times.magnitude, interp_times)))[0]\n",
    "    y_interp[where_not_normalizable] = np.nan\n",
    "\n",
    "    return y_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_column_analysis(column):\n",
    "    unit = column.split('(')[-1].split(')')[0]\n",
    "    mean = df_all[column].mean()\n",
    "    std = df_all[column].std()\n",
    "    cv = std/abs(mean)\n",
    "    print(f'Overall mean +/- std: {mean:.2f} +/- {std:.2f} {unit}')\n",
    "    print(f'Overall coefficient of variation CV: {cv:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differences_test(x, y, x_label='GROUP 1', y_label='GROUP 2', measure_label='MEASURE', units='UNITS', alpha=0.05):\n",
    "\n",
    "    # descriptive statistics\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    x_std = np.std(x, ddof=1)\n",
    "    y_std = np.std(y, ddof=1)\n",
    "    x_n = len(x)\n",
    "    y_n = len(y)\n",
    "    assert x_n == y_n, 'expected x and y to be paired but they have unequal length'\n",
    "    print(f'{x_label}: (M = {x_mean:g}, SD = {x_std:g}, N = {x_n:g})')\n",
    "    print(f'{y_label}: (M = {y_mean:g}, SD = {y_std:g}, N = {y_n:g})')\n",
    "    print()\n",
    "\n",
    "    # Shapiro-Wilk test for normality of differences\n",
    "    # - equivalent R test: shapiro.test(x-y)\n",
    "    shapiro_W, shapiro_p = sp.stats.shapiro(x-y)\n",
    "    shapiro_signif = '*' if shapiro_p < alpha else '(n.s.)'\n",
    "    print(f'H0: Differences have normal distribution, W = {shapiro_W:g},\\tp = {shapiro_p:g} {shapiro_signif}')\n",
    "\n",
    "    if shapiro_p >= 0.05:\n",
    "        print('- Because the differences can be assumed to be normal, a paired t-test will be used')\n",
    "\n",
    "        # paired one-tailed T-test for an increase in means\n",
    "        # - equivalent R test : t.test(x, y, paired=TRUE, alternative=\"greater\")\n",
    "        ttest_t, ttest_p = sp.stats.ttest_rel(x, y)\n",
    "        ttest_p = ttest_p/2 # manually divide by 2 because sp.stats.ttest_rel does not have a one-tailed setting\n",
    "        ttest_signif = '*' if ttest_p < alpha and x_mean > y_mean else '(n.s.)'\n",
    "        print(f'H0: Difference in means is not positive,  t = {ttest_t:g},\\tp = {ttest_p:g} {ttest_signif}')\n",
    "\n",
    "        # Cohen's d for effect size\n",
    "        # - equivalent R function: library(effsize); cohen.d(x, y, paired=FALSE)\n",
    "        # - I don't understand the paper cited for the R function's paired=TRUE case,\n",
    "        #   so I haven't tried to implement it here. The R function's paired=TRUE\n",
    "        #   result is a little smaller. It's unclear to me whether that result can\n",
    "        #   even still be called \"Cohen's d\", as that name does not appear in the paper.\n",
    "        pooled_std = np.sqrt(((len(x)-1)*np.var(x, ddof=1) + (len(y)-1)*np.var(y, ddof=1))/(len(x)+len(y)-2))\n",
    "        cohen_d = (np.mean(x)-np.mean(y))/pooled_std\n",
    "        print()\n",
    "        print(f'Effect size: Cohen\\'s d = {cohen_d:g}')\n",
    "        \n",
    "        if ttest_p < alpha and x_mean > y_mean:\n",
    "            print()\n",
    "            print(f'\"A paired-samples one-tailed t-test indicated that {measure_label} for the ' \\\n",
    "                  f'[{x_n}] animals were significantly [greater/longer] for ' \\\n",
    "                  f'{x_label} (M = {x_mean:.2f} {units}, SD = {x_std:.2f} {units}) than for ' \\\n",
    "                  f'{y_label} (M = {y_mean:.2f} {units}, SD = {y_std:.2f} {units}), ' \\\n",
    "                  f't = {ttest_t:.3f}, df = {x_n-1}, p = {ttest_p:.3f}, Cohen\\'s d = {cohen_d:.2f}.\"')\n",
    "        else:\n",
    "            print()\n",
    "            print(f'\"A paired-samples one-tailed t-test indicated no significant increase in {measure_label} in the [{x_n}] animals between ' \\\n",
    "                  f'{x_label} (M = {x_mean:.2f} {units}, SD = {x_std:.2f} {units}) and ' \\\n",
    "                  f'{y_label} (M = {y_mean:.2f} {units}, SD = {y_std:.2f} {units}), ' \\\n",
    "                  f't = {ttest_t:.3f}, df = {x_n-1}, p = {ttest_p:.3f}, Cohen\\'s d = {cohen_d:.2f}.\"')\n",
    "        \n",
    "        return ttest_signif\n",
    "\n",
    "    else:\n",
    "        print('- Because the differences cannot be assumed to be normal, a Wilcoxon signed rank test will be used')\n",
    "\n",
    "        # Wilcoxon signed rank test for non-zero difference in locations (medians?)\n",
    "        # - equivalent R test: wilcox.test(x, y, paired=TRUE, exact=FALSE)\n",
    "        # - a warning is raised for small sample sizes (N < 10) becauses SciPy's implementation\n",
    "        #   always calculates the p-value using a normal approximation of the test statistic\n",
    "        #   distribution, which is inaccurate for small sample size\n",
    "        # - use R to get exact p-value, with wilcox.test(x, y, paired=TRUE, exact=TRUE)\n",
    "        # - upcoming implementation of exact distribution: https://github.com/scipy/scipy/pull/10796\n",
    "        wilcoxon_W, wilcoxon_p = sp.stats.wilcoxon(x, y, correction=True)\n",
    "        wilcoxon_signif = '*' if wilcoxon_p < alpha else '(n.s.)'\n",
    "        print(f'H0: Difference in medians is zero,        W = {wilcoxon_W:g},\\tp = {wilcoxon_p:g} {wilcoxon_signif}')\n",
    "        \n",
    "        return wilcoxon_signif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip expensive calculations by loading the results from a file?\n",
    "# - with load_from_files=False, perform data processing from scratch,\n",
    "#   which takes several minutes\n",
    "# - with load_from_files=True, load the final results (dataframes)\n",
    "#   pickled last time the calculations were performed\n",
    "load_from_files = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_vars = ['df_all', 'df_exemplary_bout', 'df_exemplary_swallow']\n",
    "if load_from_files:\n",
    "    \n",
    "    for var in pickled_vars:\n",
    "        exec(f'{var} = pd.read_pickle(\"{var}.zip\")')\n",
    "\n",
    "    print('calculation results loaded from files')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # use Neo RawIO lazy loading to load much faster and using less memory\n",
    "    # - with lazy=True, filtering parameters specified in metadata are ignored\n",
    "    #     - note: filters are replaced below and applied manually anyway\n",
    "    # - with lazy=True, loading via time_slice requires neo>=0.8.0\n",
    "    lazy = True\n",
    "\n",
    "    # load the metadata containing file paths\n",
    "    metadata = neurotic.MetadataSelector(file='../../data/metadata.yml')\n",
    "\n",
    "    # filter epochs for each bout and perform calculations\n",
    "    df_list = []\n",
    "    last_data_set_name = None\n",
    "    pbar = tqdm(total=len(feeding_bouts), unit='feeding bout')\n",
    "    for (animal, food, bout_index), (data_set_name, time_window) in feeding_bouts.items():\n",
    "\n",
    "        channel_names = channel_names_by_animal[animal]\n",
    "        epoch_types = epoch_types_by_food[food]\n",
    "        burst_thresholds = burst_thresholds_by_animal[animal]\n",
    "\n",
    "        ###\n",
    "        ### LOAD DATASET\n",
    "        ###\n",
    "\n",
    "        metadata.select(data_set_name)\n",
    "\n",
    "        if data_set_name is last_data_set_name:\n",
    "            # skip reloading the data if it's already in memory\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            # ensure that the right filters are used\n",
    "            metadata['filters'] = sig_filters_by_animal[animal]\n",
    "\n",
    "            blk = neurotic.load_dataset(metadata, lazy=lazy)\n",
    "\n",
    "            if lazy:\n",
    "                # manually perform filters\n",
    "                blk = apply_filters(blk, metadata)\n",
    "\n",
    "        last_data_set_name = data_set_name\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### LOCATE BEHAVIOR EPOCHS AND SUBEPOCHS\n",
    "        ###\n",
    "\n",
    "        # construct a query for locating behaviors\n",
    "        behavior_query = f'(Type in {epoch_types}) & ({time_window[0]} <= Start) & (End <= {time_window[1]})'\n",
    "\n",
    "        # construct queries for locating epochs associated with each behavior\n",
    "        # - each query should match at most one epoch\n",
    "        # - dictionary keys are used as prefixes for the names of new columns\n",
    "        subepoch_queries = {}\n",
    "\n",
    "        subepoch_queries['B38 activity']            = (f'(Type == \"B38 activity\") & ' \\\n",
    "                                                       f'(@behavior_start-3 <= End) & (End <= @behavior_start+4)',\n",
    "                                                       'last') # use last if there are multiple matches\n",
    "                                                      # must end within a few seconds of behavior start (3 before or 4 after)\n",
    "\n",
    "        subepoch_queries['I2 protraction activity'] = f'(Type == \"I2 protraction activity\") & ' \\\n",
    "                                                      f'(@behavior_start-1 <= Start) & (End <= @behavior_end)'\n",
    "                                                      # must start no earlier than 1 second before behavior and end within it\n",
    "\n",
    "        subepoch_queries['B8 activity']             = f'(Type == \"B8 activity\") & ' \\\n",
    "                                                      f'(@behavior_start <= Start) & (End <= @behavior_end)'\n",
    "                                                      # must be fully contained within behavior\n",
    "\n",
    "        subepoch_queries['B3/6/9/10 activity']      = f'(Type == \"B3/6/9/10 activity\") & ' \\\n",
    "                                                      f'(@behavior_start <= Start) & (End <= @behavior_end)'\n",
    "                                                      # must be fully contained within behavior\n",
    "\n",
    "        subepoch_queries['B4/B5 activity']          = (f'(Type == \"B4/B5 activity\") & ' \\\n",
    "                                                       f'(@behavior_start <= Start) & (Start <= @behavior_end)',\n",
    "                                                       'first') # use first if there are multiple matches\n",
    "                                                      # must start within behavior\n",
    "        \n",
    "        subepoch_queries['Force shoulder end']      = f'(Type == \"Force shoulder end\") & ' \\\n",
    "                                                      f'(@behavior_start-3 <= End) & (End <= @behavior_start+3)'\n",
    "                                                      # must end within 3 seconds of behavior start\n",
    "        \n",
    "        subepoch_queries['Force rise start']        = f'(Type == \"Force rise start\") & ' \\\n",
    "                                                      f'(@behavior_start <= Start) & (Start <= @behavior_end)'\n",
    "                                                      # must start within behavior\n",
    "\n",
    "        subepoch_queries['Force plateau start']     = f'(Type == \"Force plateau start\") & ' \\\n",
    "                                                      f'(@behavior_start <= Start) & (Start <= @behavior_end)'\n",
    "                                                      # must start within behavior\n",
    "\n",
    "        subepoch_queries['Force plateau end']       = f'(Type == \"Force plateau end\") & ' \\\n",
    "                                                      f'(@behavior_end-2 <= Start) & (Start <= @behavior_end+2)'\n",
    "                                                      # must start within 2 seconds of behavior end\n",
    "\n",
    "        subepoch_queries['Force drop end']          = f'(Type == \"Force drop end\") & ' \\\n",
    "                                                      f'(@behavior_end-2 <= Start) & (Start <= @behavior_end+2)'\n",
    "                                                      # must start within 2 seconds of behavior end\n",
    "        \n",
    "        subepoch_queries['Inward movement']         = f'(Type == \"Inward movement\") & ' \\\n",
    "                                                      f'(@behavior_start <= Start) & (Start <= @behavior_end)'\n",
    "                                                      # must start within behavior\n",
    "\n",
    "        # construct a table in which each row is a behavior and subepoch data\n",
    "        # is added as columns, e.g. df['B38 activity start (s)']\n",
    "        df = BehaviorsDataFrame(blk.segments[0].epochs, behavior_query, subepoch_queries)\n",
    "\n",
    "        # renumber behaviors assuming all behaviors are from a single contiguous sequence\n",
    "        df = df.sort_values('Start (s)').reset_index(drop=True).rename_axis('Behavior_index')\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### START CALCULATIONS\n",
    "        ###\n",
    "\n",
    "        # some columns must have type 'object', which\n",
    "        # can be accomplished by initializing with None or np.nan\n",
    "        df['Video segmentation times (s)'] = None\n",
    "        df['Force segmentation times (s)'] = None\n",
    "        df['Force, video segmented interpolation (mN)'] = None\n",
    "        df['Force, force segmented interpolation (mN)'] = None\n",
    "        for unit in units:\n",
    "            df[f'{unit} spike train'] = None\n",
    "            df[f'{unit} firing rate (Hz)'] = None\n",
    "            df[f'{unit} firing rate, video segmented interpolation (Hz)'] = None\n",
    "            df[f'{unit} firing rate, force segmented interpolation (Hz)'] = None\n",
    "            df[f'{unit} all bursts'] = None\n",
    "\n",
    "            # while we're at it, initialize some other things that might otherwise never be given values\n",
    "#             df[f'{unit} first burst start (s)'] = np.nan\n",
    "#             df[f'{unit} first burst end (s)'] = np.nan\n",
    "#             df[f'{unit} first burst duration (s)'] = 0\n",
    "#             df[f'{unit} first burst spike count'] = 0\n",
    "#             df[f'{unit} first burst mean frequency (Hz)'] = np.nan\n",
    "#             df[f'{unit} last burst start (s)'] = np.nan\n",
    "#             df[f'{unit} last burst end (s)'] = np.nan\n",
    "#             df[f'{unit} last burst duration (s)'] = 0\n",
    "#             df[f'{unit} last burst spike count'] = 0\n",
    "#             df[f'{unit} last burst mean frequency (Hz)'] = np.nan\n",
    "            df[f'{unit} burst start (s)'] = np.nan\n",
    "            df[f'{unit} burst end (s)'] = np.nan\n",
    "            df[f'{unit} burst duration (s)'] = 0\n",
    "            df[f'{unit} burst spike count'] = 0\n",
    "            df[f'{unit} burst mean frequency (Hz)'] = 0\n",
    "            df[f'{unit} burst start (video seg normalized)'] = np.nan\n",
    "            df[f'{unit} burst end (video seg normalized)'] = np.nan\n",
    "            df[f'{unit} burst start (force seg normalized)'] = np.nan\n",
    "            df[f'{unit} burst end (force seg normalized)'] = np.nan\n",
    "        df['Inward movement start (video seg normalized)'] = np.nan\n",
    "        df['Inward movement end (video seg normalized)'] = np.nan\n",
    "        df['Inward movement start (force seg normalized)'] = np.nan\n",
    "        df['Inward movement end (force seg normalized)'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "        # get smoothed force for entire time window\n",
    "        channel = 'Force'\n",
    "        sig = get_sig(blk, channel)\n",
    "        if lazy:\n",
    "            sig = sig.time_slice(None, None)\n",
    "        sig = elephant.signal_processing.butter(sig, lowpass_freq = 5*pq.Hz)\n",
    "        sig = sig.time_slice(time_window[0]*pq.s, time_window[1]*pq.s)\n",
    "        sig = sig.rescale('mN')\n",
    "        force_smoothed_sig = sig\n",
    "\n",
    "\n",
    "\n",
    "        df['End to next start (s)'] = np.nan\n",
    "        df['Start to next start (s)'] = np.nan\n",
    "        previous_i = None\n",
    "\n",
    "        # iterate over all swallows\n",
    "        for j, i in enumerate(df.index):\n",
    "\n",
    "            behavior_start = df.loc[i, 'Start (s)']*pq.s\n",
    "            behavior_end = df.loc[i, 'End (s)']*pq.s\n",
    "            inward_movement_start = df.loc[i, 'Inward movement start (s)']*pq.s\n",
    "            inward_movement_end = df.loc[i, 'Inward movement end (s)']*pq.s\n",
    "\n",
    "            # calculate interbehavior intervals assuming all behaviors are from a single contiguous sequence\n",
    "            if previous_i is not None:\n",
    "                df.loc[previous_i, 'End to next start (s)']   = df.loc[i, 'Start (s)'] - df.loc[previous_i, 'End (s)']\n",
    "                df.loc[previous_i, 'Start to next start (s)'] = df.loc[i, 'Start (s)'] - df.loc[previous_i, 'Start (s)']\n",
    "                df.loc[previous_i, 'Inward movement start to next inward movement start (s)'] = \\\n",
    "                    df.loc[i, 'Inward movement start (s)'] - df.loc[previous_i, 'Inward movement start (s)']\n",
    "                df.loc[previous_i, 'Inward movement end to next inward movement start (s)'] = \\\n",
    "                    df.loc[i, 'Inward movement start (s)'] - df.loc[previous_i, 'Inward movement end (s)']\n",
    "            previous_i = i\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### VIDEO SEGMENTATION\n",
    "            ###\n",
    "            \n",
    "            # inward movement start and end are required\n",
    "            video_is_segmented = np.all(np.isfinite(np.array([\n",
    "                inward_movement_start, inward_movement_end])))\n",
    "            \n",
    "            if video_is_segmented:\n",
    "                inward_movement_duration = df.loc[i, 'Inward movement duration (s)']*pq.s\n",
    "                \n",
    "                # get the list of fixed times for normalization\n",
    "                video_segmentation_times = df.at[i, 'Video segmentation times (s)'] = np.array([\n",
    "                    inward_movement_start-inward_movement_duration*4,\n",
    "                    inward_movement_start-inward_movement_duration*3,\n",
    "                    inward_movement_start-inward_movement_duration*2,\n",
    "                    inward_movement_start-inward_movement_duration*1,\n",
    "                    inward_movement_start,\n",
    "                    inward_movement_end,\n",
    "                    inward_movement_end+inward_movement_duration*1,\n",
    "                    inward_movement_end+inward_movement_duration*2,\n",
    "                    inward_movement_end+inward_movement_duration*3,\n",
    "                    inward_movement_end+inward_movement_duration*4,\n",
    "                ])*pq.s # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "            else: # video is not segmented\n",
    "                video_segmentation_times = df.at[i, 'Video segmentation times (s)'] = np.array([\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                ])*pq.s # 'at', not 'loc', is important for inserting list into cell\n",
    "            \n",
    "            \n",
    "            \n",
    "            ###\n",
    "            ### FORCE SEGMENTATION\n",
    "            ###\n",
    "\n",
    "            force_shoulder_end  = df.loc[i, 'Force shoulder end start (s)']*pq.s  # start of \"Force shoulder end\" epoch\n",
    "            force_rise_start    = df.loc[i, 'Force rise start start (s)']*pq.s    # start of \"Force rise start\" epoch\n",
    "            force_plateau_start = df.loc[i, 'Force plateau start start (s)']*pq.s # start of \"Force plateau start\" epoch\n",
    "            force_plateau_end   = df.loc[i, 'Force plateau end start (s)']*pq.s   # start of \"Force plateau end\" epoch\n",
    "            force_drop_end      = df.loc[i, 'Force drop end start (s)']*pq.s      # start of \"Force drop end\" epoch\n",
    "\n",
    "            # force rise start, plateau start and end, and drop end are required\n",
    "            force_is_segmented = np.all(np.isfinite(np.array([\n",
    "                force_rise_start, force_plateau_start, force_plateau_end, force_drop_end])))\n",
    "\n",
    "            if force_is_segmented:\n",
    "                # get some times for the previous and next swallow\n",
    "                epochs_force_shoulder_end  = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force shoulder end'), None)\n",
    "                epochs_force_rise_start    = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force rise start'), None)\n",
    "                epochs_force_plateau_start = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force plateau start'), None)\n",
    "                epochs_force_plateau_end   = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force plateau end'), None)\n",
    "                epochs_force_drop_end      = next((ep for ep in blk.segments[0].epochs if ep.name == 'Force drop end'), None)\n",
    "                assert epochs_force_shoulder_end  is not None, 'failed to find \"Force shoulder end\" epochs'\n",
    "                assert epochs_force_rise_start    is not None, 'failed to find \"Force rise start\" epochs'\n",
    "                assert epochs_force_plateau_start is not None, 'failed to find \"Force plateau start\" epochs'\n",
    "                assert epochs_force_plateau_end   is not None, 'failed to find \"Force plateau end\" epochs'\n",
    "                assert epochs_force_drop_end      is not None, 'failed to find \"Force drop end\" epochs'\n",
    "\n",
    "                try:\n",
    "                    prev_force_plateau_start = df.loc[i, 'Previous force plateau start (s)'] = epochs_force_plateau_start.time_slice(None, force_rise_start)[-1]\n",
    "                    assert force_rise_start-prev_force_plateau_start < 16*pq.s, f'for swallow {i}, previous force plateau start is too far away'\n",
    "                except IndexError:\n",
    "                    prev_force_plateau_start = df.loc[i, 'Previous force plateau start (s)'] = np.nan\n",
    "\n",
    "                try:\n",
    "                    prev_force_plateau_end = df.loc[i, 'Previous force plateau end (s)'] = epochs_force_plateau_end.time_slice(None, force_rise_start)[-1]\n",
    "                    assert force_rise_start-prev_force_plateau_end < 12*pq.s, f'for swallow {i}, previous force plateau end is too far away'\n",
    "                except IndexError:\n",
    "                    prev_force_plateau_end = df.loc[i, 'Previous force plateau end (s)'] = np.nan\n",
    "\n",
    "                try:\n",
    "                    prev_force_drop_end = df.loc[i, 'Previous force drop end (s)'] = epochs_force_drop_end.time_slice(None, force_rise_start)[-1]\n",
    "                    assert force_rise_start-prev_force_drop_end < 12*pq.s, f'for swallow {i}, previous force drop end is too far away'\n",
    "                except IndexError:\n",
    "                    prev_force_drop_end = df.loc[i, 'Previous force drop end (s)'] = np.nan\n",
    "\n",
    "                try:\n",
    "                    next_force_rise_start = df.loc[i, 'Next force rise start (s)'] = epochs_force_rise_start.time_slice(force_drop_end, None)[0]\n",
    "                    assert next_force_rise_start-force_drop_end < 12*pq.s, f'for swallow {i}, next force rise start is too far away'\n",
    "                except IndexError:\n",
    "                    next_force_rise_start = df.loc[i, 'Next force rise start (s)'] = np.nan\n",
    "\n",
    "                try:\n",
    "                    next_force_shoulder_end = df.loc[i, 'Next force shoulder end (s)'] = epochs_force_shoulder_end.time_slice(force_drop_end, None)[0]\n",
    "                    if next_force_shoulder_end > next_force_rise_start:\n",
    "                        # next swallow did not have a shoulder and we instead grabbed a later shoulder\n",
    "                        next_force_shoulder_end = df.loc[i, 'Next force shoulder end (s)'] = np.nan\n",
    "                except IndexError:\n",
    "                    next_force_shoulder_end = df.loc[i, 'Next force shoulder end (s)'] = np.nan\n",
    "\n",
    "                # get the list of fixed times for normalization\n",
    "                force_segmentation_times = df.at[i, 'Force segmentation times (s)'] = np.array([\n",
    "                    prev_force_plateau_start,\n",
    "                    prev_force_plateau_end,\n",
    "                    prev_force_drop_end,\n",
    "                    force_shoulder_end,\n",
    "                    force_rise_start,\n",
    "                    force_plateau_start,\n",
    "                    force_plateau_end,\n",
    "                    force_drop_end,\n",
    "                    next_force_shoulder_end,\n",
    "                    next_force_rise_start,\n",
    "                ])*pq.s # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "            else: # force is not segmented\n",
    "                force_segmentation_times = df.at[i, 'Force segmentation times (s)'] = np.array([\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                    np.nan,\n",
    "                ])*pq.s # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### BEHAVIORAL MARKERS\n",
    "            ###\n",
    "            \n",
    "            if video_is_segmented:\n",
    "                df.loc[i, 'Inward movement start (video seg normalized)'] = \\\n",
    "                    normalize_time(video_segmentation_times.magnitude, float(inward_movement_start))\n",
    "                df.loc[i, 'Inward movement end (video seg normalized)'] = \\\n",
    "                    normalize_time(video_segmentation_times.magnitude, float(inward_movement_end))\n",
    "            \n",
    "            if force_is_segmented:\n",
    "                df.loc[i, 'Inward movement start (force seg normalized)'] = \\\n",
    "                    normalize_time(force_segmentation_times.magnitude, float(inward_movement_start))\n",
    "                df.loc[i, 'Inward movement end (force seg normalized)'] = \\\n",
    "                    normalize_time(force_segmentation_times.magnitude, float(inward_movement_end))\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### FORCE QUANTIFICATION\n",
    "            ###\n",
    "\n",
    "            if force_is_segmented:\n",
    "\n",
    "                # get smoothed force for whole behavior for remaining force calculations\n",
    "                sig = force_smoothed_sig\n",
    "                if np.isfinite(force_shoulder_end):\n",
    "                    sig = sig.time_slice(force_shoulder_end - 0.01*pq.s, force_drop_end + 0.01*pq.s)\n",
    "                else:\n",
    "                    sig = sig.time_slice(force_rise_start - 1*pq.s, force_drop_end + 0.01*pq.s)\n",
    "                sig = sig.rescale('mN')\n",
    "\n",
    "                # find force peak, baseline, and the increase\n",
    "                force_min_time = df.loc[i, 'Force minimum time (s)'] = elephant.spike_train_generation.peak_detection(sig, 999*pq.mN, sign='below')[0]\n",
    "                force_min = df.loc[i, 'Force minimum (mN)'] = sig[sig.time_index(force_min_time)][0]\n",
    "                force_peak_time = df.loc[i, 'Force peak time (s)'] = elephant.spike_train_generation.peak_detection(sig, 0*pq.mN)[0]\n",
    "                force_peak = df.loc[i, 'Force peak (mN)'] = sig[sig.time_index(force_peak_time)][0]\n",
    "                force_baseline = df.loc[i, 'Force baseline (mN)'] = sig[sig.time_index(force_rise_start)][0]\n",
    "                force_increase = df.loc[i, 'Force increase (mN)'] = force_peak-force_baseline\n",
    "\n",
    "                # find force plateau, drop, and shoulder values\n",
    "                force_plateau_start_value = df.loc[i, 'Force plateau start value (mN)'] = sig[sig.time_index(force_plateau_start)][0]\n",
    "                force_plateau_end_value = df.loc[i, 'Force plateau end value (mN)'] = sig[sig.time_index(force_plateau_end)][0]\n",
    "                force_drop_end_value = df.loc[i, 'Force drop end value (mN)'] = sig[sig.time_index(force_drop_end)][0]\n",
    "                if np.isfinite(force_shoulder_end):\n",
    "                    force_shoulder_end_value = df.loc[i, 'Force shoulder end value (mN)'] = sig[sig.time_index(force_shoulder_end)][0]\n",
    "                else:\n",
    "                    force_shoulder_end_value = np.nan\n",
    "\n",
    "                # find force rise and plateau durations\n",
    "                force_rise_duration = df.loc[i, 'Force rise duration (s)'] = force_plateau_start - force_rise_start\n",
    "                force_plateau_duration = df.loc[i, 'Force plateau duration (s)'] = force_plateau_end - force_plateau_start\n",
    "                force_rise_plateau_duration = df.loc[i, 'Force rise and plateau duration (s)'] = force_plateau_end - force_rise_start\n",
    "\n",
    "                # find average slope during rising phase\n",
    "                force_rise_increase = df.loc[i, 'Force rise increase (mN)'] = force_plateau_start_value - force_baseline\n",
    "                force_slope = df.loc[i, 'Force slope (mN/s)'] = (force_rise_increase/force_rise_duration).rescale('mN/s')\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### FORCE NORMALIZATION\n",
    "            ###\n",
    "\n",
    "            if video_is_segmented:\n",
    "                t_start = video_segmentation_times[0]-0.001*pq.s\n",
    "                t_stop = video_segmentation_times[-1]+0.001*pq.s\n",
    "                \n",
    "                channel = 'Force'\n",
    "                sig = get_sig(blk, channel)\n",
    "                sig = sig.time_slice(t_start, t_stop)\n",
    "                sig = sig.rescale(channel_units[channel_names.index(channel)])\n",
    "\n",
    "                force_video_seg_interp = df.at[i, 'Force, video segmented interpolation (mN)'] = \\\n",
    "                    resample_sig_in_normalized_time(video_segmentation_times, sig, video_seg_interp_times) # 'at', not 'loc', is important for inserting list into cell\n",
    "            \n",
    "            if force_is_segmented:\n",
    "                t_start = force_segmentation_times[0]-0.001*pq.s\n",
    "                t_stop = force_segmentation_times[-1]+0.001*pq.s\n",
    "                \n",
    "                channel = 'Force'\n",
    "                sig = get_sig(blk, channel)\n",
    "                sig = sig.time_slice(t_start, t_stop)\n",
    "                sig = sig.rescale(channel_units[channel_names.index(channel)])\n",
    "\n",
    "                force_force_seg_interp = df.at[i, 'Force, force segmented interpolation (mN)'] = \\\n",
    "                    resample_sig_in_normalized_time(force_segmentation_times, sig, force_seg_interp_times) # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### FIND SPIKE TRAINS\n",
    "            ###\n",
    "\n",
    "            if lazy:\n",
    "                if metadata['amplitude_discriminators'] is not None:\n",
    "                    for discriminator in metadata['amplitude_discriminators']:\n",
    "                        sig = get_sig(blk, discriminator['channel'])\n",
    "                        if sig is not None:\n",
    "                            sig = sig.time_slice(behavior_start - 5*pq.s, behavior_end + 5*pq.s)\n",
    "                            st = _detect_spikes(sig, discriminator, blk.segments[0].epochs)\n",
    "                            st_epoch_start = df.loc[i, discriminator['epoch']+' start (s)']*pq.s\n",
    "                            st_epoch_end = df.loc[i, discriminator['epoch']+' end (s)']*pq.s\n",
    "                            st = st.time_slice(st_epoch_start, st_epoch_end)\n",
    "                            df.at[i, f'{st.name} spike train'] = st # 'at', not 'loc', is important for inserting list into cell\n",
    "            else:\n",
    "                for spiketrain in blk.segments[0].spiketrains:\n",
    "                    discriminator = next((d for d in metadata['amplitude_discriminators'] if d['name'] == spiketrain.name), None)\n",
    "                    if discriminator is None:\n",
    "                        raise Exception(f'For data set \"{data_set_name}\", discriminator \"{spiketrain.name}\" could not be found')\n",
    "                    st_epoch_start = df.loc[i, discriminator['epoch']+' start (s)']*pq.s\n",
    "                    st_epoch_end = df.loc[i, discriminator['epoch']+' end (s)']*pq.s\n",
    "                    if np.isfinite(st_epoch_start) and np.isfinite(st_epoch_end):\n",
    "                        st = spiketrain.time_slice(st_epoch_start, st_epoch_end)\n",
    "                    else:\n",
    "                        # this unit's discriminator epoch was not located for this swallow\n",
    "                        st = None\n",
    "                    df.at[i, f'{spiketrain.name} spike train'] = st # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### QUANTIFY SPIKE TRAINS AND BURSTS\n",
    "            ###\n",
    "\n",
    "            for k, unit in enumerate(units):\n",
    "                st = df.loc[i, f'{unit} spike train']\n",
    "                if st is not None:\n",
    "\n",
    "                    # get the neural channel\n",
    "                    channel = st.annotations['channels'][0]\n",
    "                    sig = get_sig(blk, channel)\n",
    "\n",
    "                    # create a continuous smoothed firing rate representation\n",
    "                    # by convolving the spike train with a kernel\n",
    "                    # - choice of t_start and t_stop here ensures firing rates are\n",
    "                    #   recorded as zero far from the burst and can be resampled later\n",
    "                    if video_is_segmented and force_is_segmented:\n",
    "                        t_start = min(video_segmentation_times[0], force_segmentation_times[0])-0.001*pq.s\n",
    "                        t_stop = max(video_segmentation_times[-1], force_segmentation_times[-1])+0.001*pq.s\n",
    "                    elif video_is_segmented:\n",
    "                        t_start = video_segmentation_times[0]-0.001*pq.s\n",
    "                        t_stop = video_segmentation_times[-1]+0.001*pq.s\n",
    "                    elif force_is_segmented:\n",
    "                        t_start = force_segmentation_times[0]-0.001*pq.s\n",
    "                        t_stop = force_segmentation_times[-1]+0.001*pq.s\n",
    "                    else:\n",
    "                        # no segmentation available\n",
    "                        t_start = behavior_start-5*pq.s\n",
    "                        t_stop = behavior_end+5*pq.s\n",
    "                    smoothing_kernel = elephant.kernels.GaussianKernel(0.2*pq.s) # 200 ms standard deviation\n",
    "                    firing_rate = df.at[i, f'{unit} firing rate (Hz)'] = elephant.statistics.instantaneous_rate(\n",
    "                        spiketrain=st,\n",
    "                        t_start=t_start,\n",
    "                        t_stop=t_stop,\n",
    "                        sampling_period=sig.sampling_period,\n",
    "                        kernel=smoothing_kernel,\n",
    "                    ) # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "                    # normalization\n",
    "                    if video_is_segmented:\n",
    "                        firing_rate_video_seg_interp = df.at[i, f'{unit} firing rate, video segmented interpolation (Hz)'] = \\\n",
    "                            resample_sig_in_normalized_time(video_segmentation_times, firing_rate, video_seg_interp_times) # 'at', not 'loc', is important for inserting list into cell\n",
    "                    if force_is_segmented:\n",
    "                        firing_rate_force_seg_interp = df.at[i, f'{unit} firing rate, force segmented interpolation (Hz)'] = \\\n",
    "                            resample_sig_in_normalized_time(force_segmentation_times, firing_rate, force_seg_interp_times) # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "                    if st.size > 0:\n",
    "\n",
    "#                         # get the signal for the behavior with 10 seconds cushion before and after (for better baseline estimation)\n",
    "#                         sig = sig.time_slice(behavior_start - 10*pq.s, behavior_end + 10*pq.s)\n",
    "#                         sig = sig.rescale(channel_units[channel_names.index(channel)])\n",
    "\n",
    "                        # find every sequence of spikes that qualifies as a burst\n",
    "                        bursts = df.at[i, f'{unit} all bursts'] = _find_bursts(st, burst_thresholds[unit][0], burst_thresholds[unit][1]) # 'at', not 'loc', is important for inserting list into cell\n",
    "\n",
    "                        first_burst_start = np.nan\n",
    "#                         first_burst_end = np.nan\n",
    "#                         first_burst_spike_count = 0\n",
    "#                         first_burst_mean_freq = 0*pq.Hz\n",
    "#                         last_burst_start = np.nan\n",
    "                        last_burst_end = np.nan\n",
    "#                         last_burst_spike_count = 0\n",
    "#                         last_burst_mean_freq = 0*pq.Hz\n",
    "                        if len(bursts) > 0:\n",
    "\n",
    "                            for burst in zip(bursts.times, bursts.durations, bursts.array_annotations['spikes']):\n",
    "                                if is_good_burst(burst):\n",
    "                                    time, duration, n_spikes = burst\n",
    "                                    first_burst_start = time\n",
    "#                                     first_burst_start, first_burst_end = burst['Start (s)'], burst['End (s)']\n",
    "#                                     first_burst_duration = first_burst_end-first_burst_start\n",
    "#                                     df.loc[i, f'{unit} first burst start (s)'] = first_burst_start.rescale('s')\n",
    "#                                     df.loc[i, f'{unit} first burst end (s)'] = first_burst_end.rescale('s')\n",
    "#                                     first_burst_duration = df.loc[i, f'{unit} first burst duration (s)'] = first_burst_duration.rescale('s')\n",
    "#                                     first_burst_spike_count = df.loc[i, f'{unit} first burst spike count'] = st.time_slice(first_burst_start, first_burst_end).size\n",
    "#                                     first_burst_mean_freq = df.loc[i, f'{unit} first burst mean frequency (Hz)'] = ((first_burst_spike_count-1)/first_burst_duration).rescale('Hz')\n",
    "\n",
    "#                                     # find burst RAUC and mean voltage\n",
    "#                                     first_burst_rauc = df.loc[i, f'{unit} first burst RAUC (Î¼VÂ·s)'] = elephant.signal_processing.rauc(sig, baseline='mean', t_start=first_burst_start, t_stop=first_burst_end).rescale('uV*s')\n",
    "#                                     first_burst_mean_rect_voltage = df.loc[i, f'{unit} first burst mean rectified voltage (Î¼V)'] = first_burst_rauc/first_burst_duration\n",
    "\n",
    "                                    break # quit after finding first good burst\n",
    "\n",
    "                            for burst in zip(reversed(bursts.times), reversed(bursts.durations), reversed(bursts.array_annotations['spikes'])):\n",
    "                                if is_good_burst(burst):\n",
    "                                    time, duration, n_spikes = burst\n",
    "                                    last_burst_end = time + duration\n",
    "#                                     last_burst_start, last_burst_end = burst['Start (s)'], burst['End (s)']\n",
    "#                                     last_burst_duration = last_burst_end-last_burst_start\n",
    "#                                     df.loc[i, f'{unit} last burst start (s)'] = last_burst_start.rescale('s')\n",
    "#                                     df.loc[i, f'{unit} last burst end (s)'] = last_burst_end.rescale('s')\n",
    "#                                     last_burst_duration = df.loc[i, f'{unit} last burst duration (s)'] = last_burst_duration.rescale('s')\n",
    "#                                     last_burst_spike_count = df.loc[i, f'{unit} last burst spike count'] = st.time_slice(last_burst_start, last_burst_end).size\n",
    "#                                     last_burst_mean_freq = df.loc[i, f'{unit} last burst mean frequency (Hz)'] = ((last_burst_spike_count-1)/last_burst_duration).rescale('Hz')\n",
    "\n",
    "#                                     # find burst RAUC and mean voltage\n",
    "#                                     last_burst_rauc = df.loc[i, f'{unit} last burst RAUC (Î¼VÂ·s)'] = elephant.signal_processing.rauc(sig, baseline='mean', t_start=last_burst_start, t_stop=last_burst_end).rescale('uV*s')\n",
    "#                                     last_burst_mean_rect_voltage = df.loc[i, f'{unit} last burst mean rectified voltage (Î¼V)'] = last_burst_rauc/last_burst_duration\n",
    "                                    \n",
    "                                    break # quit after finding first (actually, last) good burst\n",
    "                        \n",
    "                        # merge the first and last good bursts and anything in between\n",
    "                        if np.isfinite(first_burst_start) and np.isfinite(last_burst_end):\n",
    "                            st_burst = st.time_slice(first_burst_start, last_burst_end)\n",
    "                            burst_start = df.loc[i, f'{unit} burst start (s)'] = first_burst_start\n",
    "                            burst_end = df.loc[i, f'{unit} burst end (s)'] = last_burst_end\n",
    "                            burst_duration = df.loc[i, f'{unit} burst duration (s)'] = (burst_end-burst_start)\n",
    "                            burst_spike_count = df.loc[i, f'{unit} burst spike count'] = st_burst.size\n",
    "                            if burst_spike_count > 1:\n",
    "                                burst_mean_freq = df.loc[i, f'{unit} burst mean frequency (Hz)'] = ((burst_spike_count-1)/burst_duration).rescale('Hz')\n",
    "                            if video_is_segmented:\n",
    "                                df.loc[i, f'{unit} burst start (video seg normalized)'] = normalize_time(video_segmentation_times.magnitude, float(first_burst_start))\n",
    "                                df.loc[i, f'{unit} burst end (video seg normalized)']   = normalize_time(video_segmentation_times.magnitude, float(last_burst_end))\n",
    "                            if force_is_segmented:\n",
    "                                df.loc[i, f'{unit} burst start (force seg normalized)'] = normalize_time(force_segmentation_times.magnitude, float(first_burst_start))\n",
    "                                df.loc[i, f'{unit} burst end (force seg normalized)']   = normalize_time(force_segmentation_times.magnitude, float(last_burst_end))\n",
    "\n",
    "            # B3/B6/B9\n",
    "            df['B3/B6/B9 burst start (s)'] = np.nan\n",
    "            df['B3/B6/B9 burst end (s)'] = np.nan\n",
    "            df['B3/B6/B9 burst duration (s)'] = 0\n",
    "            df['B3/B6/B9 burst spike count'] = 0\n",
    "            df['B3/B6/B9 burst mean frequency (Hz)'] = 0\n",
    "            b3b6b9_burst_start = df['B3/B6/B9 burst start (s)'] = df[['B6/B9 burst start (s)', 'B3 burst start (s)']].min(axis=1)\n",
    "            b3b6b9_burst_end = df['B3/B6/B9 burst end (s)']   = df[['B6/B9 burst end (s)',   'B3 burst end (s)']]  .max(axis=1)\n",
    "            b3b6b9_burst_duration = df['B3/B6/B9 burst duration (s)'] = b3b6b9_burst_end - b3b6b9_burst_start\n",
    "            b3b6b9_burst_spike_count = df['B3/B6/B9 burst spike count'] = df['B6/B9 burst spike count'] + df['B3 burst spike count']\n",
    "            b3b6b9_burst_mean_freq = df['B3/B6/B9 burst mean frequency (Hz)'] = (b3b6b9_burst_spike_count-1)/b3b6b9_burst_duration\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### FINISH\n",
    "        ###\n",
    "\n",
    "        # index the table on 4 variables so that this dataframe can later be merged with others\n",
    "        df['Animal'] = animal\n",
    "        df['Food'] = food\n",
    "        df['Bout_index'] = bout_index\n",
    "        df = df.reset_index().set_index(['Animal', 'Food', 'Bout_index', 'Behavior_index'])\n",
    "\n",
    "        df_list += [df]\n",
    "        \n",
    "        pbar.update()\n",
    "        \n",
    "    pbar.close()\n",
    "\n",
    "    df_all = pd.concat(df_list, sort=False).sort_index()\n",
    "\n",
    "    if exemplary_swallow in feeding_bouts:\n",
    "        # move exemplar to separate dataframe\n",
    "        df_exemplary_swallow = df_all.loc[exemplary_swallow].copy()\n",
    "        df_all = df_all.drop(exemplary_swallow)\n",
    "\n",
    "    if exemplary_bout in feeding_bouts:\n",
    "        # move exemplar to separate dataframe\n",
    "        df_exemplary_bout = df_all.loc[exemplary_bout].copy()\n",
    "        df_all = df_all.drop(exemplary_bout)\n",
    "\n",
    "    # save dataframes to files so that calculations can be skipped in the future\n",
    "    for var in pickled_vars:\n",
    "        exec(f'{var}.to_pickle(\"{var}.zip\")')\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print('elapsed time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤ª Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_sanity_checks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_sanity_checks:\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # reconstruct the original df_all, before exemplary_swallow and\n",
    "    # exemplary_bout were removed\n",
    "    df_list2 = [df_all]\n",
    "    if exemplary_swallow in feeding_bouts:\n",
    "        df_exemplary_swallow2 = df_exemplary_swallow.copy()\n",
    "        df_exemplary_swallow2['Animal'] = exemplary_swallow[0]\n",
    "        df_exemplary_swallow2['Food'] = exemplary_swallow[1]\n",
    "        df_exemplary_swallow2['Bout_index'] = exemplary_swallow[2]\n",
    "        df_exemplary_swallow2 = df_exemplary_swallow2.reset_index().set_index(['Animal', 'Food', 'Bout_index', 'Behavior_index'])\n",
    "        df_list2 += [df_exemplary_swallow2]\n",
    "    if exemplary_bout in feeding_bouts:\n",
    "        df_exemplary_bout2 = df_exemplary_bout.copy()\n",
    "        df_exemplary_bout2['Animal'] = exemplary_bout[0]\n",
    "        df_exemplary_bout2['Food'] = exemplary_bout[1]\n",
    "        df_exemplary_bout2['Bout_index'] = exemplary_bout[2]\n",
    "        df_exemplary_bout2 = df_exemplary_bout2.reset_index().set_index(['Animal', 'Food', 'Bout_index', 'Behavior_index'])\n",
    "        df_list2 += [df_exemplary_bout2]\n",
    "    df_all2 = pd.concat(df_list2, sort=False).sort_index()\n",
    "\n",
    "    # use Neo RawIO lazy loading to load much faster and using less memory\n",
    "    # - with lazy=True, filtering parameters specified in metadata are ignored\n",
    "    #     - note: filters are replaced below and applied manually anyway\n",
    "    # - with lazy=True, loading via time_slice requires neo>=0.8.0\n",
    "    lazy = True\n",
    "\n",
    "    # load the metadata containing file paths\n",
    "    metadata = neurotic.MetadataSelector(file='../../data/metadata.yml')\n",
    "\n",
    "    last_data_set_name = None\n",
    "    pbar = tqdm(total=len(feeding_bouts), unit='figure')\n",
    "    for (animal, food, bout_index), (data_set_name, time_window) in feeding_bouts.items():\n",
    "\n",
    "        channel_names = channel_names_by_animal[animal]\n",
    "        epoch_types = epoch_types_by_food[food]\n",
    "        burst_thresholds = burst_thresholds_by_animal[animal]\n",
    "\n",
    "        df = df_all2.loc[animal, food, bout_index]\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### LOAD DATASET\n",
    "        ###\n",
    "\n",
    "        metadata.select(data_set_name)\n",
    "\n",
    "        if data_set_name is last_data_set_name:\n",
    "            # skip reloading the data if it's already in memory\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            # ensure that the right filters are used\n",
    "            metadata['filters'] = sig_filters_by_animal[animal]\n",
    "\n",
    "            blk = neurotic.load_dataset(metadata, lazy=lazy)\n",
    "\n",
    "            if lazy:\n",
    "                # manually perform filters\n",
    "                blk = apply_filters(blk, metadata)\n",
    "\n",
    "        last_data_set_name = data_set_name\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### START FIGURE\n",
    "        ###\n",
    "\n",
    "#             figsize = (9.5, 10) # dimensions for notebook\n",
    "#             figsize = (11, 8.5) # dimensions for printing\n",
    "        figsize = (16, 9) # dimensions for filling wide screens\n",
    "        fig, axes = plt.subplots(len(channel_names), 1, sharex=True, figsize=figsize)\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### PLOT SIGNALS\n",
    "        ###\n",
    "\n",
    "        # plot all channels for entire time window\n",
    "        for i, channel in enumerate(channel_names):\n",
    "            plt.sca(axes[i])\n",
    "            sig = get_sig(blk, channel)\n",
    "            sig = sig.time_slice(time_window[0]*pq.s, time_window[1]*pq.s)\n",
    "            sig = sig.rescale(channel_units[i])\n",
    "            plt.plot(sig.times, sig.magnitude, c='0.8', lw=1, zorder=-1)\n",
    "\n",
    "            if i == 0:\n",
    "                plt.title(f'({animal}, {food}, {bout_index}): {data_set_name}')\n",
    "\n",
    "            plt.ylabel(sig.name + ' (' + sig.units.dimensionality.string + ')')\n",
    "            axes[i].yaxis.set_label_coords(-0.06, 0.5)\n",
    "\n",
    "            if i < len(channel_names)-1:\n",
    "                # remove right, top, and bottom plot borders, and remove x-axis\n",
    "                sns.despine(ax=plt.gca(), bottom=True)\n",
    "                plt.gca().xaxis.set_visible(False)\n",
    "            else:\n",
    "                # remove right and top plot borders, and set x-label\n",
    "                sns.despine(ax=plt.gca())\n",
    "                plt.xlabel('Time (s)')\n",
    "\n",
    "        # plot smoothed force for entire time window\n",
    "        channel = 'Force'\n",
    "        sig = get_sig(blk, channel)\n",
    "        if lazy:\n",
    "            sig = sig.time_slice(None, None)\n",
    "        sig = elephant.signal_processing.butter(sig, lowpass_freq = 5*pq.Hz)\n",
    "        sig = sig.time_slice(time_window[0]*pq.s, time_window[1]*pq.s)\n",
    "        sig = sig.rescale('mN')\n",
    "        plt.plot(sig.times, sig.magnitude, c='k', lw=1, zorder=0)\n",
    "        force_smoothed_sig = sig\n",
    "\n",
    "\n",
    "\n",
    "        # iterate over all swallows\n",
    "        for j, i in enumerate(df.index):\n",
    "\n",
    "            behavior_start = df.loc[i, 'Start (s)']*pq.s\n",
    "            behavior_end   = df.loc[i, 'End (s)']*pq.s\n",
    "            \n",
    "            ###\n",
    "            ### MOVEMENTS\n",
    "            ###\n",
    "            \n",
    "            # plot inward food movement\n",
    "            inward_movement_start = df.loc[i, 'Inward movement start (s)']*pq.s\n",
    "            inward_movement_end   = df.loc[i, 'Inward movement end (s)']*pq.s\n",
    "            if np.isfinite(inward_movement_start):\n",
    "                channel = 'Force'\n",
    "                ax = axes[channel_names.index(channel)]\n",
    "                ax.axvspan(\n",
    "                    inward_movement_start, inward_movement_end,\n",
    "                    0.99, 1,\n",
    "                    facecolor='k', edgecolor=None, lw=0)\n",
    "            \n",
    "            \n",
    "\n",
    "            ###\n",
    "            ### FORCE SEGMENTATION\n",
    "            ###\n",
    "\n",
    "            force_shoulder_end  = df.loc[i, 'Force shoulder end start (s)']*pq.s  # start of \"Force shoulder end\" epoch\n",
    "            force_rise_start    = df.loc[i, 'Force rise start start (s)']*pq.s    # start of \"Force rise start\" epoch\n",
    "            force_plateau_start = df.loc[i, 'Force plateau start start (s)']*pq.s # start of \"Force plateau start\" epoch\n",
    "            force_plateau_end   = df.loc[i, 'Force plateau end start (s)']*pq.s   # start of \"Force plateau end\" epoch\n",
    "            force_drop_end      = df.loc[i, 'Force drop end start (s)']*pq.s      # start of \"Force drop end\" epoch\n",
    "\n",
    "            # force rise start, plateau start and end, and drop end are required\n",
    "            force_is_segmented = np.all(np.isfinite(np.array([\n",
    "                force_rise_start, force_plateau_start, force_plateau_end, force_drop_end])))\n",
    "\n",
    "            force_segmentation_times = df.at[i, 'Force segmentation times (s)']\n",
    "\n",
    "            if force_is_segmented:\n",
    "\n",
    "                force_min_time = df.loc[i, 'Force minimum time (s)']\n",
    "                force_min = df.loc[i, 'Force minimum (mN)']\n",
    "                force_peak_time = df.loc[i, 'Force peak time (s)']\n",
    "                force_peak = df.loc[i, 'Force peak (mN)']\n",
    "                force_baseline = df.loc[i, 'Force baseline (mN)']\n",
    "\n",
    "                force_plateau_start_value = df.loc[i, 'Force plateau start value (mN)']\n",
    "                force_plateau_end_value = df.loc[i, 'Force plateau end value (mN)']\n",
    "                force_drop_end_value = df.loc[i, 'Force drop end value (mN)']\n",
    "                force_shoulder_end_value = df.loc[i, 'Force shoulder end value (mN)']\n",
    "\n",
    "                # get smoothed force for whole behavior for remaining force calculations\n",
    "                sig = force_smoothed_sig\n",
    "                if np.isfinite(force_shoulder_end):\n",
    "                    sig = sig.time_slice(force_shoulder_end - 0.01*pq.s, force_drop_end + 0.01*pq.s)\n",
    "                else:\n",
    "                    sig = sig.time_slice(force_rise_start - 1*pq.s, force_drop_end + 0.01*pq.s)\n",
    "                sig = sig.rescale('mN')\n",
    "\n",
    "                # plot force rise in color\n",
    "                plt.sca(axes[channel_names.index('Force')])\n",
    "                sig2 = sig.time_slice(force_rise_start, force_plateau_start)\n",
    "                plt.plot(sig2.times, sig2.magnitude, c=force_colors['rise'], lw=2, zorder=1)\n",
    "\n",
    "                # plot force plateau in color\n",
    "                sig2 = sig.time_slice(force_plateau_start, force_plateau_end)\n",
    "                plt.plot(sig2.times, sig2.magnitude, c=force_colors['plateau'], lw=2, zorder=1)\n",
    "\n",
    "                # plot force shoulder in color\n",
    "                if np.isfinite(force_shoulder_end):\n",
    "                    sig2 = sig.time_slice(force_drop_end, force_shoulder_end)\n",
    "                    plt.plot(sig2.times, sig2.magnitude, c=force_colors['shoulder'], lw=2, zorder=1)\n",
    "\n",
    "                # plot force peak, baseline, and plateau values\n",
    "                plt.plot([force_peak_time],     [force_peak],                marker=CARETDOWN,  markersize=5, color='k')\n",
    "#                 plt.plot([force_min_time],      [force_min],                 marker=CARETUP,    markersize=5, color='k')\n",
    "                plt.plot([force_rise_start],    [force_baseline],            marker=CARETUP,    markersize=5, color='k')\n",
    "                plt.plot([force_plateau_start], [force_plateau_start_value], marker=CARETRIGHT, markersize=5, color='k')\n",
    "                plt.plot([force_plateau_end],   [force_plateau_end_value],   marker=CARETLEFT,  markersize=5, color='k')\n",
    "\n",
    "                # plot segmentation boundaries across all subplots\n",
    "                for (t, y, c) in [\n",
    "                        (force_shoulder_end,  force_shoulder_end_value,  force_colors['shoulder']),\n",
    "                        (force_rise_start,    force_baseline,            force_colors['rise']),\n",
    "                        (force_plateau_start, force_plateau_start_value, force_colors['plateau']),\n",
    "                        (force_plateau_end,   force_plateau_end_value,   force_colors['plateau']),\n",
    "                        (force_drop_end,      force_drop_end_value,      force_colors['drop'])]:\n",
    "                    if np.isfinite(y):\n",
    "                        axes[-1].add_artist(patches.ConnectionPatch(\n",
    "                            xyA=(t, y), xyB=(t, 1),\n",
    "                            coordsA='data', coordsB=axes[0].get_xaxis_transform(),\n",
    "                            axesA=axes[-1], axesB=axes[0],\n",
    "                            color=c, lw=1, ls=':', zorder=-2))\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            ### SPIKES AND BURSTS\n",
    "            ###\n",
    "\n",
    "            for k, unit in enumerate(units):\n",
    "                st = df.loc[i, f'{unit} spike train']\n",
    "                if st is not None and st.size > 0:\n",
    "\n",
    "                    # get the signal for the behavior with 10 seconds cushion for spikes outside the behavior duration\n",
    "                    channel = st.annotations['channels'][0]\n",
    "                    sig = get_sig(blk, channel)\n",
    "                    sig = sig.time_slice(behavior_start - 10*pq.s, behavior_end + 10*pq.s)\n",
    "                    sig = sig.rescale(channel_units[channel_names.index(channel)])\n",
    "\n",
    "                    # plot spikes\n",
    "                    plt.sca(axes[channel_names.index(channel)])\n",
    "                    marker = ['.', 'x'][j%2] # alternate markers between behaviors\n",
    "                    spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "                    plt.scatter(st.times.rescale('s'), spike_amplitudes, marker=marker, c=unit_colors[unit])\n",
    "\n",
    "                    # plot burst windows\n",
    "                    discriminator = next((d for d in metadata['amplitude_discriminators'] if d['name'] == st.name), None)\n",
    "                    if discriminator is None:\n",
    "                        raise Exception(f'For data set \"{data_set_name}\", discriminator \"{st.name}\" could not be found')\n",
    "                    bottom = pq.Quantity(discriminator['amplitude'][0], discriminator['units']).rescale(sig.units)\n",
    "                    top = pq.Quantity(discriminator['amplitude'][1], discriminator['units']).rescale(sig.units)\n",
    "                    height = top-bottom\n",
    "                    bursts = df.at[i, f'{unit} all bursts']\n",
    "                    for burst in zip(bursts.times, bursts.durations, bursts.array_annotations['spikes']):\n",
    "                        time, duration, n_spikes = burst\n",
    "                        left = time\n",
    "                        right = time + duration\n",
    "                        width = right-left\n",
    "                        linestyle = '-' if is_good_burst(burst) else '--'\n",
    "                        rect = patches.Rectangle((left, bottom), width, height, ls=linestyle, edgecolor=unit_colors[unit], fill=False)\n",
    "                        plt.gca().add_patch(rect)\n",
    "\n",
    "                    # plot markers for edges of bursts\n",
    "                    burst_start = df.loc[i, f'{unit} burst start (s)']\n",
    "                    burst_end = df.loc[i, f'{unit} burst end (s)']\n",
    "                    if top > 0:\n",
    "                        plt.plot([burst_start], [top],    marker=CARETDOWN, markersize=5, color='k')\n",
    "                        plt.plot([burst_end],   [top],    marker=CARETDOWN, markersize=5, color='k')\n",
    "                    else:\n",
    "                        plt.plot([burst_start], [bottom], marker=CARETUP,   markersize=5, color='k')\n",
    "                        plt.plot([burst_end],   [bottom], marker=CARETUP,   markersize=5, color='k')\n",
    "\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### FINISH FIGURE\n",
    "        ###\n",
    "\n",
    "        # optimize plot margins\n",
    "        plt.subplots_adjust(\n",
    "            left   = 0.1,\n",
    "            right  = 0.99,\n",
    "            top    = 0.96,\n",
    "            bottom = 0.06,\n",
    "            hspace = 0.15,\n",
    "        )\n",
    "\n",
    "        # export figure\n",
    "        export_dir2 = os.path.join(export_dir, 'sanity-checks')\n",
    "        if not os.path.exists(export_dir2):\n",
    "            os.mkdir(export_dir2)\n",
    "        plt.gcf().savefig(os.path.join(export_dir2, f'{animal} {food} {bout_index}.png'), dpi=300)\n",
    "        \n",
    "        pbar.update()\n",
    "        \n",
    "    pbar.close()\n",
    "\n",
    "    if exemplary_swallow in feeding_bouts:\n",
    "        # rename output file for exemplar\n",
    "        animal, food, bout_index = exemplary_swallow\n",
    "        old_path = os.path.join(export_dir2, f'{animal} {food} {bout_index}.png')\n",
    "        new_path = os.path.join(export_dir2, 'exemplary_swallow.png')\n",
    "        if os.path.exists(old_path):\n",
    "            if os.path.exists(new_path):\n",
    "                os.remove(new_path)\n",
    "            os.rename(old_path, new_path)\n",
    "\n",
    "    if exemplary_bout in feeding_bouts:\n",
    "        # rename output file for exemplar\n",
    "        animal, food, bout_index = exemplary_bout\n",
    "        old_path = os.path.join(export_dir2, f'{animal} {food} {bout_index}.png')\n",
    "        new_path = os.path.join(export_dir2, 'exemplary_bout.png')\n",
    "        if os.path.exists(old_path):\n",
    "            if os.path.exists(new_path):\n",
    "                os.remove(new_path)\n",
    "            os.rename(old_path, new_path)\n",
    "\n",
    "\n",
    "    del df_all2, df_list2\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print('elapsed time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_sanity_checks:\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    if exemplary_swallow in feeding_bouts:\n",
    "        pbar = tqdm(total=len(feeding_bouts)-1, unit='figure')\n",
    "    else:\n",
    "        pbar = tqdm(total=len(feeding_bouts), unit='figure')\n",
    "    for (animal, food, bout_index), (data_set_name, time_window) in feeding_bouts.items():\n",
    "\n",
    "        channel_names = channel_names_by_animal[animal]\n",
    "\n",
    "        if (animal, food, bout_index) == exemplary_swallow:\n",
    "            # skip the lone swallow\n",
    "            continue\n",
    "        elif (animal, food, bout_index) == exemplary_bout:\n",
    "            df = df_exemplary_bout\n",
    "        else:\n",
    "            df = df_all.loc[animal, food, bout_index]\n",
    "\n",
    "        # load the data\n",
    "        metadata = neurotic.MetadataSelector('../../data/metadata.yml')\n",
    "        metadata.select(data_set_name)\n",
    "        blk = neurotic.load_dataset(metadata, lazy=True)\n",
    "\n",
    "        # figsize = (9.5, 10) # dimensions for notebook\n",
    "        # figsize = (11, 8.5) # dimensions for printing\n",
    "        # figsize = (16, 9) # dimensions for filling wide screens\n",
    "        figsize = (20, 9)\n",
    "        fig, axes = plt.subplots(len(units)+1, 3, sharex='col', figsize=figsize)\n",
    "\n",
    "        for k, unit in enumerate(units):\n",
    "            # get the subplot axes handles\n",
    "            ax_real_time, ax_force_seg, ax_video_seg = axes[k]\n",
    "\n",
    "            # set y-axis label\n",
    "            ax_real_time.set_ylabel(f'{unit} (Hz)')\n",
    "            ax_real_time.yaxis.set_label_coords(-0.06, 0.5)\n",
    "\n",
    "            # remove right, top, and bottom plot borders, and remove x-axis\n",
    "            sns.despine(ax=ax_real_time, bottom=True)\n",
    "            sns.despine(ax=ax_force_seg, bottom=True)\n",
    "            sns.despine(ax=ax_video_seg, bottom=True)\n",
    "            ax_real_time.xaxis.set_visible(False)\n",
    "            ax_force_seg.xaxis.set_visible(False)\n",
    "            ax_video_seg.xaxis.set_visible(False)\n",
    "\n",
    "            # set time plot ranges\n",
    "            ax_real_time.set_xlim([time_window[0]-5, time_window[1]+5])\n",
    "            ax_force_seg.set_xlim([force_seg_interp_times.min(), force_seg_interp_times.max()])\n",
    "#             ax_video_seg.set_xlim([video_seg_interp_times.min(), video_seg_interp_times.max()])\n",
    "            ax_video_seg.set_xlim([2, 6.5])\n",
    "\n",
    "            # elevate the Axes for units and remove background colors so that\n",
    "            # each vertical ConnectionPatch drawn later is visible behind it\n",
    "            ax_real_time.set_zorder(1)\n",
    "            ax_force_seg.set_zorder(1)\n",
    "            ax_video_seg.set_zorder(1)\n",
    "            ax_real_time.set_facecolor('none')\n",
    "            ax_force_seg.set_facecolor('none')\n",
    "            ax_video_seg.set_facecolor('none')\n",
    "\n",
    "        # remove right and top plot borders from bottom panels, and set x-label\n",
    "        ax_real_time, ax_force_seg, ax_video_seg = axes[-1]\n",
    "        sns.despine(ax=ax_real_time)\n",
    "        sns.despine(ax=ax_force_seg)\n",
    "        sns.despine(ax=ax_video_seg)\n",
    "        ax_real_time.set_xlabel('Time (s)')\n",
    "        ax_force_seg.set_xlabel('Time (normalized using force segmentation)')\n",
    "        ax_video_seg.set_xlabel('Time (normalized using video segmentation)')\n",
    "\n",
    "        # set time plot ranges\n",
    "        ax_real_time.set_xlim([time_window[0]-5, time_window[1]+5])\n",
    "        ax_force_seg.set_xlim([force_seg_interp_times.min(), force_seg_interp_times.max()])\n",
    "#         ax_video_seg.set_xlim([video_seg_interp_times.min(), video_seg_interp_times.max()])\n",
    "        ax_video_seg.set_xlim([2, 6.5])\n",
    "\n",
    "        # plot force in real time\n",
    "        ax_real_time, ax_force_seg, ax_video_seg = axes[-1]\n",
    "        channel = 'Force'\n",
    "        sig = get_sig(blk, channel)\n",
    "        sig = sig.time_slice(time_window[0]*pq.s, time_window[1]*pq.s)\n",
    "        sig = sig.rescale(channel_units[channel_names.index(channel)])\n",
    "        ax_real_time.plot(sig.times, sig.magnitude, c='0.8', lw=1)\n",
    "        ax_real_time.set_ylabel(sig.name + ' (' + sig.units.dimensionality.string + ')')\n",
    "        ax_real_time.yaxis.set_label_coords(-0.06, 0.5)\n",
    "\n",
    "        all_force_seg_normalized_times_series = {}\n",
    "        all_video_seg_normalized_times_series = {}\n",
    "        for unit in units:\n",
    "            all_force_seg_normalized_times_series[unit] = np.zeros((0, force_seg_interp_times.size))\n",
    "            all_video_seg_normalized_times_series[unit] = np.zeros((0, video_seg_interp_times.size))\n",
    "        all_force_seg_normalized_times_series['Force'] = np.zeros((0, force_seg_interp_times.size))\n",
    "        all_video_seg_normalized_times_series['Force'] = np.zeros((0, video_seg_interp_times.size))\n",
    "        \n",
    "        for j, i in enumerate(df.index):\n",
    "            \n",
    "            \n",
    "            # plot inward food movement\n",
    "            inward_movement_start = df.loc[i, 'Inward movement start (s)']\n",
    "            inward_movement_end   = df.loc[i, 'Inward movement end (s)']\n",
    "            if np.isfinite(inward_movement_start):\n",
    "                ax_real_time, ax_force_seg, ax_video_seg = axes[-1]\n",
    "                ax_real_time.axvspan(\n",
    "                    inward_movement_start, inward_movement_end,\n",
    "                    0.98, 1,\n",
    "                    facecolor='k', edgecolor=None, lw=0)\n",
    "            \n",
    "            \n",
    "            for k, unit in enumerate(units):\n",
    "                ax_real_time, ax_force_seg, ax_video_seg = axes[k]\n",
    "\n",
    "\n",
    "                # raster plot\n",
    "                st = df.loc[i, f'{unit} spike train']\n",
    "                if st is not None:\n",
    "                    ax_real_time.eventplot(positions=st, lineoffsets=-1, colors=unit_colors[unit])\n",
    "\n",
    "\n",
    "                # plot the firing rates in real time\n",
    "                firing_rate = df.loc[i, f'{unit} firing rate (Hz)']\n",
    "                if firing_rate is not None:\n",
    "                    ax_real_time.plot(firing_rate.times.rescale('s'), firing_rate, c=unit_colors[unit])\n",
    "\n",
    "\n",
    "                # plot firing rates in normalized time\n",
    "                firing_rate_force_seg_interp = df.loc[i, f'{unit} firing rate, force segmented interpolation (Hz)']\n",
    "                if firing_rate_force_seg_interp is not None:\n",
    "                    all_force_seg_normalized_times_series[unit] = np.concatenate([all_force_seg_normalized_times_series[unit], firing_rate_force_seg_interp[np.newaxis, :]])\n",
    "                    ax_force_seg.plot(force_seg_interp_times, firing_rate_force_seg_interp, c=unit_colors[unit])\n",
    "                firing_rate_video_seg_interp = df.loc[i, f'{unit} firing rate, video segmented interpolation (Hz)']\n",
    "                if firing_rate_video_seg_interp is not None:\n",
    "                    all_video_seg_normalized_times_series[unit] = np.concatenate([all_video_seg_normalized_times_series[unit], firing_rate_video_seg_interp[np.newaxis, :]])\n",
    "                    ax_video_seg.plot(video_seg_interp_times, firing_rate_video_seg_interp, c=unit_colors[unit])\n",
    "\n",
    "\n",
    "            # plot force in normalized time\n",
    "            ax_real_time, ax_force_seg, ax_video_seg = axes[-1]\n",
    "            force_force_seg_interp = df.at[i, 'Force, force segmented interpolation (mN)']\n",
    "            if force_force_seg_interp is not None:\n",
    "                all_force_seg_normalized_times_series['Force'] = np.concatenate([all_force_seg_normalized_times_series['Force'], force_force_seg_interp[np.newaxis, :]])\n",
    "                ax_force_seg.plot(force_seg_interp_times, force_force_seg_interp, c='0.8', lw=1)\n",
    "            force_video_seg_interp = df.at[i, 'Force, video segmented interpolation (mN)']\n",
    "            if force_video_seg_interp is not None:\n",
    "                all_video_seg_normalized_times_series['Force'] = np.concatenate([all_video_seg_normalized_times_series['Force'], force_video_seg_interp[np.newaxis, :]])\n",
    "                ax_video_seg.plot(video_seg_interp_times, force_video_seg_interp, c='0.8', lw=1)\n",
    "\n",
    "\n",
    "            # plot force phase boundaries in real time\n",
    "            force_segmentation_times = df.at[i, 'Force segmentation times (s)'].rescale('s').magnitude\n",
    "            for m, t in enumerate(force_segmentation_times[3:8]): # 3 = end of shoulder, 8 = end of next shoulder\n",
    "                if np.isfinite(t):\n",
    "                    if m == 1: # 1 = start of rise\n",
    "                        color = force_colors['rise']\n",
    "                    else:\n",
    "                        color = '0.75'\n",
    "                    axes[-1][0].add_artist(patches.ConnectionPatch(\n",
    "                        xyA=(t, 0), xyB=(t, 1),\n",
    "                        coordsA=axes[-1][0].get_xaxis_transform(), coordsB=axes[0][0].get_xaxis_transform(),\n",
    "                        axesA=axes[-1][0], axesB=axes[0][0],\n",
    "                        color=color, lw=1, ls=':'))\n",
    "\n",
    "\n",
    "        # plot force phase boundaries in normalized time\n",
    "        for m in range(len(force_segmentation_times)):\n",
    "            if m == 4: # 4 = start of rise\n",
    "                color = force_colors['rise']\n",
    "            else:\n",
    "                color = '0.75'\n",
    "            axes[-1][1].add_artist(patches.ConnectionPatch(\n",
    "                xyA=(m, 0), xyB=(m, 1),\n",
    "                coordsA=axes[-1][1].get_xaxis_transform(), coordsB=axes[0][1].get_xaxis_transform(),\n",
    "                axesA=axes[-1][1], axesB=axes[0][1],\n",
    "                color=color, lw=1, ls=':'))\n",
    "        \n",
    "        # plot video phase boundaries in normalized time\n",
    "        video_segmentation_times = df.at[i, 'Video segmentation times (s)'].rescale('s').magnitude # grab last swallow's as an example\n",
    "#         for m in range(len(video_segmentation_times)):\n",
    "        for m in [2, 3, 4, 5, 6]:\n",
    "            if m == 4: # 4 = start of inward movement\n",
    "                color = force_colors['rise']\n",
    "            else:\n",
    "                color = '0.75'\n",
    "            axes[-1][2].add_artist(patches.ConnectionPatch(\n",
    "                xyA=(m, 0), xyB=(m, 1),\n",
    "                coordsA=axes[-1][2].get_xaxis_transform(), coordsB=axes[0][2].get_xaxis_transform(),\n",
    "                axesA=axes[-1][2], axesB=axes[0][2],\n",
    "                color=color, lw=1, ls=':'))\n",
    "\n",
    "\n",
    "        # plot firing rate distributions\n",
    "        for k, unit in enumerate(units):\n",
    "            ax_real_time, ax_force_seg, ax_video_seg = axes[k]\n",
    "\n",
    "            firing_rate_median = np.nanmedian(all_force_seg_normalized_times_series[unit], axis=0)\n",
    "            firing_rate_q1 = np.nanquantile(all_force_seg_normalized_times_series[unit], q=0.25, axis=0)\n",
    "            firing_rate_q3 = np.nanquantile(all_force_seg_normalized_times_series[unit], q=0.75, axis=0)\n",
    "            ax_force_seg.plot(force_seg_interp_times, firing_rate_median, c='k', lw=2, zorder=3)\n",
    "            ax_force_seg.plot(force_seg_interp_times, firing_rate_q1, c='k', lw=2, ls='--')\n",
    "            ax_force_seg.plot(force_seg_interp_times, firing_rate_q3, c='k', lw=2, ls='--')\n",
    "            \n",
    "            firing_rate_median = np.nanmedian(all_video_seg_normalized_times_series[unit], axis=0)\n",
    "            firing_rate_q1 = np.nanquantile(all_video_seg_normalized_times_series[unit], q=0.25, axis=0)\n",
    "            firing_rate_q3 = np.nanquantile(all_video_seg_normalized_times_series[unit], q=0.75, axis=0)\n",
    "            ax_video_seg.plot(video_seg_interp_times, firing_rate_median, c='k', lw=2, zorder=3)\n",
    "            ax_video_seg.plot(video_seg_interp_times, firing_rate_q1, c='k', lw=2, ls='--')\n",
    "            ax_video_seg.plot(video_seg_interp_times, firing_rate_q3, c='k', lw=2, ls='--')\n",
    "\n",
    "\n",
    "        # plot force distribution\n",
    "        ax_real_time, ax_force_seg, ax_video_seg = axes[-1]\n",
    "\n",
    "        force_median = np.nanmedian(all_force_seg_normalized_times_series['Force'], axis=0)\n",
    "        force_q1 = np.nanquantile(all_force_seg_normalized_times_series['Force'], q=0.25, axis=0)\n",
    "        force_q3 = np.nanquantile(all_force_seg_normalized_times_series['Force'], q=0.75, axis=0)\n",
    "        ax_force_seg.plot(force_seg_interp_times, force_median, c='k', lw=2, zorder=3)\n",
    "        ax_force_seg.plot(force_seg_interp_times, force_q1, c='k', lw=2, ls='--')\n",
    "        ax_force_seg.plot(force_seg_interp_times, force_q3, c='k', lw=2, ls='--')\n",
    "        \n",
    "        force_median = np.nanmedian(all_video_seg_normalized_times_series['Force'], axis=0)\n",
    "        force_q1 = np.nanquantile(all_video_seg_normalized_times_series['Force'], q=0.25, axis=0)\n",
    "        force_q3 = np.nanquantile(all_video_seg_normalized_times_series['Force'], q=0.75, axis=0)\n",
    "        ax_video_seg.plot(video_seg_interp_times, force_median, c='k', lw=2, zorder=3)\n",
    "        ax_video_seg.plot(video_seg_interp_times, force_q1, c='k', lw=2, ls='--')\n",
    "        ax_video_seg.plot(video_seg_interp_times, force_q3, c='k', lw=2, ls='--')\n",
    "\n",
    "\n",
    "        plt.suptitle(f'({animal}, {food}, {bout_index}): {data_set_name}')\n",
    "        plt.tight_layout(rect=(0, 0, 1, 0.97))\n",
    "\n",
    "        # export figure\n",
    "        export_dir3 = os.path.join(export_dir, 'sanity-checks-firing-rates')\n",
    "        if not os.path.exists(export_dir3):\n",
    "            os.mkdir(export_dir3)\n",
    "        plt.gcf().savefig(os.path.join(export_dir3, f'{animal} {food} {bout_index}.png'), dpi=300)\n",
    "        \n",
    "        pbar.update()\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "    if exemplary_bout in feeding_bouts:\n",
    "        # rename output file for exemplar\n",
    "        animal, food, bout_index = exemplary_bout\n",
    "        old_path = os.path.join(export_dir3, f'{animal} {food} {bout_index}.png')\n",
    "        new_path = os.path.join(export_dir3, 'exemplary_bout.png')\n",
    "        if os.path.exists(old_path):\n",
    "            if os.path.exists(new_path):\n",
    "                os.remove(new_path)\n",
    "            os.rename(old_path, new_path)\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print('elapsed time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_with_points(x, y, hue, data, ax=None, show_points=False, describe=True):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    boxcolor = '0.75' if hue is None else None\n",
    "    pointcolor = 'k' if hue is None else None\n",
    "    edgecolor = '0.25'\n",
    "    linewidth = 0 if hue is None else 1\n",
    "    size = 4\n",
    "    \n",
    "    data = data.dropna(subset=[y])\n",
    "    \n",
    "    sns.boxplot(x=x, y=y, hue=hue, data=data, ax=ax, color=boxcolor, whis=999) # whiskers span extrema\n",
    "    \n",
    "    if show_points:\n",
    "        sns.swarmplot(x=x, y=y, hue=hue, data=data, ax=ax, color=pointcolor, linewidth=linewidth, edgecolor=edgecolor, size=size, dodge=True)\n",
    "        \n",
    "        if hue is not None:\n",
    "            # avoid duplicate legend entries\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            n = int(len(labels)/2)\n",
    "            ax.legend(handles[:n], labels[:n], title=hue)\n",
    "    \n",
    "    ax.set_xlabel(None)\n",
    "\n",
    "    if describe:\n",
    "        by = [x] if hue is None else [x, hue]\n",
    "        print(y)\n",
    "#         print(data.groupby(by)[y].describe())\n",
    "        print(data.groupby(by)[y].apply(lambda y: {\n",
    "            'N': f'{y.count()}',\n",
    "            'Median': y.median(),\n",
    "#             'Q1': y.quantile(0.25),\n",
    "#             'Q3': y.quantile(0.75),\n",
    "        }).unstack())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "default_markers = ['.', '+', 'x', '1', '4', 'o', 'D']\n",
    "default_colors = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6']\n",
    "\n",
    "def scatter2d(ax, data_subsets, xlabel, xlim, ylabel, ylim, trend=False, trend_separately=False, tooltips=False, padding=0.05, colors=default_colors, markers=default_markers):\n",
    "    \n",
    "    for j, (label, query) in enumerate(data_subsets.items()):\n",
    "        if query is not None:\n",
    "            df = df_all.query(query)\n",
    "            ax.scatter(df[xlabel], df[ylabel],\n",
    "                       label=label, marker=markers[j], c=colors[j], clip_on=False)\n",
    "            \n",
    "    all_points = df_all.query(query_union(data_subsets.values()))[[xlabel, ylabel]].dropna()\n",
    "    \n",
    "    if len(all_points) > 0:\n",
    "\n",
    "        xrange = np.ptp(all_points.iloc[:, 0])\n",
    "        xmin = min(all_points.iloc[:, 0]) - xrange * padding\n",
    "        xmax = max(all_points.iloc[:, 0]) + xrange * padding\n",
    "        if xlim is None:\n",
    "            xlim = [xmin, xmax]\n",
    "        if xlim[0] is None:\n",
    "            xlim[0] = xmin\n",
    "        if xlim[1] is None:\n",
    "            xlim[1] = xmax\n",
    "\n",
    "        yrange = np.ptp(all_points.iloc[:, 1])\n",
    "        ymin = min(all_points.iloc[:, 1]) - yrange * padding\n",
    "        ymax = max(all_points.iloc[:, 1]) + yrange * padding\n",
    "        if ylim is None:\n",
    "            ylim = [ymin, ymax]\n",
    "        if ylim[0] is None:\n",
    "            ylim[0] = ymin\n",
    "        if ylim[1] is None:\n",
    "            ylim[1] = ymax\n",
    "    \n",
    "    if trend_separately:\n",
    "        for j, (label, query) in enumerate(data_subsets.items()):\n",
    "            if query is not None:\n",
    "                df = df_all.query(query)[[xlabel, ylabel]].dropna()\n",
    "                model = sm.OLS(df.iloc[:,1], sm.add_constant(df.iloc[:,0])).fit()\n",
    "                model_stats = 'R$^2$ = {:.2f}, p = {:.5f}, n = {}'.format(model.rsquared, model.pvalues[1], len(df))\n",
    "                print(f'{label}:', model_stats)\n",
    "                model_x = np.linspace(df.iloc[:,0].min()-np.ptp(df.iloc[:,0])*0.1, df.iloc[:,0].max()+np.ptp(df.iloc[:,0])*0.1, 100)\n",
    "                model_y = model.params[0] + model.params[1] * model_x\n",
    "                ax.plot(model_x, model_y, color=colors[j])#, label=model_stats)\n",
    "                \n",
    "    if trend:\n",
    "        model = sm.OLS(all_points.iloc[:,1], sm.add_constant(all_points.iloc[:,0])).fit()\n",
    "        model_stats = 'R$^2$ = {:.2f}, p = {:.5f}, n = {}'.format(model.rsquared, model.pvalues[1], len(all_points))\n",
    "        print('All points:', model_stats)\n",
    "        model_x = np.linspace(all_points.iloc[:,0].min()-np.ptp(all_points.iloc[:,0])*0.1, all_points.iloc[:,0].max()+np.ptp(all_points.iloc[:,0])*0.1, 100)\n",
    "        model_y = model.params[0] + model.params[1] * model_x\n",
    "        ax.plot(model_x, model_y, color='k')#, label=model_stats)\n",
    "    \n",
    "    if tooltips:\n",
    "        # create a transparent layer containing all points for detecting mouse events\n",
    "        sc = ax.scatter(all_points.iloc[:, 0], all_points.iloc[:, 1], label=None,\n",
    "                        alpha=0, # transparent points\n",
    "                        s=0.001, # small radius of tooltip activation\n",
    "                       )\n",
    "        \n",
    "        # initialize a hidden empty tooltip\n",
    "        annot = ax.annotate(\n",
    "            '', xy=(0, 0),                              # initialize empty at origin\n",
    "            xytext=(0, 15), textcoords='offset points', # position text above point\n",
    "            color='k', size=10, ha='center',            # small black centered text\n",
    "            bbox=dict(fc='w', lw=0, alpha=0.6),         # transparent white background\n",
    "            arrowprops=dict(shrink=0, headlength=7, headwidth=7, width=0, lw=0, color='k'), # small black arrow\n",
    "        )\n",
    "        annot.set_visible(False)\n",
    "        \n",
    "        # prepare tooltip contents\n",
    "        tooltip_text = [' '.join([str(x) for x in index]) for index in list(all_points.index)]\n",
    "        \n",
    "        # bind the animation of tooltips to a hover event\n",
    "        def hover(event):\n",
    "            if event.inaxes == ax:\n",
    "                cont, ind = sc.contains(event)\n",
    "                if cont:\n",
    "                    point_index = ind['ind'][0]\n",
    "                    pos = sc.get_offsets()[point_index]\n",
    "                    annot.xy = pos\n",
    "                    annot.set_text(tooltip_text[point_index])\n",
    "                    annot.set_visible(True)\n",
    "                    ax.figure.canvas.draw_idle()\n",
    "                else:\n",
    "                    if annot.get_visible():\n",
    "                        annot.set_visible(False)\n",
    "                        ax.figure.canvas.draw_idle()\n",
    "        ax.figure.canvas.mpl_connect('motion_notify_event', hover)\n",
    "        \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylabel(ylabel)#.replace('rectified ',''))\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyplot_with_scalebars(\n",
    "    blk,\n",
    "    t_start,\n",
    "    t_stop,\n",
    "    plots,\n",
    "    \n",
    "    outfile_basename=None, # base name of output files\n",
    "    export_only=False,     # if True, will not render in notebook\n",
    "    formats=['pdf', 'svg', 'png'], # extensions of output files\n",
    "    dpi=300,               # resolution (applicable only for PNG)\n",
    "    \n",
    "    figsize=(14, 7),       # figure size in inches\n",
    "    linewidth=1,           # thickness of lines in points\n",
    "    layout_settings=None,  # positioning of plot edges and the space between plots\n",
    "    \n",
    "    x_scalebar=1*pq.s,     # size of the time scale bar in seconds\n",
    "    ylabel_padding=10,     # space between trace labels and plots\n",
    "    scalebar_padding=1,    # space between scale bars and plots\n",
    "    scalebar_sep=5,        # space between scale bars and scale labels\n",
    "    barwidth=2,            # thickness of scale bars\n",
    "):\n",
    "    \n",
    "    if export_only:\n",
    "        plt.ioff()\n",
    "        \n",
    "    fig, axes = plt.subplots(len(plots), 1, sharex=True, figsize=figsize, squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, p in enumerate(plots):\n",
    "\n",
    "        # get the subplot axes handle\n",
    "        ax = axes[i]\n",
    "\n",
    "        # select and rescale a channel for the subplot\n",
    "        sig = next((sig for sig in blk.segments[0].analogsignals if sig.name == p['channel']), None)\n",
    "        assert sig is not None, f\"Signal with name {p['channel']} not found\"\n",
    "        sig = sig.time_slice(t_start, t_stop)\n",
    "        sig = sig.rescale(p['units'])\n",
    "\n",
    "        # downsample the data\n",
    "        sig_downsampled = DownsampleNeoSignal(sig, p.get('decimation_factor', 1))\n",
    "\n",
    "        # specify the x- and y-data for the subplot\n",
    "        ax.plot(\n",
    "            sig_downsampled.times,\n",
    "            sig_downsampled.as_quantity(),\n",
    "            linewidth=linewidth,\n",
    "            color=p.get('color', 'k'),\n",
    "        )\n",
    "        \n",
    "        # hide the box around the subplot\n",
    "        ax.set_frame_on(False)\n",
    "\n",
    "        # specify the y-axis label\n",
    "        ylabel = p.get('ylabel', sig.name)\n",
    "        if ylabel is not None:\n",
    "            ax.set_ylabel(ylabel, rotation='horizontal', ha='right', va='center', labelpad=ylabel_padding)\n",
    "\n",
    "        # specify the plot range\n",
    "        ax.set_xlim([t_start, t_stop])\n",
    "        ax.set_ylim(p['ylim'])\n",
    "\n",
    "        # disable tick marks\n",
    "        ax.tick_params(\n",
    "            bottom=False,\n",
    "            left=False,\n",
    "            labelbottom=False,\n",
    "            labelleft=False)\n",
    "\n",
    "        # add y-axis scale bar\n",
    "        if p['scalebar'] is not None:\n",
    "            add_scalebar(ax,\n",
    "                sizey=p['scalebar'],\n",
    "                labely=f'{p[\"scalebar\"]} {sig.units.dimensionality.string}',\n",
    "\n",
    "                loc='center left',\n",
    "                bbox_to_anchor=(1, 0.5),\n",
    "\n",
    "                borderpad=scalebar_padding,\n",
    "                sep=scalebar_sep,\n",
    "                barwidth=barwidth,\n",
    "            )\n",
    "        \n",
    "    # add time scale bar below final plot\n",
    "    if x_scalebar is not None:\n",
    "        add_scalebar(axes[-1],\n",
    "            sizex=x_scalebar.rescale(sig.times.units).magnitude,\n",
    "            labelx=f'{x_scalebar.magnitude:g} {x_scalebar.units.dimensionality.string}',\n",
    "\n",
    "            loc='upper right',\n",
    "            bbox_to_anchor=(1, 0),\n",
    "\n",
    "            borderpad=scalebar_padding,\n",
    "            sep=scalebar_sep,\n",
    "            barwidth=barwidth,\n",
    "        )\n",
    "\n",
    "    # adjust the white space around and between the subplots\n",
    "    if layout_settings is None:\n",
    "        fig.tight_layout(h_pad=0, w_pad=0, pad=0)\n",
    "    else:\n",
    "        plt.subplots_adjust(**layout_settings)\n",
    "\n",
    "    if outfile_basename is not None:\n",
    "        # specify file metadata (applicable only for PDF)\n",
    "        metadata = dict(\n",
    "            Subject = 'Data file: '  + blk.file_origin + '\\n' +\n",
    "                      'Start time: ' + str(t_start)    + '\\n' +\n",
    "                      'End time: '   + str(t_stop),\n",
    "        )\n",
    "\n",
    "        # write the figure to files\n",
    "        for ext in formats:\n",
    "            fig.savefig(f'{outfile_basename}.{ext}', metadata=metadata, dpi=dpi)\n",
    "\n",
    "    if export_only:\n",
    "        plt.ion()\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_unloaded_vs_loaded(df, y, ax, color='0.25', show_statistics=True, show_signif=True, alpha=0.05):\n",
    "    \n",
    "#     df = df.reset_index()\n",
    "#     df.loc[df['Food'] == 'Regular nori', 'Food'] = 'Unloaded'\n",
    "#     df.loc[df['Food'] == 'Tape nori',    'Food'] = 'Loaded'\n",
    "    \n",
    "#     # plot points\n",
    "#     sns.stripplot(\n",
    "#         x='Food', y=y, hue='Animal',\n",
    "#         data=df.groupby(['Animal', 'Food'])[y].mean().reset_index(),\n",
    "#         order=['Unloaded', 'Loaded'],\n",
    "#         jitter=False,\n",
    "#         palette=[color], # do not desaturate by animal\n",
    "#         ax=ax,\n",
    "#         clip_on=False,\n",
    "#     )\n",
    "#     ax.legend_.remove()\n",
    "\n",
    "#     # plot lines connecting points\n",
    "#     ax.plot([\n",
    "#         df.query('Food == \"Unloaded\"').groupby('Animal')[y].mean(),\n",
    "#         df.query('Food == \"Loaded\"').groupby('Animal')[y].mean()\n",
    "#     ], color='0.75', clip_on=False)\n",
    "\n",
    "#     ax.set_xlim([-0.25, 1.25])\n",
    "#     ax.set_xlabel(None)\n",
    "#     sns.despine(ax=ax)\n",
    "\n",
    "#     if show_statistics:\n",
    "#         print(df.groupby(['Animal', 'Food'])[y].apply(lambda x: {'Mean': x.mean(), 'Count': x.count()}).unstack([1, 2])[['Unloaded', 'Loaded']])\n",
    "#         print()\n",
    "#         signif = differences_test(\n",
    "#             x=df.query('Food == \"Loaded\"').groupby('Animal')[y].mean(),\n",
    "#             y=df.query('Food == \"Unloaded\"').groupby('Animal')[y].mean(),\n",
    "#             x_label='loaded swallows',\n",
    "#             y_label='unloaded swallows',\n",
    "#             measure_label='mean [' + ' '.join(y.split()[:-1]) + ']',\n",
    "#             units=y.split()[-1].strip('()'),\n",
    "#             alpha=alpha,\n",
    "#         )\n",
    "        \n",
    "#         if show_signif and signif == '*':\n",
    "#             ax.annotate(\n",
    "#                 '*',\n",
    "#                 xy=(0.5, 1), xycoords='axes fraction',\n",
    "#                 xytext=(0, -20), textcoords='offset points',\n",
    "#                 ha='center', fontsize='xx-large', color='0.5',\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unloaded_vs_loaded(df, y, ax, color='0.25', show_all=False, show_statistics=True, show_signif=True, bracket_width=1.0, alpha=0.05):\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    df.loc[df['Food'] == 'Regular nori', 'Food'] = 'Unloaded'\n",
    "    df.loc[df['Food'] == 'Tape nori',    'Food'] = 'Loaded'\n",
    "    \n",
    "#     # plot means\n",
    "#     sns.pointplot(\n",
    "#         x='Animal', y=y, hue='Food',\n",
    "#         data=df,\n",
    "#         hue_order=['Unloaded', 'Loaded'],\n",
    "#         dodge=0.4,\n",
    "# #         palette=[color], # do not desaturate by food\n",
    "#         palette=['0.75'], # do not desaturate by food\n",
    "#         join=False,\n",
    "#         estimator=np.mean,\n",
    "#         ci='sd',\n",
    "#         scale=0.6,\n",
    "#         capsize=0.2,\n",
    "#         errwidth=1,\n",
    "#         ax=ax,\n",
    "#     )\n",
    "    \n",
    "#     # plot individual swallows points\n",
    "#     if show_all:\n",
    "#         sns.stripplot(\n",
    "#             x='Animal', y=y, hue='Food',\n",
    "#             data=df,\n",
    "#             hue_order=['Unloaded', 'Loaded'],\n",
    "#             jitter=False,\n",
    "#             dodge=True,\n",
    "#             linewidth=1,\n",
    "#             marker='_',\n",
    "#             color='k',\n",
    "#             ax=ax,\n",
    "#             clip_on=False,\n",
    "#         )\n",
    "\n",
    "    for food, offset in {'Unloaded': -0.2, 'Loaded': 0.2}.items():\n",
    "        for i, (animal, values) in enumerate(df[df['Food'] == food].groupby('Animal')[y]):\n",
    "            \n",
    "            # plot the mean\n",
    "            ax.scatter(\n",
    "                [i+offset], values.mean(),\n",
    "                color=color,\n",
    "                s=15,\n",
    "                zorder=2, clip_on=False,\n",
    "            )\n",
    "            \n",
    "            # plot the standard error of the mean\n",
    "            lower_sem = values.mean()-values.sem()\n",
    "            upper_sem = values.mean()+values.sem()\n",
    "            ax.plot(\n",
    "                [i+offset]*2, [lower_sem, upper_sem],\n",
    "                color=color, lw=1,\n",
    "                zorder=2, clip_on=False,\n",
    "            )\n",
    "            ax.plot(\n",
    "                [i+offset-0.1, i+offset+0.1], [lower_sem, lower_sem],\n",
    "                color=color, lw=1,\n",
    "                zorder=2, clip_on=False,\n",
    "            )\n",
    "            ax.plot(\n",
    "                [i+offset-0.1, i+offset+0.1], [upper_sem, upper_sem],\n",
    "                color=color, lw=1,\n",
    "                zorder=2, clip_on=False,\n",
    "            )\n",
    "            \n",
    "            if show_all:\n",
    "                # plot individual swallows points\n",
    "                ax.scatter(\n",
    "                    [i+offset]*values.size, values,\n",
    "\n",
    "                    facecolors='none',\n",
    "                    edgecolors='k',\n",
    "                    linewidths=1,\n",
    "                    s=12,\n",
    "\n",
    "#                     facecolors='k',\n",
    "#                     marker='_',\n",
    "#                     s=18,\n",
    "\n",
    "                    zorder=3,\n",
    "                )\n",
    "            ax.axvline(x=i+offset, c='0.9', lw=0.5, zorder=-2)\n",
    "            ax.annotate(\n",
    "                food[0],\n",
    "                xy=(i+offset, 1), xycoords=ax.get_xaxis_transform(),\n",
    "                ha='center', va='bottom', fontsize='x-small', color='0.5',\n",
    "            )\n",
    "            if food == 'Unloaded': # do it just once\n",
    "                ax.annotate(\n",
    "                    i+1,\n",
    "                    xy=(i, 1), xycoords=ax.get_xaxis_transform(),\n",
    "                    xytext=(0, 10), textcoords='offset points',\n",
    "                    ha='center', va='bottom', fontsize='x-small', color='0.5',\n",
    "                )\n",
    "\n",
    "    # plot lines connecting points\n",
    "    for i, (y_unloaded, y_loaded) in enumerate(zip(\n",
    "                df.query('Food == \"Unloaded\"').groupby('Animal')[y].mean(),\n",
    "                df.query('Food == \"Loaded\"').groupby('Animal')[y].mean())):\n",
    "        ax.plot(\n",
    "            [i-0.2, i+0.2],\n",
    "            [y_unloaded, y_loaded],\n",
    "            color='0.75',\n",
    "            zorder=-1,\n",
    "        )\n",
    "\n",
    "    ax.tick_params(bottom=False, labelbottom=False)\n",
    "#     ax.legend_.remove()\n",
    "    ax.set_xlabel(None)\n",
    "    sns.despine(ax=ax, bottom=True)\n",
    "\n",
    "    if show_statistics:\n",
    "        print(df.groupby(['Animal', 'Food'])[y].apply(lambda x: {'Mean': x.mean(), 'Count': x.count()}).unstack([1, 2])[['Unloaded', 'Loaded']])\n",
    "        print()\n",
    "        signif = differences_test(\n",
    "            x=df.query('Food == \"Loaded\"').groupby('Animal')[y].mean(),\n",
    "            y=df.query('Food == \"Unloaded\"').groupby('Animal')[y].mean(),\n",
    "            x_label='loaded swallows',\n",
    "            y_label='unloaded swallows',\n",
    "            measure_label='mean [' + ' '.join(y.split()[:-1]) + ']',\n",
    "            units=y.split()[-1].strip('()'),\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        \n",
    "        if show_signif and signif == '*':\n",
    "            from matplotlib.patches import ArrowStyle\n",
    "            ax.annotate(\n",
    "                '*',\n",
    "                xy=(0.5, -0.01), xycoords='axes fraction',\n",
    "                xytext=(0, -22), textcoords='offset points',\n",
    "                arrowprops=dict(arrowstyle=ArrowStyle.BracketB(widthB=bracket_width, lengthB=0.2, angleB=None), color='0.5'),\n",
    "                ha='center', fontsize='x-large', color='0.5',\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [FIGURE 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_exemplary_swallow\n",
    "(data_set_name, time_window) = feeding_bouts[exemplary_swallow]\n",
    "animal, _, _ = exemplary_swallow\n",
    "behavior = 0\n",
    "\n",
    "t_start, t_stop = time_window*pq.s\n",
    "plots = [\n",
    "    {'channel': 'BN2', 'ylabel': None, 'units': 'uV', 'ylim': [-60, 38], 'scalebar': 50},\n",
    "]\n",
    "plot_names = [p['channel'] for p in plots]\n",
    "plot_units = [p['units'] for p in plots]\n",
    "\n",
    "kwargs = dict(\n",
    "    figsize = (5, 2),\n",
    "    linewidth = 0.5,\n",
    "    x_scalebar = None, # 1*pq.s,\n",
    ")\n",
    "\n",
    "# load the metadata\n",
    "metadata = neurotic.MetadataSelector('../../data/metadata.yml')\n",
    "metadata.select(data_set_name)\n",
    "\n",
    "# ensure that the right filters are used\n",
    "metadata['filters'] = sig_filters_by_animal[animal]\n",
    "\n",
    "# load the data\n",
    "blk = neurotic.load_dataset(metadata, lazy=True)\n",
    "\n",
    "# manually perform filters\n",
    "blk = apply_filters(blk, metadata)\n",
    "\n",
    "#################################\n",
    "\n",
    "epochs = [\n",
    "    {'name': 'B38 activity',       'label': 'B38 activity',      'color': 'B38',      'time': [t_start.magnitude, 2978.89]},\n",
    "    {'name': 'B3/6/9/10 activity', 'label': 'B3/B6/B9 activity', 'color': 'B3/B6/B9', 'time': [2979.23, 2983.28]},\n",
    "]\n",
    "\n",
    "amplitude_discriminators = [\n",
    "    {'name': 'B38',   'channel': 'BN2', 'epoch': 'B38 activity',       'amplitude': [  6,  15], 'units': 'uV'}, # actual thresholds used: [7, 20]\n",
    "    {'name': 'B6/B9', 'channel': 'BN2', 'epoch': 'B3/6/9/10 activity', 'amplitude': [-17,  -9], 'units': 'uV'}, # actual thresholds used: [-25, -9]\n",
    "    {'name': 'B3',    'channel': 'BN2', 'epoch': 'B3/6/9/10 activity', 'amplitude': [ 20,  30], 'units': 'uV'}, # actual thresholds used: [-60, -25]\n",
    "]\n",
    "\n",
    "spike_trains = []\n",
    "for discriminator in amplitude_discriminators:\n",
    "    sig = get_sig(blk, discriminator['channel'])\n",
    "    if sig is not None:\n",
    "        sig = sig.time_slice(t_start, t_stop)\n",
    "        st = _detect_spikes(sig, discriminator, blk.segments[0].epochs)\n",
    "        epoch = next((ep for ep in epochs if ep['name'] == discriminator['epoch']))\n",
    "        st_epoch_start = epoch['time'][0]*pq.s\n",
    "        st_epoch_end = epoch['time'][1]*pq.s\n",
    "        st = st.time_slice(st_epoch_start, st_epoch_end)\n",
    "        spike_trains.append(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Figure 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the signal\n",
    "fig, axes = prettyplot_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "ax = axes[0]\n",
    "\n",
    "# # add time scale bar\n",
    "# add_scalebar(ax,\n",
    "#     sizex=1, labelx='1 s',\n",
    "#     loc='lower right', bbox_to_anchor=(1, 0),\n",
    "#     borderpad=0.5, sep=5, barwidth=2,\n",
    "# )\n",
    "\n",
    "#################################\n",
    "\n",
    "fig.tight_layout(h_pad=0, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'figure-3A.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Figure 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the signal\n",
    "fig, axes = prettyplot_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "ax = axes[0]\n",
    "\n",
    "# # add time scale bar\n",
    "# add_scalebar(ax,\n",
    "#     sizex=1, labelx='1 s',\n",
    "#     loc='lower right', bbox_to_anchor=(1, 0),\n",
    "#     borderpad=0.5, sep=5, barwidth=2,\n",
    "# )\n",
    "\n",
    "#################################\n",
    "\n",
    "# add epoch bars\n",
    "for d in epochs:\n",
    "    label = d['label']\n",
    "    color = d['color']\n",
    "    left, right = d['time']\n",
    "    bottom, top = 0.15, 0.18\n",
    "    width = right-left\n",
    "    height = top-bottom\n",
    "    rect = patches.Rectangle((left, bottom), width, height, linewidth=0, facecolor=unit_colors[color], fill=True, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    ax.add_patch(rect)\n",
    "    ax.annotate(label, xy=((right-left)/2+left, bottom-0.02), xycoords=('data', 'axes fraction'), ha='center', va='top', c=unit_colors[color])\n",
    "\n",
    "#################################\n",
    "\n",
    "# add amplitude discriminator thresholds\n",
    "for d in amplitude_discriminators:\n",
    "    unit = d['name']\n",
    "    epoch = next((ep for ep in epochs if ep['name'] == d['epoch']))\n",
    "    left, right = epoch['time']\n",
    "    bottom, top = d['amplitude']\n",
    "    ax.hlines(y=bottom, xmin=left, xmax=right, color=unit_colors[unit], ls='--')\n",
    "    ax.hlines(y=top,    xmin=left, xmax=right, color=unit_colors[unit], ls='--')\n",
    "\n",
    "###############################\n",
    "\n",
    "fig.tight_layout(h_pad=0, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'figure-3B.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Figure 3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the signal\n",
    "fig, axes = prettyplot_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "ax = axes[0]\n",
    "\n",
    "# # add time scale bar\n",
    "# add_scalebar(ax,\n",
    "#     sizex=1, labelx='1 s',\n",
    "#     loc='lower right', bbox_to_anchor=(1, 0),\n",
    "#     borderpad=0.5, sep=5, barwidth=2,\n",
    "# )\n",
    "\n",
    "#################################\n",
    "\n",
    "# add epoch bars\n",
    "for d in epochs:\n",
    "    label = d['label']\n",
    "    color = d['color']\n",
    "    left, right = d['time']\n",
    "    bottom, top = 0.15, 0.18\n",
    "    width = right-left\n",
    "    height = top-bottom\n",
    "    rect = patches.Rectangle((left, bottom), width, height, linewidth=0, facecolor=unit_colors[color], fill=True, clip_on=False, transform=ax.get_xaxis_transform())\n",
    "    ax.add_patch(rect)\n",
    "    ax.annotate(label, xy=((right-left)/2+left, bottom-0.02), xycoords=('data', 'axes fraction'), ha='center', va='top', c=unit_colors[color])\n",
    "\n",
    "#################################\n",
    "\n",
    "# add amplitude discriminator thresholds\n",
    "for d in amplitude_discriminators:\n",
    "    unit = d['name']\n",
    "    epoch = next((ep for ep in epochs if ep['name'] == d['epoch']))\n",
    "    left, right = epoch['time']\n",
    "    bottom, top = d['amplitude']\n",
    "    ax.hlines(y=bottom, xmin=left, xmax=right, color=unit_colors[unit], ls='--')\n",
    "    ax.hlines(y=top,    xmin=left, xmax=right, color=unit_colors[unit], ls='--')\n",
    "\n",
    "###############################\n",
    "\n",
    "# add spike markers\n",
    "for st in spike_trains:\n",
    "    # get the neural channel\n",
    "    channel = st.annotations['channels'][0]\n",
    "    sig = get_sig(blk, channel)\n",
    "\n",
    "    # get the signal for the entire bout\n",
    "    sig = sig.time_slice(t_start, t_stop)\n",
    "    sig = sig.rescale(plot_units[plot_names.index(channel)])\n",
    "\n",
    "    # plot spikes\n",
    "    spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "    ax.scatter(st.times.rescale('s'), spike_amplitudes, marker='.', s=30, c=unit_colors[st.name], zorder=3)\n",
    "\n",
    "#################################\n",
    "\n",
    "fig.tight_layout(h_pad=0, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'figure-3C.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Figure 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the signal\n",
    "fig, axes = prettyplot_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "ax = axes[0]\n",
    "\n",
    "# add time scale bar\n",
    "add_scalebar(ax,\n",
    "    sizex=1, labelx='1 s',\n",
    "    loc='lower right', bbox_to_anchor=(1, 0),\n",
    "    borderpad=0.5, sep=5, barwidth=2,\n",
    ")\n",
    "\n",
    "#################################\n",
    "\n",
    "# add spike markers\n",
    "for st in spike_trains:\n",
    "    # get the neural channel\n",
    "    channel = st.annotations['channels'][0]\n",
    "    sig = get_sig(blk, channel)\n",
    "\n",
    "    # get the signal for the entire bout\n",
    "    sig = sig.time_slice(t_start, t_stop)\n",
    "    sig = sig.rescale(plot_units[plot_names.index(channel)])\n",
    "\n",
    "    # plot spikes\n",
    "    spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "    ax.scatter(st.times.rescale('s'), spike_amplitudes, marker='.', s=30, c=unit_colors[st.name], zorder=3)\n",
    "\n",
    "#################################\n",
    "\n",
    "unit_burst_boxes = {\n",
    "    'B38':   [-12, 12],\n",
    "    'B6/B9': [-20, 15],\n",
    "    'B3':    [-43, 32],\n",
    "}\n",
    "\n",
    "# plot burst windows\n",
    "for k, unit in enumerate(unit_burst_boxes.keys()):\n",
    "    left = df.loc[behavior, f'{unit} burst start (s)']\n",
    "    right = df.loc[behavior, f'{unit} burst end (s)']\n",
    "    if np.isfinite(left) and np.isfinite(right):\n",
    "        width = right-left\n",
    "        bottom, top = unit_burst_boxes[unit]\n",
    "        height = top-bottom\n",
    "        rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor=unit_colors[unit], fill=False, zorder=3, clip_on=False)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# add unit name labels\n",
    "ax.annotate('B38',   xy=(2978.15, 0.75), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B38'])\n",
    "ax.annotate('B6/B9', xy=(2980.10, 0.79), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B6/B9'])\n",
    "ax.annotate('B3',    xy=(2981.85, 0.87), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['B3'])\n",
    "\n",
    "#################################\n",
    "\n",
    "fig.tight_layout(h_pad=0, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'figure-3D.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [FIGURE 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Figure 4C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplary_swallow_unloaded = ('JG12', 'Regular nori', 1, 1)\n",
    "animal, food, bout, behavior = exemplary_swallow_unloaded\n",
    "df = df_all.loc[(animal, food, bout)]\n",
    "(data_set_name, time_window) = 'IN VIVO / JG12 / 2019-05-10 / 002', [234.94, 242.14] # t = 237.1, twidth = 7.2 ==> [234.94, 242.14]\n",
    "\n",
    "t_start, t_stop = time_window*pq.s\n",
    "plots = [\n",
    "#     {'channel': 'BN2', 'ylabel': None, 'units': 'uV', 'ylim': [-60, 38], 'scalebar': 50},\n",
    "    {'channel': 'BN2', 'ylabel': None, 'units': 'uV', 'ylim': [-60, 45], 'scalebar': 50},\n",
    "]\n",
    "plot_names = [p['channel'] for p in plots]\n",
    "plot_units = [p['units'] for p in plots]\n",
    "\n",
    "kwargs = dict(\n",
    "    figsize = (5, 2),\n",
    "    linewidth = 0.5,\n",
    "    x_scalebar = None, # 1*pq.s,\n",
    ")\n",
    "\n",
    "# load the metadata\n",
    "metadata = neurotic.MetadataSelector('../../data/metadata.yml')\n",
    "metadata.select(data_set_name)\n",
    "\n",
    "# ensure that the right filters are used\n",
    "metadata['filters'] = sig_filters_by_animal[animal]\n",
    "\n",
    "# load the data\n",
    "blk = neurotic.load_dataset(metadata, lazy=True)\n",
    "\n",
    "# manually perform filters\n",
    "blk = apply_filters(blk, metadata)\n",
    "\n",
    "#################################\n",
    "\n",
    "epochs = [\n",
    "#     {'name': 'B38 activity',       'label': 'B38',      'color': 'B38',      'time': [t_start.magnitude, 2978.89]},\n",
    "    {'name': 'B3/6/9/10 activity', 'label': 'B3/B6/B9', 'color': 'B3/B6/B9', 'time': [236.55, 239.53]},\n",
    "]\n",
    "\n",
    "amplitude_discriminators = [\n",
    "#     {'name': 'B38',   'channel': 'BN2', 'epoch': 'B38 activity',       'amplitude': [  6,  15], 'units': 'uV'}, # actual thresholds used: [7, 20]\n",
    "    {'name': 'B6/B9', 'channel': 'BN2', 'epoch': 'B3/6/9/10 activity', 'amplitude': [-17,  -9], 'units': 'uV'}, # actual thresholds used: [-25, -9]\n",
    "    {'name': 'B3',    'channel': 'BN2', 'epoch': 'B3/6/9/10 activity', 'amplitude': [ 20,  45], 'units': 'uV'}, # actual thresholds used: [-60, -25]\n",
    "]\n",
    "\n",
    "spike_trains = []\n",
    "for discriminator in amplitude_discriminators:\n",
    "    sig = get_sig(blk, discriminator['channel'])\n",
    "    if sig is not None:\n",
    "        sig = sig.time_slice(t_start, t_stop)\n",
    "        st = _detect_spikes(sig, discriminator, blk.segments[0].epochs)\n",
    "        epoch = next((ep for ep in epochs if ep['name'] == discriminator['epoch']))\n",
    "        st_epoch_start = epoch['time'][0]*pq.s\n",
    "        st_epoch_end = epoch['time'][1]*pq.s\n",
    "        st = st.time_slice(st_epoch_start, st_epoch_end)\n",
    "        spike_trains.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the signal\n",
    "fig, axes = prettyplot_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "ax = axes[0]\n",
    "\n",
    "# add time scale bar\n",
    "add_scalebar(ax,\n",
    "    sizex=1, labelx='1 s',\n",
    "    loc='lower right', bbox_to_anchor=(1, 0),\n",
    "    borderpad=0.5, sep=5, barwidth=2,\n",
    ")\n",
    "\n",
    "#################################\n",
    "\n",
    "# add spike markers\n",
    "for st in spike_trains:\n",
    "    # get the neural channel\n",
    "    channel = st.annotations['channels'][0]\n",
    "    sig = get_sig(blk, channel)\n",
    "\n",
    "    # get the signal for the entire bout\n",
    "    sig = sig.time_slice(t_start, t_stop)\n",
    "    sig = sig.rescale(plot_units[plot_names.index(channel)])\n",
    "\n",
    "    # plot spikes\n",
    "    spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "    ax.scatter(st.times.rescale('s'), spike_amplitudes, marker='.', s=20, c=unit_colors[st.name], zorder=3)\n",
    "\n",
    "#################################\n",
    "\n",
    "unit_burst_boxes = {\n",
    "#     'B38':   [-12, 12],\n",
    "#     'B6/B9': [-20, 15],\n",
    "    'B6/B9': [-20, 20],\n",
    "    'B3':    [-43, 32],\n",
    "}\n",
    "\n",
    "# plot burst windows\n",
    "for k, unit in enumerate(unit_burst_boxes.keys()):\n",
    "    left = df.loc[behavior, f'{unit} burst start (s)']\n",
    "    right = df.loc[behavior, f'{unit} burst end (s)']\n",
    "    if np.isfinite(left) and np.isfinite(right):\n",
    "        width = right-left\n",
    "        bottom, top = unit_burst_boxes[unit]\n",
    "        height = top-bottom\n",
    "        rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor=unit_colors[unit], fill=False, zorder=3, clip_on=False)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# add unit name labels\n",
    "# ax.annotate('B38',   xy=(2978.15, 0.75), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B38'])\n",
    "ax.annotate('B6/B9', xy=(237.75, 0.80), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B6/B9'])\n",
    "ax.annotate('B3',    xy=(239.35, 0.90), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['B3'])\n",
    "\n",
    "#################################\n",
    "\n",
    "fig.tight_layout(h_pad=0, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'figure-4C.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Figure 4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_exemplary_swallow\n",
    "(data_set_name, time_window) = feeding_bouts[exemplary_swallow]\n",
    "animal, _, _ = exemplary_swallow\n",
    "behavior = 0\n",
    "\n",
    "t_start, t_stop = time_window*pq.s\n",
    "plots = [\n",
    "#     {'channel': 'BN2', 'ylabel': None, 'units': 'uV', 'ylim': [-60, 38], 'scalebar': 50},\n",
    "    {'channel': 'BN2', 'ylabel': None, 'units': 'uV', 'ylim': [-60, 45], 'scalebar': 50},\n",
    "]\n",
    "plot_names = [p['channel'] for p in plots]\n",
    "plot_units = [p['units'] for p in plots]\n",
    "\n",
    "kwargs = dict(\n",
    "    figsize = (5, 2),\n",
    "    linewidth = 0.5,\n",
    "    x_scalebar = None, # 1*pq.s,\n",
    ")\n",
    "\n",
    "# load the metadata\n",
    "metadata = neurotic.MetadataSelector('../../data/metadata.yml')\n",
    "metadata.select(data_set_name)\n",
    "\n",
    "# ensure that the right filters are used\n",
    "metadata['filters'] = sig_filters_by_animal[animal]\n",
    "\n",
    "# load the data\n",
    "blk = neurotic.load_dataset(metadata, lazy=True)\n",
    "\n",
    "# manually perform filters\n",
    "blk = apply_filters(blk, metadata)\n",
    "\n",
    "#################################\n",
    "\n",
    "epochs = [\n",
    "#     {'name': 'B38 activity',       'label': 'B38',      'color': 'B38',      'time': [t_start.magnitude, 2978.89]},\n",
    "    {'name': 'B3/6/9/10 activity', 'label': 'B3/B6/B9', 'color': 'B3/B6/B9', 'time': [2979.23, 2983.28]},\n",
    "]\n",
    "\n",
    "amplitude_discriminators = [\n",
    "#     {'name': 'B38',   'channel': 'BN2', 'epoch': 'B38 activity',       'amplitude': [  6,  15], 'units': 'uV'}, # actual thresholds used: [7, 20]\n",
    "    {'name': 'B6/B9', 'channel': 'BN2', 'epoch': 'B3/6/9/10 activity', 'amplitude': [-17,  -9], 'units': 'uV'}, # actual thresholds used: [-25, -9]\n",
    "    {'name': 'B3',    'channel': 'BN2', 'epoch': 'B3/6/9/10 activity', 'amplitude': [ 20,  30], 'units': 'uV'}, # actual thresholds used: [-60, -25]\n",
    "]\n",
    "\n",
    "spike_trains = []\n",
    "for discriminator in amplitude_discriminators:\n",
    "    sig = get_sig(blk, discriminator['channel'])\n",
    "    if sig is not None:\n",
    "        sig = sig.time_slice(t_start, t_stop)\n",
    "        st = _detect_spikes(sig, discriminator, blk.segments[0].epochs)\n",
    "        epoch = next((ep for ep in epochs if ep['name'] == discriminator['epoch']))\n",
    "        st_epoch_start = epoch['time'][0]*pq.s\n",
    "        st_epoch_end = epoch['time'][1]*pq.s\n",
    "        st = st.time_slice(st_epoch_start, st_epoch_end)\n",
    "        spike_trains.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the signal\n",
    "fig, axes = prettyplot_with_scalebars(blk, t_start, t_stop, plots, **kwargs)\n",
    "ax = axes[0]\n",
    "\n",
    "# add time scale bar\n",
    "add_scalebar(ax,\n",
    "    sizex=1, labelx='1 s',\n",
    "    loc='lower right', bbox_to_anchor=(1, 0),\n",
    "    borderpad=0.5, sep=5, barwidth=2,\n",
    ")\n",
    "\n",
    "#################################\n",
    "\n",
    "# add spike markers\n",
    "for st in spike_trains:\n",
    "    # get the neural channel\n",
    "    channel = st.annotations['channels'][0]\n",
    "    sig = get_sig(blk, channel)\n",
    "\n",
    "    # get the signal for the entire bout\n",
    "    sig = sig.time_slice(t_start, t_stop)\n",
    "    sig = sig.rescale(plot_units[plot_names.index(channel)])\n",
    "\n",
    "    # plot spikes\n",
    "    spike_amplitudes = np.array([sig[sig.time_index(t)] for t in st]) * pq.Quantity(sig.units)\n",
    "    ax.scatter(st.times.rescale('s'), spike_amplitudes, marker='.', s=20, c=unit_colors[st.name], zorder=3)\n",
    "\n",
    "#################################\n",
    "\n",
    "unit_burst_boxes = {\n",
    "#     'B38':   [-12, 12],\n",
    "#     'B6/B9': [-20, 15],\n",
    "    'B6/B9': [-20, 20],\n",
    "    'B3':    [-43, 32],\n",
    "}\n",
    "\n",
    "# plot burst windows\n",
    "for k, unit in enumerate(unit_burst_boxes.keys()):\n",
    "    left = df.loc[behavior, f'{unit} burst start (s)']\n",
    "    right = df.loc[behavior, f'{unit} burst end (s)']\n",
    "    if np.isfinite(left) and np.isfinite(right):\n",
    "        width = right-left\n",
    "        bottom, top = unit_burst_boxes[unit]\n",
    "        height = top-bottom\n",
    "        rect = patches.Rectangle((left, bottom), width, height, linewidth=2, ls='-', edgecolor=unit_colors[unit], fill=False, zorder=3, clip_on=False)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# add unit name labels\n",
    "# ax.annotate('B38',   xy=(2978.15, 0.75), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B38'])\n",
    "ax.annotate('B6/B9', xy=(2980.10, 0.79), xycoords=('data', 'axes fraction'), ha='center', c=unit_colors['B6/B9'])\n",
    "ax.annotate('B3',    xy=(2981.85, 0.82), xycoords=('data', 'axes fraction'), ha='left',   c=unit_colors['B3'])\n",
    "\n",
    "#################################\n",
    "\n",
    "fig.tight_layout(h_pad=0, w_pad=0, pad=0)\n",
    "\n",
    "fig.savefig(os.path.join(export_dir, 'figure-4D.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
